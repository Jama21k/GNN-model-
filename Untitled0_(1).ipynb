{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jama21k/GNN-model-/blob/main/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"GNN Model.ipynb\n",
        "\n",
        "Automatically generated by Colab.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1pSO-VEyn5Cywjw9sXKn2fjXdVbI3hAsH\n",
        "\"\"\"\n",
        "\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"Untitled2.ipynb\n",
        "\n",
        "Automatically generated by Colab.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/193StgLnr4doKklAxwBiQsVX3njEfb1oa\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# @title **Initial Setup Cell** (Run First)\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "\n",
        "# 2. Upload your data file\n",
        "print(\"Please upload your data file:\")\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "DATA_PATH = list(uploaded.keys())[0]  # Automatically gets filename"
      ],
      "metadata": {
        "id": "VXRNqOTdxViM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "APFVCarUBg0v",
        "outputId": "9bbe09ea-18f0-4972-c6aa-3c49fe9857c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting weather forecasting pipeline...\n",
            "Using data from: updated_data (4).xlsx\n",
            "\n",
            "1. Loading and preprocessing data...\n",
            "Filtered data to 5 stations: ['Barstow', 'Fresno', 'LA Downtown', 'Mammoth Lakes', 'San Diego']\n",
            "Train set: 204503 samples (75.2%)\n",
            "Validation set: 35888 samples (13.2%)\n",
            "Test set: 31678 samples (11.6%)\n",
            "Found 5 stations across 5 regions\n",
            "Using features: ['HourlyRelativeHumidity', 'HourlyStationPressure', 'Temperature_C', 'hour_of_day', 'day_of_year']\n",
            "\n",
            "2. Creating DataLoaders...\n",
            "WARNING: No valid windows found in dataset. Using reduced requirements.\n",
            "Found 193977 windows with relaxed continuity requirements\n",
            "WARNING: No valid windows found in dataset. Using reduced requirements.\n",
            "Found 33478 windows with relaxed continuity requirements\n",
            "WARNING: No valid windows found in dataset. Using reduced requirements.\n",
            "Found 29461 windows with relaxed continuity requirements\n",
            "\n",
            "3. Creating model...\n",
            "Model created with 18904 parameters\n",
            "\n",
            "4. Training model...\n",
            "Beginning training...\n",
            "Epoch 1/20:\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# ============================================================\n",
        "# @title **ðŸ“Š Load and Process Data**\n",
        "# ============================================================\n",
        "# ============================================================\n",
        "# @title **ðŸ“¥ Import Dependencies**\n",
        "# ============================================================\n",
        "\n",
        "import torch\n",
        "import time\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import os\n",
        "from datetime import datetime, timedelta\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\"\"\"# Running Model\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "def load_data(file_path):\n",
        "    \"\"\"\n",
        "    Load weather data, clean missing values, and interpolate along the temporal dimension.\n",
        "    Handles 'VRB' in HourlyWindDirection by replacing it with NaN.\n",
        "    \"\"\"\n",
        "    # Determine file type and read\n",
        "    if file_path.endswith('.xlsx'):\n",
        "        df = pd.read_excel(file_path, engine='openpyxl')\n",
        "    else:\n",
        "        df = pd.read_csv(file_path, encoding='ISO-8859-1')\n",
        "\n",
        "    # Convert timestamp\n",
        "    if 'timestamp' in df.columns:\n",
        "        df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
        "    elif 'DATE' in df.columns:\n",
        "        df['timestamp'] = pd.to_datetime(df['DATE'])\n",
        "        df = df.rename(columns={'DATE': 'date_original'})\n",
        "\n",
        "    # Ensure time-based ordering before interpolation\n",
        "    df = df.sort_values(by='timestamp')\n",
        "\n",
        "    # Add hour of day feature\n",
        "    df['hour_of_day'] = df['timestamp'].dt.hour / 23.0  # Normalize to [0,1]\n",
        "\n",
        "    # Add day of year feature (normalized)\n",
        "    df['day_of_year'] = df['timestamp'].dt.dayofyear / 365.0  # Normalize to [0,1]\n",
        "\n",
        "    # Add region information (assuming station_id or similar column exists)\n",
        "    # This would be replaced with actual mapping in your implementation\n",
        "    if 'station_id' in df.columns:\n",
        "        # Example mapping - you would replace this with your actual regions\n",
        "        region_mapping = {\n",
        "            # Coastal stations\n",
        "            'San Francisco': 'coastal',\n",
        "            'San Diego': 'coastal',\n",
        "            # Mountain stations\n",
        "            'Mammoth Lakes': 'mountain',\n",
        "            'South Lake Tahoe': 'mountain',\n",
        "            # Desert stations\n",
        "            'Palm Spring': 'desert',\n",
        "            'Barstow': 'desert',\n",
        "            # Valley stations\n",
        "            'Fresno': 'valley',\n",
        "            'Sacramento': 'valley',\n",
        "            # Urban stations\n",
        "            'LA Downtown': 'urban',\n",
        "            'San Jose': 'urban'\n",
        "        }\n",
        "        df['region'] = df['station_id'].map(region_mapping)\n",
        "\n",
        "    # Interpolate missing values along the time dimension\n",
        "    df.interpolate(method='linear', limit_direction='both', inplace=True)\n",
        "\n",
        "    # MODIFICATION: Filter to keep only one station per region\n",
        "    if 'region' in df.columns:\n",
        "        # Get one representative station per region\n",
        "        station_regions = df.groupby('station_id')['region'].first()\n",
        "        regions = station_regions.unique()\n",
        "        kept_stations = []\n",
        "\n",
        "        for region in regions:\n",
        "            # Get the first station ID for each region\n",
        "            region_stations = station_regions[station_regions == region].index\n",
        "            if len(region_stations) > 0:\n",
        "                kept_stations.append(region_stations[0])\n",
        "\n",
        "            # If we already have 5 stations, stop\n",
        "            if len(kept_stations) >= 5:\n",
        "                break\n",
        "\n",
        "        # Filter the dataframe to keep only these stations\n",
        "        df = df[df['station_id'].isin(kept_stations)]\n",
        "        print(f\"Filtered data to {len(kept_stations)} stations: {kept_stations}\")\n",
        "\n",
        "    return df\n",
        "\n",
        "def split_data_by_time(df):\n",
        "    \"\"\"\n",
        "    Split data into training (2021-2023), validation (Jan-Jun 2024),\n",
        "    and testing (Jul-Dec 2024) sets.\n",
        "    \"\"\"\n",
        "    # Create year and month columns for easy filtering\n",
        "    df['year'] = df['timestamp'].dt.year\n",
        "    df['month'] = df['timestamp'].dt.month\n",
        "\n",
        "    # Training: 2021-2023\n",
        "    train_df = df[(df['year'] >= 2021) & (df['year'] <= 2023)]\n",
        "\n",
        "    # Validation: Jan-Jun 2024\n",
        "    val_df = df[(df['year'] == 2024) & (df['month'] <= 6)]\n",
        "\n",
        "    # Testing: Jul-Dec 2024\n",
        "    test_df = df[(df['year'] == 2024) & (df['month'] > 6)]\n",
        "\n",
        "    print(f\"Train set: {len(train_df)} samples ({len(train_df)/len(df)*100:.1f}%)\")\n",
        "    print(f\"Validation set: {len(val_df)} samples ({len(val_df)/len(df)*100:.1f}%)\")\n",
        "    print(f\"Test set: {len(test_df)} samples ({len(test_df)/len(df)*100:.1f}%)\")\n",
        "\n",
        "    return train_df, val_df, test_df\n",
        "\n",
        "def normalize_features(train_df, val_df, test_df, feature_cols):\n",
        "    \"\"\"\n",
        "    Normalize features using StandardScaler fitted on training data\n",
        "    \"\"\"\n",
        "    scaler = StandardScaler()\n",
        "\n",
        "    # Fit on training data\n",
        "    scaler.fit(train_df[feature_cols])\n",
        "\n",
        "    # Transform all datasets\n",
        "    train_scaled = scaler.transform(train_df[feature_cols])\n",
        "    val_scaled = scaler.transform(val_df[feature_cols])\n",
        "    test_scaled = scaler.transform(test_df[feature_cols])\n",
        "\n",
        "    # Convert back to DataFrames\n",
        "    train_norm = pd.DataFrame(train_scaled, columns=feature_cols, index=train_df.index)\n",
        "    val_norm = pd.DataFrame(val_scaled, columns=feature_cols, index=val_df.index)\n",
        "    test_norm = pd.DataFrame(test_scaled, columns=feature_cols, index=test_df.index)\n",
        "\n",
        "    return train_norm, val_norm, test_norm, scaler\n",
        "\n",
        "\n",
        "class WeatherDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset for weather forecasting with sliding window approach.\n",
        "    Input: sequence of weather data\n",
        "    Output: next time step(s) temperature\n",
        "    \"\"\"\n",
        "    def __init__(self, df, station_ids, feature_cols, seq_length=24, forecast_horizon=24):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            df: DataFrame with weather data\n",
        "            station_ids: List of station IDs\n",
        "            feature_cols: List of feature columns to use as input\n",
        "            seq_length: Length of input sequence (in hours)\n",
        "            forecast_horizon: How many hours ahead to predict\n",
        "        \"\"\"\n",
        "        self.df = df\n",
        "        self.station_ids = station_ids\n",
        "        self.feature_cols = feature_cols\n",
        "        self.seq_length = seq_length\n",
        "        self.forecast_horizon = forecast_horizon\n",
        "        self.n_stations = len(station_ids)\n",
        "\n",
        "        # Get unique timestamps\n",
        "        self.timestamps = sorted(df['timestamp'].unique())\n",
        "\n",
        "        # Filter valid timestamps (those that have enough history and future data)\n",
        "        valid_idx = []\n",
        "        for i in range(len(self.timestamps) - (seq_length + forecast_horizon - 1)):\n",
        "            # Check if we have continuous data for this window\n",
        "            current_time = self.timestamps[i]\n",
        "            end_time = self.timestamps[i + seq_length + forecast_horizon - 1]\n",
        "            expected_duration = timedelta(hours=seq_length + forecast_horizon - 1)\n",
        "\n",
        "            if (end_time - current_time) == expected_duration:\n",
        "                valid_idx.append(i)\n",
        "\n",
        "        self.valid_indices = valid_idx\n",
        "\n",
        "        # Add a safety check to ensure we have at least one valid window\n",
        "        if len(self.valid_indices) == 0:\n",
        "            print(f\"WARNING: No valid windows found in dataset. Using reduced requirements.\")\n",
        "            # Fall back to allowing any windows where we have both input and output data\n",
        "            valid_idx = []\n",
        "            for i in range(len(self.timestamps) - seq_length):\n",
        "                if i + seq_length < len(self.timestamps):\n",
        "                    valid_idx.append(i)\n",
        "            self.valid_indices = valid_idx\n",
        "            self.fallback_mode = True\n",
        "            print(f\"Found {len(self.valid_indices)} windows with relaxed continuity requirements\")\n",
        "        else:\n",
        "            self.fallback_mode = False\n",
        "            print(f\"Created dataset with {len(self.valid_indices)} valid windows\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return max(1, len(self.valid_indices))  # Ensure length is at least 1\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Safety check\n",
        "        if len(self.valid_indices) == 0:\n",
        "            # Return dummy data if no valid indices\n",
        "            X = np.zeros((len(self.feature_cols), self.n_stations, self.seq_length))\n",
        "            y = np.zeros((self.n_stations, self.forecast_horizon))\n",
        "            return torch.FloatTensor(X), torch.FloatTensor(y)\n",
        "\n",
        "        # Get actual data when possible\n",
        "        start_idx = self.valid_indices[idx % len(self.valid_indices)]\n",
        "\n",
        "        # Get timestamps for this window\n",
        "        input_timestamps = self.timestamps[start_idx:start_idx + self.seq_length]\n",
        "\n",
        "        # For target timestamps, handle potential fallback mode\n",
        "        if self.fallback_mode:\n",
        "            # In fallback mode, use whatever timestamps are available for targets\n",
        "            available_horizon = min(self.forecast_horizon,\n",
        "                                   len(self.timestamps) - (start_idx + self.seq_length))\n",
        "            target_timestamps = self.timestamps[start_idx + self.seq_length:\n",
        "                                              start_idx + self.seq_length + available_horizon]\n",
        "        else:\n",
        "            target_timestamps = self.timestamps[start_idx + self.seq_length:\n",
        "                                              start_idx + self.seq_length + self.forecast_horizon]\n",
        "\n",
        "        # Initialize tensors\n",
        "        # [features, stations, time]\n",
        "        X = np.zeros((len(self.feature_cols), self.n_stations, self.seq_length))\n",
        "        # [stations, forecast_horizon]\n",
        "        y = np.zeros((self.n_stations, self.forecast_horizon))\n",
        "\n",
        "        # Fill in data for each station\n",
        "        for s_idx, station_id in enumerate(self.station_ids):\n",
        "            # Input sequence\n",
        "            for t_idx, ts in enumerate(input_timestamps):\n",
        "                station_data = self.df[(self.df['timestamp'] == ts) &\n",
        "                                      (self.df['station_id'] == station_id)]\n",
        "\n",
        "                if not station_data.empty:\n",
        "                    for f_idx, feat in enumerate(self.feature_cols):\n",
        "                        X[f_idx, s_idx, t_idx] = station_data[feat].values[0]\n",
        "\n",
        "            # Target sequence (temperature only)\n",
        "            for t_idx, ts in enumerate(target_timestamps):\n",
        "                if t_idx < self.forecast_horizon:  # Safety check\n",
        "                    station_data = self.df[(self.df['timestamp'] == ts) &\n",
        "                                        (self.df['station_id'] == station_id)]\n",
        "\n",
        "                    if not station_data.empty:\n",
        "                        # Assuming 'Temperature_C' is the target\n",
        "                        y[s_idx, t_idx] = station_data['Temperature_C'].values[0]\n",
        "\n",
        "        return torch.FloatTensor(X), torch.FloatTensor(y)\n",
        "\n",
        "def create_dataloaders(train_df, val_df, test_df, station_ids, feature_cols,\n",
        "                       seq_length=24, forecast_horizon=24, batch_size=16):  # MODIFIED: smaller batch size\n",
        "    \"\"\"\n",
        "    Create PyTorch DataLoaders for train, validation and test sets\n",
        "    \"\"\"\n",
        "    # Create datasets\n",
        "    train_dataset = WeatherDataset(train_df, station_ids, feature_cols, seq_length, forecast_horizon)\n",
        "    val_dataset = WeatherDataset(val_df, station_ids, feature_cols, seq_length, forecast_horizon)\n",
        "    test_dataset = WeatherDataset(test_df, station_ids, feature_cols, seq_length, forecast_horizon)\n",
        "\n",
        "    # Create dataloaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    return train_loader, val_loader, test_loader\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, normalized_shape, eps=1e-5, elementwise_affine=True):\n",
        "        super(LayerNorm, self).__init__()\n",
        "        self.normalized_shape = normalized_shape\n",
        "        self.eps = eps\n",
        "        self.elementwise_affine = elementwise_affine\n",
        "        if self.elementwise_affine:\n",
        "            self.weight = nn.Parameter(torch.ones(normalized_shape))\n",
        "            self.bias = nn.Parameter(torch.zeros(normalized_shape))\n",
        "\n",
        "    def forward(self, x, idx=None):\n",
        "        if self.normalized_shape[0] == 1:\n",
        "            return x\n",
        "        if idx is None:\n",
        "            return F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
        "        else:\n",
        "            return F.layer_norm(x, self.normalized_shape,\n",
        "                                self.weight[:, idx, :], self.bias[:, idx, :], self.eps)\n",
        "\n",
        "class SimpleGraphConstructor(nn.Module):\n",
        "    \"\"\"\n",
        "    Simplified Graph Learning Layer\n",
        "    \"\"\"\n",
        "    def __init__(self, nnodes, dim, device, alpha=3):\n",
        "        super(SimpleGraphConstructor, self).__init__()\n",
        "        self.nnodes = nnodes\n",
        "        self.dim = dim\n",
        "        self.device = device\n",
        "        self.alpha = alpha\n",
        "\n",
        "        # Use node embeddings to learn the graph structure\n",
        "        self.emb1 = nn.Embedding(nnodes, dim)\n",
        "        self.emb2 = nn.Embedding(nnodes, dim)\n",
        "        self.lin1 = nn.Linear(dim, dim)\n",
        "        self.lin2 = nn.Linear(dim, dim)\n",
        "\n",
        "    def forward(self, idx):\n",
        "        # Get node embeddings\n",
        "        nodevec1 = self.emb1(idx)\n",
        "        nodevec2 = self.emb2(idx)\n",
        "\n",
        "        # Transform node embeddings\n",
        "        nodevec1 = torch.tanh(self.alpha * self.lin1(nodevec1))\n",
        "        nodevec2 = torch.tanh(self.alpha * self.lin2(nodevec2))\n",
        "\n",
        "        # Compute adjacency matrix\n",
        "        a = torch.mm(nodevec1, nodevec2.transpose(1, 0))\n",
        "        adj = F.softmax(F.relu(a), dim=1)\n",
        "\n",
        "        return adj\n",
        "\n",
        "class TemporalBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Simplified temporal convolution block with gating mechanism\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, dilation=1):\n",
        "        super(TemporalBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size,\n",
        "                              padding=dilation*(kernel_size-1)//2, dilation=dilation)\n",
        "        self.conv2 = nn.Conv1d(in_channels, out_channels, kernel_size,\n",
        "                              padding=dilation*(kernel_size-1)//2, dilation=dilation)\n",
        "        self.residual = nn.Conv1d(in_channels, out_channels, 1) if in_channels != out_channels else nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Gating mechanism\n",
        "        filter = torch.tanh(self.conv1(x))\n",
        "        gate = torch.sigmoid(self.conv2(x))\n",
        "        x = filter * gate\n",
        "\n",
        "        # Residual connection\n",
        "        res = self.residual(x)\n",
        "        return x + res\n",
        "\n",
        "class GraphConvolution(nn.Module):\n",
        "    \"\"\"\n",
        "    Simplified graph convolution layer\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(GraphConvolution, self).__init__()\n",
        "        self.linear = nn.Linear(in_channels, out_channels)\n",
        "\n",
        "    def forward(self, x, adj):\n",
        "        # Graph convolution\n",
        "        # x shape: [batch, stations, channels]\n",
        "        # adj shape: [stations, stations]\n",
        "\n",
        "        # Apply graph convolution for each item in the batch\n",
        "        batch_size = x.size(0)\n",
        "        n_stations = x.size(1)\n",
        "\n",
        "        # Reshape for matrix multiplication with adjacency matrix\n",
        "        x_reshaped = x.view(batch_size, n_stations, -1)\n",
        "\n",
        "        # Apply adjacency matrix - handle batching correctly\n",
        "        x = torch.bmm(adj.unsqueeze(0).expand(batch_size, -1, -1), x_reshaped)\n",
        "\n",
        "        # Apply linear transformation\n",
        "        x = self.linear(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class SimpleHierarchicalGNN(nn.Module):\n",
        "    \"\"\"\n",
        "    Simplified Hierarchical GNN for temperature forecasting\n",
        "    \"\"\"\n",
        "    def __init__(self, n_stations, seq_length, input_dim, forecast_horizon=24, hidden_dim=32,\n",
        "                 n_layers=2, dropout=0.1, node_dim=16, device='cuda'):\n",
        "        super(SimpleHierarchicalGNN, self).__init__()\n",
        "        self.n_stations = n_stations\n",
        "        self.seq_length = seq_length\n",
        "        self.forecast_horizon = forecast_horizon\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.dropout = dropout\n",
        "\n",
        "        # Input projection\n",
        "        self.start_conv = nn.Conv2d(input_dim, hidden_dim, kernel_size=(1, 1))\n",
        "\n",
        "        # Graph constructor for station relationships\n",
        "        self.graph_constructor = SimpleGraphConstructor(n_stations, node_dim, device)\n",
        "\n",
        "        # Temporal convolutional blocks\n",
        "        self.temporal_blocks = nn.ModuleList()\n",
        "        self.graph_convs = nn.ModuleList()\n",
        "        self.norms = nn.ModuleList()\n",
        "\n",
        "        for i in range(n_layers):\n",
        "            dilation = 2 ** i\n",
        "            self.temporal_blocks.append(\n",
        "                TemporalBlock(hidden_dim, hidden_dim, kernel_size=3, dilation=dilation)\n",
        "            )\n",
        "            self.graph_convs.append(\n",
        "                GraphConvolution(hidden_dim, hidden_dim)\n",
        "            )\n",
        "            self.norms.append(\n",
        "                LayerNorm((hidden_dim, seq_length - (dilation * 2)), elementwise_affine=True)\n",
        "            )\n",
        "\n",
        "        # Output projection for forecasting multiple steps\n",
        "        self.end_conv = nn.Conv2d(hidden_dim, forecast_horizon, kernel_size=(1, 1))\n",
        "\n",
        "        # Station indices for graph construction\n",
        "        self.stations_idx = torch.arange(n_stations).to(device)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x: Input tensor of shape [batch_size, features, stations, time]\n",
        "        returns: Predictions of shape [batch_size, stations, forecast_horizon]\n",
        "        \"\"\"\n",
        "        # Get batch size\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # Learn the graph structure among stations\n",
        "        adj = self.graph_constructor(self.stations_idx)\n",
        "\n",
        "        # Input projection\n",
        "        x = self.start_conv(x)\n",
        "\n",
        "        # Reshape for temporal convolutions [batch, channels, stations, time] -> [batch*stations, channels, time]\n",
        "        x = x.permute(0, 2, 1, 3).contiguous()\n",
        "        x = x.view(batch_size * self.n_stations, self.hidden_dim, -1)\n",
        "\n",
        "        skip_connections = []\n",
        "\n",
        "        # Process through layers\n",
        "        for i in range(self.n_layers):\n",
        "            # Temporal convolution\n",
        "            res = x\n",
        "            x = self.temporal_blocks[i](x)\n",
        "            x = F.dropout(x, self.dropout, training=self.training)\n",
        "\n",
        "            # Reshape for graph convolution [batch*stations, channels, time] -> [batch, stations, channels, time]\n",
        "            x_graph = x.view(batch_size, self.n_stations, self.hidden_dim, -1)\n",
        "\n",
        "            # Graph convolution for each time step\n",
        "            time_len = x_graph.size(-1)\n",
        "            x_spatial = []\n",
        "            for t in range(time_len):\n",
        "                # Apply graph convolution\n",
        "                x_t = x_graph[..., t]  # [batch, stations, channels]\n",
        "                x_t = self.graph_convs[i](x_t, adj)  # [batch, stations, channels]\n",
        "                x_spatial.append(x_t.unsqueeze(-1))\n",
        "\n",
        "            # Combine results from all time steps\n",
        "            x_spatial = torch.cat(x_spatial, dim=-1)  # [batch, stations, channels, time]\n",
        "\n",
        "            # Reshape back to [batch*stations, channels, time]\n",
        "            x = x_spatial.view(batch_size * self.n_stations, self.hidden_dim, -1)\n",
        "\n",
        "            # Add residual connection\n",
        "            x = res[:, :, -x.size(2):] + x\n",
        "\n",
        "            # Store for skip connection\n",
        "            skip_connections.append(x)\n",
        "\n",
        "        # Reshape back to [batch, channels, stations, time]\n",
        "        x = x.view(batch_size, self.n_stations, self.hidden_dim, -1)\n",
        "        x = x.permute(0, 2, 1, 3).contiguous()\n",
        "\n",
        "        # Output projection to get temperature prediction\n",
        "        x = self.end_conv(x)  # [batch, forecast_horizon, stations, remaining_time]\n",
        "\n",
        "        # Take the last time step for each forecast horizon\n",
        "        x = x[:, :, :, -1]  # [batch, forecast_horizon, stations]\n",
        "\n",
        "        # Reshape to [batch, stations, forecast_horizon]\n",
        "        x = x.permute(0, 2, 1)\n",
        "\n",
        "        return x\n",
        "\n",
        "def create_model(data_config, device='cuda'):\n",
        "    \"\"\"\n",
        "    Helper function to create the model based on your data configuration\n",
        "    \"\"\"\n",
        "    model = SimpleHierarchicalGNN(\n",
        "        n_stations=data_config['n_stations'],\n",
        "        seq_length=data_config['seq_length'],\n",
        "        input_dim=data_config['input_dim'],\n",
        "        forecast_horizon=data_config['forecast_horizon'],\n",
        "        hidden_dim=32,\n",
        "        n_layers=2,\n",
        "        dropout=0.2,\n",
        "        node_dim=16,\n",
        "        device=device\n",
        "    )\n",
        "    return model\n",
        "\n",
        "def train_epoch(model, train_loader, optimizer, criterion, device):\n",
        "    \"\"\"Train the model for one epoch\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    epoch_start_time = time.time()\n",
        "    batch_times = []\n",
        "\n",
        "    for batch_idx, (X, y) in enumerate(train_loader):\n",
        "        batch_start_time = time.time()\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        optimizer.zero_grad()\n",
        "        y_pred = model(X)\n",
        "        loss = criterion(y_pred, y)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Calculate batch time\n",
        "        batch_end_time = time.time()\n",
        "        batch_time = batch_end_time - batch_start_time\n",
        "        batch_times.append(batch_time)\n",
        "\n",
        "        # Print progress every 10 batches\n",
        "        if (batch_idx + 1) % 10 == 0 or (batch_idx + 1) == len(train_loader):\n",
        "            print(f\"  Batch {batch_idx+1}/{len(train_loader)}: Loss = {loss.item():.4f}, Time = {batch_time:.2f}s\")\n",
        "\n",
        "    epoch_time = time.time() - epoch_start_time\n",
        "    avg_batch_time = sum(batch_times) / len(batch_times)\n",
        "    print(f\"  Epoch completed in {epoch_time:.2f}s, Avg batch time: {avg_batch_time:.2f}s\")\n",
        "\n",
        "    return total_loss / len(train_loader)\n",
        "\n",
        "def validate(model, val_loader, criterion, device):\n",
        "    \"\"\"Validate the model\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in val_loader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            y_pred = model(X)\n",
        "            loss = criterion(y_pred, y)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(val_loader)\n",
        "\n",
        "def evaluate(model, test_loader, scaler, device):\n",
        "    \"\"\"Evaluate the model and compute metrics\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in test_loader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            y_pred = model(X)\n",
        "\n",
        "            # Move to CPU for numpy conversion\n",
        "            all_preds.append(y_pred.cpu().numpy())\n",
        "            all_targets.append(y.cpu().numpy())\n",
        "\n",
        "    # Concatenate batches\n",
        "    all_preds = np.concatenate(all_preds, axis=0)\n",
        "    all_targets = np.concatenate(all_targets, axis=0)\n",
        "\n",
        "    # Calculate metrics for each station and forecast horizon\n",
        "    n_stations = all_preds.shape[1]\n",
        "    forecast_horizon = all_preds.shape[2]\n",
        "\n",
        "    # Initialize metrics containers\n",
        "    mae_by_station = np.zeros((n_stations, forecast_horizon))\n",
        "    rmse_by_station = np.zeros((n_stations, forecast_horizon))\n",
        "    r2_by_station = np.zeros((n_stations, forecast_horizon))\n",
        "\n",
        "    # Calculate metrics\n",
        "    for s in range(n_stations):\n",
        "        for h in range(forecast_horizon):\n",
        "            y_true = all_targets[:, s, h]\n",
        "            y_pred = all_preds[:, s, h]\n",
        "\n",
        "            # Skip if all values are identical (can happen with missing data)\n",
        "            if np.std(y_true) == 0:\n",
        "                continue\n",
        "\n",
        "            mae_by_station[s, h] = mean_absolute_error(y_true, y_pred)\n",
        "            rmse_by_station[s, h] = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "            r2_by_station[s, h] = r2_score(y_true, y_pred)\n",
        "\n",
        "    # Calculate average metrics across stations\n",
        "    avg_mae_by_horizon = np.mean(mae_by_station, axis=0)\n",
        "    avg_rmse_by_horizon = np.mean(rmse_by_station, axis=0)\n",
        "    avg_r2_by_horizon = np.mean(r2_by_station, axis=0)\n",
        "\n",
        "    # Overall averages\n",
        "    overall_mae = np.mean(mae_by_station)\n",
        "    overall_rmse = np.mean(rmse_by_station)\n",
        "    overall_r2 = np.mean(r2_by_station)\n",
        "\n",
        "    metrics = {\n",
        "        'mae_by_station': mae_by_station,\n",
        "        'rmse_by_station': rmse_by_station,\n",
        "        'r2_by_station': r2_by_station,\n",
        "        'avg_mae_by_horizon': avg_mae_by_horizon,\n",
        "        'avg_rmse_by_horizon': avg_rmse_by_horizon,\n",
        "        'avg_r2_by_horizon': avg_r2_by_horizon,\n",
        "        'overall_mae': overall_mae,\n",
        "        'overall_rmse': overall_rmse,\n",
        "        'overall_r2': overall_r2\n",
        "    }\n",
        "\n",
        "    return metrics, all_preds, all_targets\n",
        "\n",
        "def plot_training_history(train_losses, val_losses):\n",
        "    \"\"\"Plot training and validation losses\"\"\"\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(train_losses, label='Training Loss')\n",
        "    plt.plot(val_losses, label='Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "def plot_forecast_by_station(predictions, targets, station_names, timestamp=0, forecast_horizon=24):\n",
        "    \"\"\"\n",
        "    Plot predictions vs. actual for each station\n",
        "    Args:\n",
        "        predictions: numpy array [batch, stations, forecast_horizon]\n",
        "        targets: numpy array [batch, stations, forecast_horizon]\n",
        "        station_names: list of station names\n",
        "        timestamp: which batch index to plot\n",
        "    \"\"\"\n",
        "    n_stations = len(station_names)\n",
        "    n_cols = min(3, n_stations)\n",
        "    n_rows = (n_stations + n_cols - 1) // n_cols\n",
        "\n",
        "    plt.figure(figsize=(15, n_rows * 4))\n",
        "\n",
        "    for i, station in enumerate(station_names):\n",
        "        plt.subplot(n_rows, n_cols, i+1)\n",
        "\n",
        "        # Get data for this station\n",
        "        pred = predictions[timestamp, i, :]\n",
        "        actual = targets[timestamp, i, :]\n",
        "\n",
        "        # Plot\n",
        "        hours = np.arange(forecast_horizon)\n",
        "        plt.plot(hours, actual, 'b-', label='Actual')\n",
        "        plt.plot(hours, pred, 'r--', label='Predicted')\n",
        "\n",
        "        plt.title(f'Station: {station}')\n",
        "        plt.xlabel('Hours ahead')\n",
        "        plt.ylabel('Temperature (Â°C)')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_metrics_by_horizon(metrics):\n",
        "    \"\"\"Plot metrics by forecast horizon\"\"\"\n",
        "    forecast_horizon = len(metrics['avg_mae_by_horizon'])\n",
        "\n",
        "    plt.figure(figsize=(15, 5))\n",
        "\n",
        "    # Plot MAE\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.plot(range(1, forecast_horizon+1), metrics['avg_mae_by_horizon'])\n",
        "    plt.title('MAE by Forecast Horizon')\n",
        "    plt.xlabel('Hours Ahead')\n",
        "    plt.ylabel('MAE (Â°C)')\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Plot RMSE\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.plot(range(1, forecast_horizon+1), metrics['avg_rmse_by_horizon'])\n",
        "    plt.title('RMSE by Forecast Horizon')\n",
        "    plt.xlabel('Hours Ahead')\n",
        "    plt.ylabel('RMSE (Â°C)')\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Plot RÂ²\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.plot(range(1, forecast_horizon+1), metrics['avg_r2_by_horizon'])\n",
        "    plt.title('RÂ² by Forecast Horizon')\n",
        "    plt.xlabel('Hours Ahead')\n",
        "    plt.ylabel('RÂ²')\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_station_performance_heatmap(metrics, station_names):\n",
        "    \"\"\"Plot heatmap of metrics by station and forecast horizon\"\"\"\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
        "\n",
        "    # MAE heatmap\n",
        "    sns.heatmap(metrics['mae_by_station'], ax=axes[0], cmap='YlOrRd',\n",
        "                xticklabels=np.arange(1, metrics['mae_by_station'].shape[1]+1, 4),\n",
        "                yticklabels=station_names, annot=False)\n",
        "    axes[0].set_title('MAE by Station and Horizon')\n",
        "    axes[0].set_xlabel('Hours Ahead')\n",
        "    axes[0].set_ylabel('Station')\n",
        "\n",
        "    # RMSE heatmap\n",
        "    sns.heatmap(metrics['rmse_by_station'], ax=axes[1], cmap='YlOrRd',\n",
        "                xticklabels=np.arange(1, metrics['rmse_by_station'].shape[1]+1, 4),\n",
        "                yticklabels=station_names, annot=False)\n",
        "    axes[1].set_title('RMSE by Station and Horizon')\n",
        "    axes[1].set_xlabel('Hours Ahead')\n",
        "    axes[1].set_ylabel('Station')\n",
        "\n",
        "    # RÂ² heatmap\n",
        "    sns.heatmap(metrics['r2_by_station'], ax=axes[2], cmap='YlGnBu',\n",
        "                xticklabels=np.arange(1, metrics['r2_by_station'].shape[1]+1, 4),\n",
        "                yticklabels=station_names, annot=False)\n",
        "    axes[2].set_title('RÂ² by Station and Horizon')\n",
        "    axes[2].set_xlabel('Hours Ahead')\n",
        "    axes[2].set_ylabel('Station')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def analyze_by_region(metrics, station_names, station_regions):\n",
        "    \"\"\"Analyze performance by region\"\"\"\n",
        "    unique_regions = np.unique(station_regions)\n",
        "    n_regions = len(unique_regions)\n",
        "\n",
        "    # Initialize containers\n",
        "    mae_by_region = np.zeros((n_regions, metrics['mae_by_station'].shape[1]))\n",
        "    rmse_by_region = np.zeros((n_regions, metrics['rmse_by_station'].shape[1]))\n",
        "    r2_by_region = np.zeros((n_regions, metrics['r2_by_station'].shape[1]))\n",
        "\n",
        "    # Group by region\n",
        "    for i, region in enumerate(unique_regions):\n",
        "        region_stations = [j for j, r in enumerate(station_regions) if r == region]\n",
        "\n",
        "        mae_by_region[i] = np.mean(metrics['mae_by_station'][region_stations], axis=0)\n",
        "        rmse_by_region[i] = np.mean(metrics['rmse_by_station'][region_stations], axis=0)\n",
        "        r2_by_region[i] = np.mean(metrics['r2_by_station'][region_stations], axis=0)\n",
        "\n",
        "    # Plot results\n",
        "    plt.figure(figsize=(15, 5))\n",
        "\n",
        "    # Plot MAE\n",
        "    plt.subplot(1, 3, 1)\n",
        "    for i, region in enumerate(unique_regions):\n",
        "        plt.plot(range(1, mae_by_region.shape[1]+1), mae_by_region[i], label=region)\n",
        "    plt.title('MAE by Region')\n",
        "    plt.xlabel('Hours Ahead')\n",
        "    plt.ylabel('MAE (Â°C)')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Plot RMSE\n",
        "    plt.subplot(1, 3, 2)\n",
        "    for i, region in enumerate(unique_regions):\n",
        "        plt.plot(range(1, rmse_by_region.shape[1]+1), rmse_by_region[i], label=region)\n",
        "    plt.title('RMSE by Region')\n",
        "    plt.xlabel('Hours Ahead')\n",
        "    plt.ylabel('RMSE (Â°C)')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Plot RÂ²\n",
        "    plt.subplot(1, 3, 3)\n",
        "    for i, region in enumerate(unique_regions):\n",
        "        plt.plot(range(1, r2_by_region.shape[1]+1), r2_by_region[i], label=region)\n",
        "    plt.title('RÂ² by Region')\n",
        "    plt.xlabel('Hours Ahead')\n",
        "    plt.ylabel('RÂ²')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Return region metrics\n",
        "    region_metrics = {\n",
        "        'mae_by_region': mae_by_region,\n",
        "        'rmse_by_region': rmse_by_region,\n",
        "        'r2_by_region': r2_by_region,\n",
        "        'region_names': unique_regions\n",
        "    }\n",
        "\n",
        "    return region_metrics\n",
        "\n",
        "\n",
        "\n",
        "def run_weather_forecasting_pipeline(data_path, output_dir='./model_outputs'):\n",
        "    \"\"\"\n",
        "    Main function to run the entire weather forecasting pipeline\n",
        "    \"\"\"\n",
        "    print(\"Starting weather forecasting pipeline...\")\n",
        "    print(f\"Using data from: {data_path}\")\n",
        "\n",
        "    # Create output directory if it doesn't exist\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    try:\n",
        "        #----------------\n",
        "        # 1. Data Loading\n",
        "        #----------------\n",
        "        print(\"\\n1. Loading and preprocessing data...\")\n",
        "        df = load_data(data_path)\n",
        "\n",
        "        # Add a check to ensure there's a station_id column\n",
        "        if 'station_id' not in df.columns:\n",
        "            # If missing, try to create one from existing columns or use default\n",
        "            if 'STATION' in df.columns:\n",
        "                df['station_id'] = df['STATION']\n",
        "            else:\n",
        "                print(\"WARNING: No station_id column found, creating a default one\")\n",
        "                df['station_id'] = 'STATION1'  # Default station ID\n",
        "\n",
        "        # Add a check to ensure there's a region column\n",
        "        if 'region' not in df.columns:\n",
        "            print(\"WARNING: No region column found, creating a default one\")\n",
        "            df['region'] = 'default_region'  # Default region\n",
        "\n",
        "        # Continue with normal pipeline\n",
        "        train_df, val_df, test_df = split_data_by_time(df)\n",
        "\n",
        "        # Get unique stations and their regions\n",
        "        station_ids = df['station_id'].unique()\n",
        "        n_stations = len(station_ids)\n",
        "\n",
        "        # Create a mapping of stations to regions\n",
        "        station_to_region = {}\n",
        "        for station in station_ids:\n",
        "            region = df[df['station_id'] == station]['region'].iloc[0]\n",
        "            station_to_region[station] = region\n",
        "\n",
        "        # List of regions for each station (in order)\n",
        "        station_regions = [station_to_region[s] for s in station_ids]\n",
        "\n",
        "        print(f\"Found {n_stations} stations across {len(set(station_regions))} regions\")\n",
        "\n",
        "        # Define features to use\n",
        "        # Check if we have all required columns\n",
        "        required_cols = ['HourlyRelativeHumidity', 'HourlyStationPressure', 'Temperature_C']\n",
        "        available_cols = [col for col in required_cols if col in df.columns]\n",
        "\n",
        "        if len(available_cols) < len(required_cols):\n",
        "            missing_cols = set(required_cols) - set(available_cols)\n",
        "            print(f\"WARNING: Missing columns: {missing_cols}\")\n",
        "            # Try to find substitutes for missing columns\n",
        "            if 'Temperature_C' not in available_cols:\n",
        "                if 'HourlyDryBulbTemperature' in df.columns:\n",
        "                    print(\"Using HourlyDryBulbTemperature in place of Temperature_C\")\n",
        "                    df['Temperature_C'] = df['HourlyDryBulbTemperature']\n",
        "                    available_cols.append('Temperature_C')\n",
        "                elif 'TEMP' in df.columns:\n",
        "                    print(\"Using TEMP in place of Temperature_C\")\n",
        "                    df['Temperature_C'] = df['TEMP']\n",
        "                    available_cols.append('Temperature_C')\n",
        "\n",
        "            if 'HourlyRelativeHumidity' not in available_cols and 'HUMIDITY' in df.columns:\n",
        "                print(\"Using HUMIDITY in place of HourlyRelativeHumidity\")\n",
        "                df['HourlyRelativeHumidity'] = df['HUMIDITY']\n",
        "                available_cols.append('HourlyRelativeHumidity')\n",
        "\n",
        "            if 'HourlyStationPressure' not in available_cols and 'PRESSURE' in df.columns:\n",
        "                print(\"Using PRESSURE in place of HourlyStationPressure\")\n",
        "                df['HourlyStationPressure'] = df['PRESSURE']\n",
        "                available_cols.append('HourlyStationPressure')\n",
        "\n",
        "        # Add hour_of_day and day_of_year to available columns\n",
        "        if 'hour_of_day' in df.columns:\n",
        "            available_cols.append('hour_of_day')\n",
        "        if 'day_of_year' in df.columns:\n",
        "            available_cols.append('day_of_year')\n",
        "\n",
        "        feature_cols = available_cols\n",
        "        print(f\"Using features: {feature_cols}\")\n",
        "\n",
        "        # Verify we have the Temperature_C column\n",
        "        if 'Temperature_C' not in feature_cols:\n",
        "            raise ValueError(\"Required column 'Temperature_C' is missing and no suitable replacement found\")\n",
        "\n",
        "        # Normalize features\n",
        "        train_norm, val_norm, test_norm, scaler = normalize_features(\n",
        "            train_df, val_df, test_df, feature_cols)\n",
        "\n",
        "        # Add normalized features back to dataframes\n",
        "        for col in feature_cols:\n",
        "            train_df[col] = train_norm[col]\n",
        "            val_df[col] = val_norm[col]\n",
        "            test_df[col] = test_norm[col]\n",
        "\n",
        "        #----------------\n",
        "        # 2. Create DataLoaders\n",
        "        #----------------\n",
        "        print(\"\\n2. Creating DataLoaders...\")\n",
        "\n",
        "        # Define data configuration\n",
        "        seq_length = 24  # 24 hours of input\n",
        "        forecast_horizon = 24  # predict next 24 hours\n",
        "        # CHANGED: Smaller batch size for faster training\n",
        "        batch_size = 16  # Changed from 32\n",
        "\n",
        "        # Create DataLoaders\n",
        "        train_loader, val_loader, test_loader = create_dataloaders(\n",
        "            train_df, val_df, test_df, station_ids, feature_cols,\n",
        "            seq_length=seq_length, forecast_horizon=forecast_horizon, batch_size=batch_size\n",
        "        )\n",
        "\n",
        "        #----------------\n",
        "        # 3. Create Model\n",
        "        #----------------\n",
        "        print(\"\\n3. Creating model...\")\n",
        "\n",
        "        # Define model configuration\n",
        "        data_config = {\n",
        "            'n_stations': n_stations,\n",
        "            'seq_length': seq_length,\n",
        "            'input_dim': len(feature_cols),\n",
        "            'forecast_horizon': forecast_horizon\n",
        "        }\n",
        "\n",
        "        # Create model\n",
        "        model = create_model(data_config, device)\n",
        "        model.to(device)\n",
        "\n",
        "        print(f\"Model created with {sum(p.numel() for p in model.parameters())} parameters\")\n",
        "\n",
        "        #----------------\n",
        "        # 4. Train Model\n",
        "        #----------------\n",
        "        print(\"\\n4. Training model...\")\n",
        "\n",
        "      # Define training parameters\n",
        "        # CHANGED: Reduced epochs\n",
        "        num_epochs = 20  # Changed from 30\n",
        "        learning_rate = 0.001\n",
        "\n",
        "        # Define loss function and optimizer\n",
        "        criterion = nn.MSELoss()\n",
        "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, factor=0.5)\n",
        "\n",
        "        # Training loop\n",
        "        train_losses = []\n",
        "        val_losses = []\n",
        "        best_val_loss = float('inf')\n",
        "        best_model_path = os.path.join(output_dir, 'best_model.pth')\n",
        "\n",
        "        print(\"Beginning training...\")\n",
        "        total_start_time = time.time()\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            epoch_start_time = time.time()\n",
        "            print(f\"Epoch {epoch+1}/{num_epochs}:\")\n",
        "\n",
        "            # Train\n",
        "            train_loss = train_epoch(model, train_loader, optimizer, criterion, device)\n",
        "            train_losses.append(train_loss)\n",
        "\n",
        "            # Validate\n",
        "            val_start_time = time.time()\n",
        "            val_loss = validate(model, val_loader, criterion, device)\n",
        "            val_time = time.time() - val_start_time\n",
        "            val_losses.append(val_loss)\n",
        "\n",
        "            # Update learning rate\n",
        "            scheduler.step(val_loss)\n",
        "\n",
        "            # Calculate epoch time\n",
        "            epoch_time = time.time() - epoch_start_time\n",
        "\n",
        "            # Save best model\n",
        "            if val_loss < best_val_loss:\n",
        "                best_val_loss = val_loss\n",
        "                torch.save(model.state_dict(), best_model_path)\n",
        "                print(f\"  Results: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f} (best)\")\n",
        "            else:\n",
        "                print(f\"  Results: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}\")\n",
        "\n",
        "            print(f\"  Epoch time: {epoch_time:.2f}s (Validation: {val_time:.2f}s)\")\n",
        "            print(\"-\" * 50)\n",
        "\n",
        "        total_training_time = time.time() - total_start_time\n",
        "        print(f\"Total training time: {total_training_time:.2f}s ({total_training_time/60:.2f} minutes)\")\n",
        "\n",
        "        # Plot training history\n",
        "        plot_training_history(train_losses, val_losses)\n",
        "\n",
        "        # Load best model\n",
        "        model.load_state_dict(torch.load(best_model_path))\n",
        "\n",
        "        #----------------\n",
        "        # 5. Evaluate Model\n",
        "        #----------------\n",
        "        print(\"\\n5. Evaluating model...\")\n",
        "\n",
        "        # Evaluate on test set\n",
        "        metrics, all_preds, all_targets = evaluate(model, test_loader, scaler, device)\n",
        "\n",
        "        # Print overall metrics\n",
        "        print(f\"Overall MAE: {metrics['overall_mae']:.4f}Â°C\")\n",
        "        print(f\"Overall RMSE: {metrics['overall_rmse']:.4f}Â°C\")\n",
        "        print(f\"Overall RÂ²: {metrics['overall_r2']:.4f}\")\n",
        "\n",
        "        # Plot metrics by forecast horizon\n",
        "        plot_metrics_by_horizon(metrics)\n",
        "\n",
        "        # Plot station performance\n",
        "        plot_station_performance_heatmap(metrics, station_ids)\n",
        "\n",
        "        # Analyze by region\n",
        "        region_metrics = analyze_by_region(metrics, station_ids, station_regions)\n",
        "\n",
        "        # Plot sample forecasts\n",
        "        print(\"\\n6. Plotting sample forecasts...\")\n",
        "        plot_forecast_by_station(all_preds, all_targets, station_ids, timestamp=0)\n",
        "\n",
        "        print(\"\\nWeather forecasting pipeline completed!\")\n",
        "        return model, metrics, region_metrics\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in pipeline: {str(e)}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return None, None, None\n",
        "\n",
        "# Example usage:\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # Run the pipeline\n",
        "    model, metrics, region_metrics = run_weather_forecasting_pipeline(DATA_PATH)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0SJBQxI5z7uX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}