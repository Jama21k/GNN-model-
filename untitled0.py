# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PvfcmulxWGxNi5VT5ikprtRnrHEDjAK8
"""

pip install openpyxl

# @title **Initial Setup Cell** (Run First)
import os

# 1. Upload your data file via file explorer
print("Please upload your data file:")
from google.colab import files
uploaded = files.upload()
DATA_PATH = list(uploaded.keys())[0]  # Automatically gets filename

# 2. Set checkpoint directory (local to Colab session)
CHECKPOINT_DIR = "/content/weather_checkpoints"
os.makedirs(CHECKPOINT_DIR, exist_ok=True)

# 3. Verify GPU
import torch
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")



print("âœ… All packages installed successfully!")

# ============================================================
# @title **ðŸ“Š Load and Process Data**
# ============================================================
# ============================================================
# @title **ðŸ“¥ Import Dependencies**
# ============================================================

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import os
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')

"""# Running Model

"""

def load_data(file_path):
    """
    Load weather data, clean missing values, and interpolate along the temporal dimension.
    Handles 'VRB' in HourlyWindDirection by replacing it with NaN.
    """
    # Determine file type and read
    if file_path.endswith('.xlsx'):
        df = pd.read_excel(file_path, engine='openpyxl')
    else:
        df = pd.read_csv(file_path, encoding='ISO-8859-1')

    # Convert timestamp
    if 'timestamp' in df.columns:
        df['timestamp'] = pd.to_datetime(df['timestamp'])
    elif 'DATE' in df.columns:
        df['timestamp'] = pd.to_datetime(df['DATE'])
        df = df.rename(columns={'DATE': 'date_original'})


    # Ensure time-based ordering before interpolation
    df = df.sort_values(by='timestamp')

    # Add hour of day feature
    df['hour_of_day'] = df['timestamp'].dt.hour / 23.0  # Normalize to [0,1]

    # Add day of year feature (normalized)
    df['day_of_year'] = df['timestamp'].dt.dayofyear / 365.0  # Normalize to [0,1]

    # Add region information (assuming station_id or similar column exists)
    # This would be replaced with actual mapping in your implementation
    if 'station_id' in df.columns:
        # Example mapping - you would replace this with your actual regions
        region_mapping = {
            # Coastal stations
            'San Francisco': 'coastal',
            'San Diego': 'coastal',
            # Mountain stations
            'Mammoth Lakes': 'mountain',
            'South Lake Tahoe': 'mountain',
            # Desert stations
            'Palm Spring': 'desert',
            'Barstow': 'desert',
            # Valley stations
            'Fresno': 'valley',
            'Sacramento': 'valley',
            # Urban stations
            'LA Downtown': 'urban',
            'San Jose': 'urban'
        }
        df['region'] = df['station_id'].map(region_mapping)

    # Interpolate missing values along the time dimension
    df.interpolate(method='linear', limit_direction='both', inplace=True)

    return df

def split_data_by_time(df):
    """
    Split data into training (2021-2023), validation (Jan-Jun 2024),
    and testing (Jul-Dec 2024) sets.
    """
    # Create year and month columns for easy filtering
    df['year'] = df['timestamp'].dt.year
    df['month'] = df['timestamp'].dt.month

    # Training: 2021-2023
    train_df = df[(df['year'] >= 2021) & (df['year'] <= 2023)]

    # Validation: Jan-Jun 2024
    val_df = df[(df['year'] == 2024) & (df['month'] <= 6)]

    # Testing: Jul-Dec 2024
    test_df = df[(df['year'] == 2024) & (df['month'] > 6)]

    print(f"Train set: {len(train_df)} samples ({len(train_df)/len(df)*100:.1f}%)")
    print(f"Validation set: {len(val_df)} samples ({len(val_df)/len(df)*100:.1f}%)")
    print(f"Test set: {len(test_df)} samples ({len(test_df)/len(df)*100:.1f}%)")

    return train_df, val_df, test_df

def normalize_features(train_df, val_df, test_df, feature_cols):
    """
    Normalize features using StandardScaler fitted on training data
    """
    scaler = StandardScaler()

    # Fit on training data
    scaler.fit(train_df[feature_cols])

    # Transform all datasets
    train_scaled = scaler.transform(train_df[feature_cols])
    val_scaled = scaler.transform(val_df[feature_cols])
    test_scaled = scaler.transform(test_df[feature_cols])

    # Convert back to DataFrames
    train_norm = pd.DataFrame(train_scaled, columns=feature_cols, index=train_df.index)
    val_norm = pd.DataFrame(val_scaled, columns=feature_cols, index=val_df.index)
    test_norm = pd.DataFrame(test_scaled, columns=feature_cols, index=test_df.index)

    return train_norm, val_norm, test_norm, scaler

class WeatherDataset(Dataset):
    """
    Dataset for weather forecasting with sliding window approach.
    Input: sequence of weather data
    Output: next time step(s) temperature
    """
    def __init__(self, df, station_ids, feature_cols, seq_length=24, forecast_horizon=24):
        """
        Args:
            df: DataFrame with weather data
            station_ids: List of station IDs
            feature_cols: List of feature columns to use as input
            seq_length: Length of input sequence (in hours)
            forecast_horizon: How many hours ahead to predict
        """
        self.df = df
        self.station_ids = station_ids
        self.feature_cols = feature_cols
        self.seq_length = seq_length
        self.forecast_horizon = forecast_horizon
        self.n_stations = len(station_ids)

        # Get unique timestamps
        self.timestamps = sorted(df['timestamp'].unique())

        # Filter valid timestamps (those that have enough history and future data)
        valid_idx = []
        for i in range(len(self.timestamps) - (seq_length + forecast_horizon - 1)):
            # Check if we have continuous data for this window
            current_time = self.timestamps[i]
            end_time = self.timestamps[i + seq_length + forecast_horizon - 1]
            expected_duration = timedelta(hours=seq_length + forecast_horizon - 1)

            if (end_time - current_time) == expected_duration:
                valid_idx.append(i)

        self.valid_indices = valid_idx
        print(f"Created dataset with {len(self.valid_indices)} valid windows")

    def __len__(self):
        return len(self.valid_indices)

    def __getitem__(self, idx):
        start_idx = self.valid_indices[idx]

        # Get timestamps for this window
        input_timestamps = self.timestamps[start_idx:start_idx + self.seq_length]
        target_timestamps = self.timestamps[start_idx + self.seq_length:
                                           start_idx + self.seq_length + self.forecast_horizon]

        # Initialize tensors
        # [features, stations, time]
        X = np.zeros((len(self.feature_cols), self.n_stations, self.seq_length))
        # [stations, forecast_horizon]
        y = np.zeros((self.n_stations, self.forecast_horizon))

        # Fill in data for each station
        for s_idx, station_id in enumerate(self.station_ids):
            # Input sequence
            for t_idx, ts in enumerate(input_timestamps):
                station_data = self.df[(self.df['timestamp'] == ts) &
                                      (self.df['station_id'] == station_id)]

                if not station_data.empty:
                    for f_idx, feat in enumerate(self.feature_cols):
                        X[f_idx, s_idx, t_idx] = station_data[feat].values[0]

            # Target sequence (temperature only)
            for t_idx, ts in enumerate(target_timestamps):
                station_data = self.df[(self.df['timestamp'] == ts) &
                                      (self.df['station_id'] == station_id)]

                if not station_data.empty:
                    # Assuming 'Temperature_C' is the target
                    y[s_idx, t_idx] = station_data['Temperature_C'].values[0]

        return torch.FloatTensor(X), torch.FloatTensor(y)

def create_dataloaders(train_df, val_df, test_df, station_ids, feature_cols,
                       seq_length=24, forecast_horizon=24, batch_size=32):
    """
    Create PyTorch DataLoaders for train, validation and test sets
    """
    # Create datasets
    train_dataset = WeatherDataset(train_df, station_ids, feature_cols, seq_length, forecast_horizon)
    val_dataset = WeatherDataset(val_df, station_ids, feature_cols, seq_length, forecast_horizon)
    test_dataset = WeatherDataset(test_df, station_ids, feature_cols, seq_length, forecast_horizon)

    # Create dataloaders
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

    return train_loader, val_loader, test_loader

class WeatherDataset(Dataset):
    """
    Dataset for weather forecasting with sliding window approach.
    Input: sequence of weather data
    Output: next time step(s) temperature
    """
    def __init__(self, df, station_ids, feature_cols, seq_length=24, forecast_horizon=24):
        """
        Args:
            df: DataFrame with weather data
            station_ids: List of station IDs
            feature_cols: List of feature columns to use as input
            seq_length: Length of input sequence (in hours)
            forecast_horizon: How many hours ahead to predict
        """
        self.df = df
        self.station_ids = station_ids
        self.feature_cols = feature_cols
        self.seq_length = seq_length
        self.forecast_horizon = forecast_horizon
        self.n_stations = len(station_ids)

        # Get unique timestamps
        self.timestamps = sorted(df['timestamp'].unique())

        # Filter valid timestamps (those that have enough history and future data)
        valid_idx = []
        for i in range(len(self.timestamps) - (seq_length + forecast_horizon - 1)):
            # Check if we have continuous data for this window
            current_time = self.timestamps[i]
            end_time = self.timestamps[i + seq_length + forecast_horizon - 1]
            expected_duration = timedelta(hours=seq_length + forecast_horizon - 1)

            if (end_time - current_time) == expected_duration:
                valid_idx.append(i)

        self.valid_indices = valid_idx

        # Add a safety check to ensure we have at least one valid window
        if len(self.valid_indices) == 0:
            print(f"WARNING: No valid windows found in dataset. Using reduced requirements.")
            # Fall back to allowing any windows where we have both input and output data
            valid_idx = []
            for i in range(len(self.timestamps) - seq_length):
                if i + seq_length < len(self.timestamps):
                    valid_idx.append(i)
            self.valid_indices = valid_idx
            self.fallback_mode = True
            print(f"Found {len(self.valid_indices)} windows with relaxed continuity requirements")
        else:
            self.fallback_mode = False
            print(f"Created dataset with {len(self.valid_indices)} valid windows")

    def __len__(self):
        return max(1, len(self.valid_indices))  # Ensure length is at least 1

    def __getitem__(self, idx):
        # Safety check
        if len(self.valid_indices) == 0:
            # Return dummy data if no valid indices
            X = np.zeros((len(self.feature_cols), self.n_stations, self.seq_length))
            y = np.zeros((self.n_stations, self.forecast_horizon))
            return torch.FloatTensor(X), torch.FloatTensor(y)

        # Get actual data when possible
        start_idx = self.valid_indices[idx % len(self.valid_indices)]

        # Get timestamps for this window
        input_timestamps = self.timestamps[start_idx:start_idx + self.seq_length]

        # For target timestamps, handle potential fallback mode
        if self.fallback_mode:
            # In fallback mode, use whatever timestamps are available for targets
            available_horizon = min(self.forecast_horizon,
                                   len(self.timestamps) - (start_idx + self.seq_length))
            target_timestamps = self.timestamps[start_idx + self.seq_length:
                                              start_idx + self.seq_length + available_horizon]
        else:
            target_timestamps = self.timestamps[start_idx + self.seq_length:
                                              start_idx + self.seq_length + self.forecast_horizon]

        # Initialize tensors
        # [features, stations, time]
        X = np.zeros((len(self.feature_cols), self.n_stations, self.seq_length))
        # [stations, forecast_horizon]
        y = np.zeros((self.n_stations, self.forecast_horizon))

        # Fill in data for each station
        for s_idx, station_id in enumerate(self.station_ids):
            # Input sequence
            for t_idx, ts in enumerate(input_timestamps):
                station_data = self.df[(self.df['timestamp'] == ts) &
                                      (self.df['station_id'] == station_id)]

                if not station_data.empty:
                    for f_idx, feat in enumerate(self.feature_cols):
                        X[f_idx, s_idx, t_idx] = station_data[feat].values[0]

            # Target sequence (temperature only)
            for t_idx, ts in enumerate(target_timestamps):
                if t_idx < self.forecast_horizon:  # Safety check
                    station_data = self.df[(self.df['timestamp'] == ts) &
                                        (self.df['station_id'] == station_id)]

                    if not station_data.empty:
                        # Assuming 'Temperature_C' is the target
                        y[s_idx, t_idx] = station_data['Temperature_C'].values[0]

        return torch.FloatTensor(X), torch.FloatTensor(y)

def create_dataloaders(train_df, val_df, test_df, station_ids, feature_cols,
                       seq_length=24, forecast_horizon=24, batch_size=32):
    """
    Create PyTorch DataLoaders for train, validation and test sets
    """
    # Create datasets
    train_dataset = WeatherDataset(train_df, station_ids, feature_cols, seq_length, forecast_horizon)
    val_dataset = WeatherDataset(val_df, station_ids, feature_cols, seq_length, forecast_horizon)
    test_dataset = WeatherDataset(test_df, station_ids, feature_cols, seq_length, forecast_horizon)

    # Create dataloaders
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

    return train_loader, val_loader, test_loader

class LayerNorm(nn.Module):
    def __init__(self, normalized_shape, eps=1e-5, elementwise_affine=True):
        super(LayerNorm, self).__init__()
        self.normalized_shape = normalized_shape
        self.eps = eps
        self.elementwise_affine = elementwise_affine
        if self.elementwise_affine:
            self.weight = nn.Parameter(torch.ones(normalized_shape))
            self.bias = nn.Parameter(torch.zeros(normalized_shape))

    def forward(self, x, idx=None):
        if self.normalized_shape[0] == 1:
            return x
        if idx is None:
            return F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)
        else:
            return F.layer_norm(x, self.normalized_shape,
                                self.weight[:, idx, :], self.bias[:, idx, :], self.eps)

class SimpleGraphConstructor(nn.Module):
    """
    Simplified Graph Learning Layer
    """
    def __init__(self, nnodes, dim, device, alpha=3):
        super(SimpleGraphConstructor, self).__init__()
        self.nnodes = nnodes
        self.dim = dim
        self.device = device
        self.alpha = alpha

        # Use node embeddings to learn the graph structure
        self.emb1 = nn.Embedding(nnodes, dim)
        self.emb2 = nn.Embedding(nnodes, dim)
        self.lin1 = nn.Linear(dim, dim)
        self.lin2 = nn.Linear(dim, dim)

    def forward(self, idx):
        # Get node embeddings
        nodevec1 = self.emb1(idx)
        nodevec2 = self.emb2(idx)

        # Transform node embeddings
        nodevec1 = torch.tanh(self.alpha * self.lin1(nodevec1))
        nodevec2 = torch.tanh(self.alpha * self.lin2(nodevec2))

        # Compute adjacency matrix
        a = torch.mm(nodevec1, nodevec2.transpose(1, 0))
        adj = F.softmax(F.relu(a), dim=1)

        return adj

class TemporalBlock(nn.Module):
    """
    Simplified temporal convolution block with gating mechanism
    """
    def __init__(self, in_channels, out_channels, kernel_size=3, dilation=1):
        super(TemporalBlock, self).__init__()
        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size,
                              padding=dilation*(kernel_size-1)//2, dilation=dilation)
        self.conv2 = nn.Conv1d(in_channels, out_channels, kernel_size,
                              padding=dilation*(kernel_size-1)//2, dilation=dilation)
        self.residual = nn.Conv1d(in_channels, out_channels, 1) if in_channels != out_channels else nn.Identity()

    def forward(self, x):
        # Gating mechanism
        filter = torch.tanh(self.conv1(x))
        gate = torch.sigmoid(self.conv2(x))
        x = filter * gate

        # Residual connection
        res = self.residual(x)
        return x + res

class GraphConvolution(nn.Module):
    """
    Simplified graph convolution layer
    """
    def __init__(self, in_channels, out_channels):
        super(GraphConvolution, self).__init__()
        self.linear = nn.Linear(in_channels, out_channels)

    def forward(self, x, adj):
        # Graph convolution
        # x shape: [batch, stations, channels]
        # adj shape: [stations, stations]

        # Apply graph convolution for each item in the batch
        batch_size = x.size(0)
        n_stations = x.size(1)

        # Reshape for matrix multiplication with adjacency matrix
        x_reshaped = x.view(batch_size, n_stations, -1)

        # Apply adjacency matrix - handle batching correctly
        x = torch.bmm(adj.unsqueeze(0).expand(batch_size, -1, -1), x_reshaped)

        # Apply linear transformation
        x = self.linear(x)

        return x

class SimpleHierarchicalGNN(nn.Module):
    """
    Simplified Hierarchical GNN for temperature forecasting
    """
    def __init__(self, n_stations, seq_length, input_dim, forecast_horizon=24, hidden_dim=32,
                 n_layers=2, dropout=0.1, node_dim=16, device='cuda'):
        super(SimpleHierarchicalGNN, self).__init__()
        self.n_stations = n_stations
        self.seq_length = seq_length
        self.forecast_horizon = forecast_horizon
        self.n_layers = n_layers
        self.hidden_dim = hidden_dim
        self.dropout = dropout

        # Input projection
        self.start_conv = nn.Conv2d(input_dim, hidden_dim, kernel_size=(1, 1))

        # Graph constructor for station relationships
        self.graph_constructor = SimpleGraphConstructor(n_stations, node_dim, device)

        # Temporal convolutional blocks
        self.temporal_blocks = nn.ModuleList()
        self.graph_convs = nn.ModuleList()
        self.norms = nn.ModuleList()

        for i in range(n_layers):
            dilation = 2 ** i
            self.temporal_blocks.append(
                TemporalBlock(hidden_dim, hidden_dim, kernel_size=3, dilation=dilation)
            )
            self.graph_convs.append(
                GraphConvolution(hidden_dim, hidden_dim)
            )
            self.norms.append(
                LayerNorm((hidden_dim, seq_length - (dilation * 2)), elementwise_affine=True)
            )

        # Output projection for forecasting multiple steps
        self.end_conv = nn.Conv2d(hidden_dim, forecast_horizon, kernel_size=(1, 1))

        # Station indices for graph construction
        self.stations_idx = torch.arange(n_stations).to(device)

    def forward(self, x):
        """
        x: Input tensor of shape [batch_size, features, stations, time]
        returns: Predictions of shape [batch_size, stations, forecast_horizon]
        """
        # Get batch size
        batch_size = x.size(0)

        # Learn the graph structure among stations
        adj = self.graph_constructor(self.stations_idx)

        # Input projection
        x = self.start_conv(x)

        # Reshape for temporal convolutions [batch, channels, stations, time] -> [batch*stations, channels, time]
        x = x.permute(0, 2, 1, 3).contiguous()
        x = x.view(batch_size * self.n_stations, self.hidden_dim, -1)

        skip_connections = []

        # Process through layers
        for i in range(self.n_layers):
            # Temporal convolution
            res = x
            x = self.temporal_blocks[i](x)
            x = F.dropout(x, self.dropout, training=self.training)

            # Reshape for graph convolution [batch*stations, channels, time] -> [batch, stations, channels, time]
            x_graph = x.view(batch_size, self.n_stations, self.hidden_dim, -1)



            # Graph convolution for each time step
            time_len = x_graph.size(-1)
            x_spatial = []
            for t in range(time_len):
              x_t = x_graph[..., t]  # [batch, stations, channels]
              x_t = self.graph_convs[i](x_t, adj)  # [batch, stations, channels]
              x_spatial.append(x_t.unsqueeze(-1))

            # Combine results from all time steps
            x_spatial = torch.cat(x_spatial, dim=-1)  # [batch, stations, channels, time]

            # Reshape back to [batch*stations, channels, time]
            x = x_spatial.view(batch_size * self.n_stations, self.hidden_dim, -1)

            # Add residual connection
            x = res[:, :, -x.size(2):] + x

            # Store for skip connection
            skip_connections.append(x)

        # Reshape back to [batch, channels, stations, time]
        x = x.view(batch_size, self.n_stations, self.hidden_dim, -1)
        x = x.permute(0, 2, 1, 3).contiguous()

        # Output projection to get temperature prediction
        x = self.end_conv(x)  # [batch, forecast_horizon, stations, remaining_time]

        # Take the last time step for each forecast horizon
        x = x[:, :, :, -1]  # [batch, forecast_horizon, stations]

        # Reshape to [batch, stations, forecast_horizon]
        x = x.permute(0, 2, 1)

        return x

def create_model(data_config, device='cuda'):
    """
    Helper function to create the model based on your data configuration
    """
    model = SimpleHierarchicalGNN(
        n_stations=data_config['n_stations'],
        seq_length=data_config['seq_length'],
        input_dim=data_config['input_dim'],
        forecast_horizon=data_config['forecast_horizon'],
        hidden_dim=64,
        n_layers=3,
        dropout=0.2,
        node_dim=32,
        device=device
    )
    return model

class LayerNorm(nn.Module):
    def __init__(self, normalized_shape, eps=1e-5, elementwise_affine=True):
        super(LayerNorm, self).__init__()
        self.normalized_shape = normalized_shape
        self.eps = eps
        self.elementwise_affine = elementwise_affine
        if self.elementwise_affine:
            self.weight = nn.Parameter(torch.ones(normalized_shape))
            self.bias = nn.Parameter(torch.zeros(normalized_shape))

    def forward(self, x, idx=None):
        if self.normalized_shape[0] == 1:
            return x
        if idx is None:
            return F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)
        else:
            return F.layer_norm(x, self.normalized_shape,
                                self.weight[:, idx, :], self.bias[:, idx, :], self.eps)

class SimpleGraphConstructor(nn.Module):
    """
    Simplified Graph Learning Layer
    """
    def __init__(self, nnodes, dim, device, alpha=3):
        super(SimpleGraphConstructor, self).__init__()
        self.nnodes = nnodes
        self.dim = dim
        self.device = device
        self.alpha = alpha

        # Use node embeddings to learn the graph structure
        self.emb1 = nn.Embedding(nnodes, dim)
        self.emb2 = nn.Embedding(nnodes, dim)
        self.lin1 = nn.Linear(dim, dim)
        self.lin2 = nn.Linear(dim, dim)

    def forward(self, idx):
        # Get node embeddings
        nodevec1 = self.emb1(idx)
        nodevec2 = self.emb2(idx)

        # Transform node embeddings
        nodevec1 = torch.tanh(self.alpha * self.lin1(nodevec1))
        nodevec2 = torch.tanh(self.alpha * self.lin2(nodevec2))

        # Compute adjacency matrix
        a = torch.mm(nodevec1, nodevec2.transpose(1, 0))
        adj = F.softmax(F.relu(a), dim=1)

        return adj

class TemporalBlock(nn.Module):
    """
    Simplified temporal convolution block with gating mechanism
    """
    def __init__(self, in_channels, out_channels, kernel_size=3, dilation=1):
        super(TemporalBlock, self).__init__()
        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size,
                              padding=dilation*(kernel_size-1)//2, dilation=dilation)
        self.conv2 = nn.Conv1d(in_channels, out_channels, kernel_size,
                              padding=dilation*(kernel_size-1)//2, dilation=dilation)
        self.residual = nn.Conv1d(in_channels, out_channels, 1) if in_channels != out_channels else nn.Identity()

    def forward(self, x):
        # Gating mechanism
        filter = torch.tanh(self.conv1(x))
        gate = torch.sigmoid(self.conv2(x))
        x = filter * gate

        # Residual connection
        res = self.residual(x)
        return x + res

class GraphConvolution(nn.Module):
    """
    Simplified graph convolution layer
    """
    def __init__(self, in_channels, out_channels):
        super(GraphConvolution, self).__init__()
        self.linear = nn.Linear(in_channels, out_channels)
   # DS change
    def forward(self, x, adj):
        # Add shape checks and reshaping
        print(f"Debug - x shape: {x.shape}, adj shape: {adj.shape}")  # Debug line

        # Reshape if input is flattened (e.g., [320, 64] -> [32, 10, 64])
        if x.dim() == 2:
            batch_size = adj.size(0)  # adj is [stations, stations], e.g., 10x10
            x = x.view(-1, batch_size, self.linear.in_features)

        # Graph convolution (now x is [batch, stations, channels])
        x = torch.matmul(adj, x)  # [batch, stations, channels]
        x = self.linear(x)
        return x

class SimpleHierarchicalGNN(nn.Module):
    """
    Simplified Hierarchical GNN for temperature forecasting
    """
    def __init__(self, n_stations, seq_length, input_dim, forecast_horizon=24, hidden_dim=32,
                 n_layers=2, dropout=0.1, node_dim=16, device='cuda'):
        super(SimpleHierarchicalGNN, self).__init__()
        self.n_stations = n_stations
        self.seq_length = seq_length
        self.forecast_horizon = forecast_horizon
        self.n_layers = n_layers
        self.hidden_dim = hidden_dim
        self.dropout = dropout

        # Input projection
        self.start_conv = nn.Conv2d(input_dim, hidden_dim, kernel_size=(1, 1))

        # Graph constructor for station relationships
        self.graph_constructor = SimpleGraphConstructor(n_stations, node_dim, device)

        # Temporal convolutional blocks
        self.temporal_blocks = nn.ModuleList()
        self.graph_convs = nn.ModuleList()
        self.norms = nn.ModuleList()

        for i in range(n_layers):
            dilation = 2 ** i
            self.temporal_blocks.append(
                TemporalBlock(hidden_dim, hidden_dim, kernel_size=3, dilation=dilation)
            )
            self.graph_convs.append(
                GraphConvolution(hidden_dim, hidden_dim)
            )
            self.norms.append(
                LayerNorm((hidden_dim, seq_length - (dilation * 2)), elementwise_affine=True)
            )

        # Output projection for forecasting multiple steps
        self.end_conv = nn.Conv2d(hidden_dim, forecast_horizon, kernel_size=(1, 1))

        # Station indices for graph construction
        self.stations_idx = torch.arange(n_stations).to(device)

    def forward(self, x):
        """
        x: Input tensor of shape [batch_size, features, stations, time]
        returns: Predictions of shape [batch_size, stations, forecast_horizon]
        """
        # Get batch size
        batch_size = x.size(0)

        # Learn the graph structure among stations
        adj = self.graph_constructor(self.stations_idx)

        # Input projection
        x = self.start_conv(x)

        # Reshape for temporal convolutions [batch, channels, stations, time] -> [batch*stations, channels, time]
        x = x.permute(0, 2, 1, 3).contiguous()
        x = x.view(batch_size * self.n_stations, self.hidden_dim, -1)

        skip_connections = []

        # Process through layers
        for i in range(self.n_layers):
            # Temporal convolution
            res = x
            x = self.temporal_blocks[i](x)
            x = F.dropout(x, self.dropout, training=self.training)

            # Reshape for graph convolution [batch*stations, channels, time] -> [batch, stations, channels, time]
            x_graph = x.view(batch_size, self.n_stations, self.hidden_dim, -1)

            # Graph convolution for each time step
            time_len = x_graph.size(-1)
            x_spatial = []
            for t in range(time_len):
                # Apply graph convolution
                x_t = x_graph[..., t]  # [batch, stations, channels]
                x_t = self.graph_convs[i](x_t, adj)  # [batch, stations, channels]
                x_spatial.append(x_t.unsqueeze(-1))

            # Combine results from all time steps
            x_spatial = torch.cat(x_spatial, dim=-1)  # [batch, stations, channels, time]

            # Reshape back to [batch*stations, channels, time]
            x = x_spatial.view(batch_size * self.n_stations, self.hidden_dim, -1)

            # Add residual connection
            x = res[:, :, -x.size(2):] + x

            # Store for skip connection
            skip_connections.append(x)

        # Reshape back to [batch, channels, stations, time]
        x = x.view(batch_size, self.n_stations, self.hidden_dim, -1)
        x = x.permute(0, 2, 1, 3).contiguous()

        # Output projection to get temperature prediction
        x = self.end_conv(x)  # [batch, forecast_horizon, stations, remaining_time]

        # Take the last time step for each forecast horizon
        x = x[:, :, :, -1]  # [batch, forecast_horizon, stations]

        # Reshape to [batch, stations, forecast_horizon]
        x = x.permute(0, 2, 1)

        return x

def create_model(data_config, device='cuda'):
    """
    Helper function to create the model based on your data configuration
    """
    model = SimpleHierarchicalGNN(
        n_stations=data_config['n_stations'],
        seq_length=data_config['seq_length'],
        input_dim=data_config['input_dim'],
        forecast_horizon=data_config['forecast_horizon'],
        hidden_dim=64,
        n_layers=3,
        dropout=0.2,
        node_dim=32,
        device=device
    )
    return model

def train_epoch(model, train_loader, optimizer, criterion, device):
    """Train the model for one epoch"""
    model.train()
    total_loss = 0
    for X, y in train_loader:
        X, y = X.to(device), y.to(device)

        # Forward pass
        optimizer.zero_grad()
        y_pred = model(X)
        loss = criterion(y_pred, y)

        # Backward pass
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    return total_loss / len(train_loader)

def validate(model, val_loader, criterion, device):
    """Validate the model"""
    model.eval()
    total_loss = 0
    with torch.no_grad():
        for X, y in val_loader:
            X, y = X.to(device), y.to(device)
            y_pred = model(X)
            loss = criterion(y_pred, y)
            total_loss += loss.item()

    return total_loss / len(val_loader)

def evaluate(model, test_loader, scaler, device):
    """Evaluate the model and compute metrics"""
    model.eval()

    all_preds = []
    all_targets = []

    with torch.no_grad():
        for X, y in test_loader:
            X, y = X.to(device), y.to(device)
            y_pred = model(X)

            # Move to CPU for numpy conversion
            all_preds.append(y_pred.cpu().numpy())
            all_targets.append(y.cpu().numpy())

    # Concatenate batches
    all_preds = np.concatenate(all_preds, axis=0)
    all_targets = np.concatenate(all_targets, axis=0)

    # Calculate metrics for each station and forecast horizon
    n_stations = all_preds.shape[1]
    forecast_horizon = all_preds.shape[2]

    # Initialize metrics containers
    mae_by_station = np.zeros((n_stations, forecast_horizon))
    rmse_by_station = np.zeros((n_stations, forecast_horizon))
    r2_by_station = np.zeros((n_stations, forecast_horizon))

    # Calculate metrics
    for s in range(n_stations):
        for h in range(forecast_horizon):
            y_true = all_targets[:, s, h]
            y_pred = all_preds[:, s, h]

            # Skip if all values are identical (can happen with missing data)
            if np.std(y_true) == 0:
                continue

            mae_by_station[s, h] = mean_absolute_error(y_true, y_pred)
            rmse_by_station[s, h] = np.sqrt(mean_squared_error(y_true, y_pred))
            r2_by_station[s, h] = r2_score(y_true, y_pred)

    # Calculate average metrics across stations
    avg_mae_by_horizon = np.mean(mae_by_station, axis=0)
    avg_rmse_by_horizon = np.mean(rmse_by_station, axis=0)
    avg_r2_by_horizon = np.mean(r2_by_station, axis=0)

    # Overall averages
    overall_mae = np.mean(mae_by_station)
    overall_rmse = np.mean(rmse_by_station)
    overall_r2 = np.mean(r2_by_station)

    metrics = {
        'mae_by_station': mae_by_station,
        'rmse_by_station': rmse_by_station,
        'r2_by_station': r2_by_station,
        'avg_mae_by_horizon': avg_mae_by_horizon,
        'avg_rmse_by_horizon': avg_rmse_by_horizon,
        'avg_r2_by_horizon': avg_r2_by_horizon,
        'overall_mae': overall_mae,
        'overall_rmse': overall_rmse,
        'overall_r2': overall_r2
    }

    return metrics, all_preds, all_targets

def plot_training_history(train_losses, val_losses):
    """Plot training and validation losses"""
    plt.figure(figsize=(10, 6))
    plt.plot(train_losses, label='Training Loss')
    plt.plot(val_losses, label='Validation Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Training and Validation Loss')
    plt.legend()
    plt.grid(True)
    plt.show()

def plot_forecast_by_station(predictions, targets, station_names, timestamp=0, forecast_horizon=24):
    """
    Plot predictions vs. actual for each station
    Args:
        predictions: numpy array [batch, stations, forecast_horizon]
        targets: numpy array [batch, stations, forecast_horizon]
        station_names: list of station names
        timestamp: which batch index to plot
    """
    n_stations = len(station_names)
    n_cols = min(3, n_stations)
    n_rows = (n_stations + n_cols - 1) // n_cols

    plt.figure(figsize=(15, n_rows * 4))

    for i, station in enumerate(station_names):
        plt.subplot(n_rows, n_cols, i+1)

        # Get data for this station
        pred = predictions[timestamp, i, :]
        actual = targets[timestamp, i, :]

        # Plot
        hours = np.arange(forecast_horizon)
        plt.plot(hours, actual, 'b-', label='Actual')
        plt.plot(hours, pred, 'r--', label='Predicted')

        plt.title(f'Station: {station}')
        plt.xlabel('Hours ahead')
        plt.ylabel('Temperature (Â°C)')
        plt.legend()
        plt.grid(True)

    plt.tight_layout()
    plt.show()

def plot_metrics_by_horizon(metrics):
    """Plot metrics by forecast horizon"""
    forecast_horizon = len(metrics['avg_mae_by_horizon'])

    plt.figure(figsize=(15, 5))

    # Plot MAE
    plt.subplot(1, 3, 1)
    plt.plot(range(1, forecast_horizon+1), metrics['avg_mae_by_horizon'])
    plt.title('MAE by Forecast Horizon')
    plt.xlabel('Hours Ahead')
    plt.ylabel('MAE (Â°C)')
    plt.grid(True)

    # Plot RMSE
    plt.subplot(1, 3, 2)
    plt.plot(range(1, forecast_horizon+1), metrics['avg_rmse_by_horizon'])
    plt.title('RMSE by Forecast Horizon')
    plt.xlabel('Hours Ahead')
    plt.ylabel('RMSE (Â°C)')
    plt.grid(True)

    # Plot RÂ²
    plt.subplot(1, 3, 3)
    plt.plot(range(1, forecast_horizon+1), metrics['avg_r2_by_horizon'])
    plt.title('RÂ² by Forecast Horizon')
    plt.xlabel('Hours Ahead')
    plt.ylabel('RÂ²')
    plt.grid(True)

    plt.tight_layout()
    plt.show()

def plot_station_performance_heatmap(metrics, station_names):
    """Plot heatmap of metrics by station and forecast horizon"""
    fig, axes = plt.subplots(1, 3, figsize=(18, 6))

    # MAE heatmap
    sns.heatmap(metrics['mae_by_station'], ax=axes[0], cmap='YlOrRd',
                xticklabels=np.arange(1, metrics['mae_by_station'].shape[1]+1, 4),
                yticklabels=station_names, annot=False)
    axes[0].set_title('MAE by Station and Horizon')
    axes[0].set_xlabel('Hours Ahead')
    axes[0].set_ylabel('Station')

    # RMSE heatmap
    sns.heatmap(metrics['rmse_by_station'], ax=axes[1], cmap='YlOrRd',
                xticklabels=np.arange(1, metrics['rmse_by_station'].shape[1]+1, 4),
                yticklabels=station_names, annot=False)
    axes[1].set_title('RMSE by Station and Horizon')
    axes[1].set_xlabel('Hours Ahead')
    axes[1].set_ylabel('Station')

    # RÂ² heatmap
    sns.heatmap(metrics['r2_by_station'], ax=axes[2], cmap='YlGnBu',
                xticklabels=np.arange(1, metrics['r2_by_station'].shape[1]+1, 4),
                yticklabels=station_names, annot=False)
    axes[2].set_title('RÂ² by Station and Horizon')
    axes[2].set_xlabel('Hours Ahead')
    axes[2].set_ylabel('Station')

    plt.tight_layout()
    plt.show()

def analyze_by_region(metrics, station_names, station_regions):
    """Analyze performance by region"""
    unique_regions = np.unique(station_regions)
    n_regions = len(unique_regions)

    # Initialize containers
    mae_by_region = np.zeros((n_regions, metrics['mae_by_station'].shape[1]))
    rmse_by_region = np.zeros((n_regions, metrics['rmse_by_station'].shape[1]))
    r2_by_region = np.zeros((n_regions, metrics['r2_by_station'].shape[1]))

    # Group by region
    for i, region in enumerate(unique_regions):
        region_stations = [j for j, r in enumerate(station_regions) if r == region]

        mae_by_region[i] = np.mean(metrics['mae_by_station'][region_stations], axis=0)
        rmse_by_region[i] = np.mean(metrics['rmse_by_station'][region_stations], axis=0)
        r2_by_region[i] = np.mean(metrics['r2_by_station'][region_stations], axis=0)

    # Plot results
    plt.figure(figsize=(15, 5))

    # Plot MAE
    plt.subplot(1, 3, 1)
    for i, region in enumerate(unique_regions):
        plt.plot(range(1, mae_by_region.shape[1]+1), mae_by_region[i], label=region)
    plt.title('MAE by Region')
    plt.xlabel('Hours Ahead')
    plt.ylabel('MAE (Â°C)')
    plt.legend()
    plt.grid(True)

    # Plot RMSE
    plt.subplot(1, 3, 2)
    for i, region in enumerate(unique_regions):
        plt.plot(range(1, rmse_by_region.shape[1]+1), rmse_by_region[i], label=region)
    plt.title('RMSE by Region')
    plt.xlabel('Hours Ahead')
    plt.ylabel('RMSE (Â°C)')
    plt.legend()
    plt.grid(True)

    # Plot RÂ²
    plt.subplot(1, 3, 3)
    for i, region in enumerate(unique_regions):
        plt.plot(range(1, r2_by_region.shape[1]+1), r2_by_region[i], label=region)
    plt.title('RÂ² by Region')
    plt.xlabel('Hours Ahead')
    plt.ylabel('RÂ²')
    plt.legend()
    plt.grid(True)

    plt.tight_layout()
    plt.show()

    # Return region metrics
    region_metrics = {
        'mae_by_region': mae_by_region,
        'rmse_by_region': rmse_by_region,
        'r2_by_region': r2_by_region,
        'region_names': unique_regions
    }

    return region_metrics

def run_weather_forecasting_pipeline(data_path, output_dir='./model_outputs'):
    """
    Main function to run the entire weather forecasting pipeline
    """
    print("Starting weather forecasting pipeline...")
    print(f"Using data from: {data_path}")

    # Create output directory if it doesn't exist
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    try:
        #----------------
        # 1. Data Loading
        #----------------
        print("\n1. Loading and preprocessing data...")
        df = load_data(data_path)

        # Add a check to ensure there's a station_id column
        if 'station_id' not in df.columns:
            # If missing, try to create one from existing columns or use default
            if 'STATION' in df.columns:
                df['station_id'] = df['STATION']
            else:
                print("WARNING: No station_id column found, creating a default one")
                df['station_id'] = 'STATION1'  # Default station ID

        # Add a check to ensure there's a region column
        if 'region' not in df.columns:
            print("WARNING: No region column found, creating a default one")
            df['region'] = 'default_region'  # Default region

        # Continue with normal pipeline
        train_df, val_df, test_df = split_data_by_time(df)

        # Get unique stations and their regions
        station_ids = df['station_id'].unique()
        n_stations = len(station_ids)

        # Create a mapping of stations to regions
        station_to_region = {}
        for station in station_ids:
            region = df[df['station_id'] == station]['region'].iloc[0]
            station_to_region[station] = region

        # List of regions for each station (in order)
        station_regions = [station_to_region[s] for s in station_ids]

        print(f"Found {n_stations} stations across {len(set(station_regions))} regions")

        # Define features to use
        # Check if we have all required columns
        required_cols = ['HourlyRelativeHumidity', 'HourlyStationPressure', 'Temperature_C']
        available_cols = [col for col in required_cols if col in df.columns]

        if len(available_cols) < len(required_cols):
            missing_cols = set(required_cols) - set(available_cols)
            print(f"WARNING: Missing columns: {missing_cols}")
            # Try to find substitutes for missing columns
            if 'Temperature_C' not in available_cols:
                if 'HourlyDryBulbTemperature' in df.columns:
                    print("Using HourlyDryBulbTemperature in place of Temperature_C")
                    df['Temperature_C'] = df['HourlyDryBulbTemperature']
                    available_cols.append('Temperature_C')
                elif 'TEMP' in df.columns:
                    print("Using TEMP in place of Temperature_C")
                    df['Temperature_C'] = df['TEMP']
                    available_cols.append('Temperature_C')

            if 'HourlyRelativeHumidity' not in available_cols and 'HUMIDITY' in df.columns:
                print("Using HUMIDITY in place of HourlyRelativeHumidity")
                df['HourlyRelativeHumidity'] = df['HUMIDITY']
                available_cols.append('HourlyRelativeHumidity')

            if 'HourlyStationPressure' not in available_cols and 'PRESSURE' in df.columns:
                print("Using PRESSURE in place of HourlyStationPressure")
                df['HourlyStationPressure'] = df['PRESSURE']
                available_cols.append('HourlyStationPressure')

        # Add hour_of_day and day_of_year to available columns
        if 'hour_of_day' in df.columns:
            available_cols.append('hour_of_day')
        if 'day_of_year' in df.columns:
            available_cols.append('day_of_year')

        feature_cols = available_cols
        print(f"Using features: {feature_cols}")

        # Verify we have the Temperature_C column
        if 'Temperature_C' not in feature_cols:
            raise ValueError("Required column 'Temperature_C' is missing and no suitable replacement found")

        # Normalize features
        train_norm, val_norm, test_norm, scaler = normalize_features(
            train_df, val_df, test_df, feature_cols)

        # Add normalized features back to dataframes
        for col in feature_cols:
            train_df[col] = train_norm[col]
            val_df[col] = val_norm[col]
            test_df[col] = test_norm[col]

        #----------------
        # 2. Create DataLoaders
        #----------------
        print("\n2. Creating DataLoaders...")

        # Define data configuration
        seq_length = 24  # 24 hours of input
        forecast_horizon = 24  # predict next 24 hours
        batch_size = 32

        # Create DataLoaders
        train_loader, val_loader, test_loader = create_dataloaders(
            train_df, val_df, test_df, station_ids, feature_cols,
            seq_length=seq_length, forecast_horizon=forecast_horizon, batch_size=batch_size
        )

        #----------------
        # 3. Create Model
        #----------------
        print("\n3. Creating model...")

        # Define model configuration
        data_config = {
            'n_stations': n_stations,
            'seq_length': seq_length,
            'input_dim': len(feature_cols),
            'forecast_horizon': forecast_horizon
        }

        # Create model
        model = create_model(data_config, device)
        model.to(device)

        print(f"Model created with {sum(p.numel() for p in model.parameters())} parameters")

        #----------------
        # 4. Train Model
        #----------------
        print("\n4. Training model...")

        # Define training parameters
        num_epochs = 30
        learning_rate = 0.001

        # Define loss function and optimizer
        criterion = nn.MSELoss()
        optimizer = optim.Adam(model.parameters(), lr=learning_rate)
        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, factor=0.5)

        # Training loop
        train_losses = []
        val_losses = []
        best_val_loss = float('inf')
        best_model_path = os.path.join(output_dir, 'best_model.pth')

        for epoch in range(num_epochs):
            # Train
            train_loss = train_epoch(model, train_loader, optimizer, criterion, device)
            train_losses.append(train_loss)

            # Validate
            val_loss = validate(model, val_loader, criterion, device)
            val_losses.append(val_loss)

            # Update learning rate
            scheduler.step(val_loss)

            # Save best model
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                torch.save(model.state_dict(), best_model_path)
                print(f"Epoch {epoch+1}/{num_epochs}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f} (best)")
            else:
                print(f"Epoch {epoch+1}/{num_epochs}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}")

        # Plot training history
        plot_training_history(train_losses, val_losses)

        # Load best model
        model.load_state_dict(torch.load(best_model_path))

        #----------------
        # 5. Evaluate Model
        #----------------
        print("\n5. Evaluating model...")

        # Evaluate on test set
        metrics, all_preds, all_targets = evaluate(model, test_loader, scaler, device)

        # Print overall metrics
        print(f"Overall MAE: {metrics['overall_mae']:.4f}Â°C")
        print(f"Overall RMSE: {metrics['overall_rmse']:.4f}Â°C")
        print(f"Overall RÂ²: {metrics['overall_r2']:.4f}")

        # Plot metrics by forecast horizon
        plot_metrics_by_horizon(metrics)

        # Plot station performance
        plot_station_performance_heatmap(metrics, station_ids)

        # Analyze by region
        region_metrics = analyze_by_region(metrics, station_ids, station_regions)

        # Plot sample forecasts
        print("\n6. Plotting sample forecasts...")
        plot_forecast_by_station(all_preds, all_targets, station_ids, timestamp=0)

        print("\nWeather forecasting pipeline completed!")
        return model, metrics, region_metrics

    except Exception as e:
        print(f"Error in pipeline: {str(e)}")
        import traceback
        traceback.print_exc()
        return None, None, None

# Example usage:
if __name__ == "__main__":

    # Run the pipeline
    model, metrics, region_metrics = run_weather_forecasting_pipeline(DATA_PATH)