{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "jgERb2TfLBD2",
        "outputId": "c2a7947b-dcb6-4aae-d93c-6b8115372fcf"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"GNN Model.ipynb\n",
        "\n",
        "Automatically generated by Colab.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1pSO-VEyn5Cywjw9sXKn2fjXdVbI3hAsH\n",
        "\"\"\"\n",
        "\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"Untitled2.ipynb\n",
        "\n",
        "Automatically generated by Colab.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/193StgLnr4doKklAxwBiQsVX3njEfb1oa\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "DATA_PATH = r\"C:\\Users\\hu4227mo-s\\OneDrive - Lund University\\updated_data (4).xlsx\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sfm57HeJLMIC",
        "outputId": "106f6344-0529-41da-f161-c70c8ff1ddae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "California Weather Forecasting with Temporal Fusion Transformer\n",
            "======================================================================\n",
            "Filtered data to 5 stations: ['San Francisco', 'Mammoth Lakes', 'Palm Spring', 'Fresno', 'LA Downtown']\n",
            "Loaded data with 268746 records\n",
            "Found 209 records for Spring target day (4/15/2024)\n",
            "Found 169 records for Summer target day (7/20/2024)\n",
            "Found 165 records for Fall target day (10/10/2024)\n",
            "Found 216 records for Winter target day (1/15/2024)\n",
            "\n",
            "Preparing unified dataset from all historical data\n",
            "Using 5 stations: ['Palm Spring' 'LA Downtown' 'San Francisco' 'Fresno' 'Mammoth Lakes']\n",
            "WARNING: No valid windows found in dataset. Using reduced requirements.\n",
            "Found 216133 windows with relaxed continuity requirements\n",
            "Training unified model on all historical data...\n",
            "Starting training for 20 epochs with patience 5...\n",
            "  Batch 10/9456, Loss: 0.0374, Time: 0.101s\n",
            "  Batch 20/9456, Loss: 0.0408, Time: 0.087s\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 957\u001b[39m\n\u001b[32m    955\u001b[39m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[32m    956\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTraining unified model on all historical data...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m957\u001b[39m model, train_losses, val_losses = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    958\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    959\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    960\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    961\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    962\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    963\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\n\u001b[32m    964\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    966\u001b[39m \u001b[38;5;66;03m# Plot training curves\u001b[39;00m\n\u001b[32m    967\u001b[39m plt.figure(figsize=(\u001b[32m10\u001b[39m, \u001b[32m6\u001b[39m))\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 543\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, train_loader, val_loader, learning_rate, epochs, patience)\u001b[39m\n\u001b[32m    540\u001b[39m train_batches = \u001b[32m0\u001b[39m\n\u001b[32m    542\u001b[39m batch_times = []\n\u001b[32m--> \u001b[39m\u001b[32m543\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    544\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_start\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtime\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    545\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Move to device - handle different types of input structures\u001b[39;49;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:708\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    706\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    707\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m708\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    710\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    711\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    712\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    713\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    714\u001b[39m ):\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:764\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    762\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    763\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m764\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    765\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    766\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:50\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.auto_collation:\n\u001b[32m     49\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.dataset, \u001b[33m\"\u001b[39m\u001b[33m__getitems__\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset.__getitems__:\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     52\u001b[39m         data = [\u001b[38;5;28mself\u001b[39m.dataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataset.py:420\u001b[39m, in \u001b[36mSubset.__getitems__\u001b[39m\u001b[34m(self, indices)\u001b[39m\n\u001b[32m    418\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset.__getitems__([\u001b[38;5;28mself\u001b[39m.indices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m    419\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m420\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 277\u001b[39m, in \u001b[36mWeatherDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m    275\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m t_idx, ts \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(output_timestamps):\n\u001b[32m    276\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m t_idx < \u001b[38;5;28mself\u001b[39m.forecast_horizon:  \u001b[38;5;66;03m# Safety check\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m         station_data = \u001b[38;5;28mself\u001b[39m.df[\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtimestamp\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[43mts\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m&\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[43m                            \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mstation_id\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[43mstation_id\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[32m    280\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m station_data.empty:\n\u001b[32m    281\u001b[39m             y[s_idx, t_idx] = station_data[\u001b[33m'\u001b[39m\u001b[33mTemperature_C\u001b[39m\u001b[33m'\u001b[39m].values[\u001b[32m0\u001b[39m]\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hu4227mo-s\\.conda\\envs\\pytorch_env\\Lib\\site-packages\\pandas\\core\\ops\\common.py:76\u001b[39m, in \u001b[36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m     72\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[32m     74\u001b[39m other = item_from_zerodim(other)\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hu4227mo-s\\.conda\\envs\\pytorch_env\\Lib\\site-packages\\pandas\\core\\arraylike.py:70\u001b[39m, in \u001b[36mOpsMixin.__and__\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m     68\u001b[39m \u001b[38;5;129m@unpack_zerodim_and_defer\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m__and__\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__and__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_logical_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mand_\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hu4227mo-s\\.conda\\envs\\pytorch_env\\Lib\\site-packages\\pandas\\core\\series.py:6130\u001b[39m, in \u001b[36mSeries._logical_method\u001b[39m\u001b[34m(self, other, op)\u001b[39m\n\u001b[32m   6127\u001b[39m lvalues = \u001b[38;5;28mself\u001b[39m._values\n\u001b[32m   6128\u001b[39m rvalues = extract_array(other, extract_numpy=\u001b[38;5;28;01mTrue\u001b[39;00m, extract_range=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m6130\u001b[39m res_values = \u001b[43mops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlogical_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6131\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._construct_result(res_values, name=res_name)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hu4227mo-s\\.conda\\envs\\pytorch_env\\Lib\\site-packages\\pandas\\core\\ops\\array_ops.py:448\u001b[39m, in \u001b[36mlogical_op\u001b[39m\u001b[34m(left, right, op)\u001b[39m\n\u001b[32m    446\u001b[39m     is_other_int_dtype = rvalues.dtype.kind \u001b[38;5;129;01min\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33miu\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_other_int_dtype:\n\u001b[32m--> \u001b[39m\u001b[32m448\u001b[39m         rvalues = \u001b[43mfill_bool\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    450\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    451\u001b[39m     \u001b[38;5;66;03m# i.e. scalar\u001b[39;00m\n\u001b[32m    452\u001b[39m     is_other_int_dtype = lib.is_integer(rvalues)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hu4227mo-s\\.conda\\envs\\pytorch_env\\Lib\\site-packages\\pandas\\core\\ops\\array_ops.py:420\u001b[39m, in \u001b[36mlogical_op.<locals>.fill_bool\u001b[39m\u001b[34m(x, left)\u001b[39m\n\u001b[32m    417\u001b[39m         x[mask] = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    419\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m left \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m left.dtype.kind == \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m420\u001b[39m     x = \u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    421\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import time\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import os\n",
        "from datetime import datetime, timedelta\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cpu')  # Using CPU as specified\n",
        "\n",
        "# ============================================================\n",
        "# MODIFIED DATA LOADING AND PROCESSING FOR TARGETED SEASONAL ANALYSIS\n",
        "# ============================================================\n",
        "\n",
        "def load_data(file_path):\n",
        "    \"\"\"\n",
        "    Load weather data, clean missing values, and filter to 5 stations representing different topographies.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Determine file type and read\n",
        "        if file_path.endswith('.xlsx'):\n",
        "            df = pd.read_excel(file_path, engine='openpyxl')\n",
        "        else:\n",
        "            df = pd.read_csv(file_path, encoding='ISO-8859-1')\n",
        "        # Rest of function stays the same\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading data: {e}\")\n",
        "        return pd.DataFrame()  # Return empty DataFrame\n",
        "\n",
        "    # Convert timestamp\n",
        "    if 'timestamp' in df.columns:\n",
        "        df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
        "    elif 'DATE' in df.columns:\n",
        "        df['timestamp'] = pd.to_datetime(df['DATE'])\n",
        "        df = df.rename(columns={'DATE': 'date_original'})\n",
        "\n",
        "    # Ensure time-based ordering before interpolation\n",
        "    df = df.sort_values(by='timestamp')\n",
        "\n",
        "    # Add hour of day feature - sine/cosine encoding for cyclical pattern\n",
        "    df['hour_sin'] = np.sin(2 * np.pi * df['timestamp'].dt.hour / 24)\n",
        "    df['hour_cos'] = np.cos(2 * np.pi * df['timestamp'].dt.hour / 24)\n",
        "\n",
        "    # Add day of year feature - sine/cosine encoding for cyclical pattern\n",
        "    df['day_sin'] = np.sin(2 * np.pi * df['timestamp'].dt.dayofyear / 365.25)\n",
        "    df['day_cos'] = np.cos(2 * np.pi * df['timestamp'].dt.dayofyear / 365.25)\n",
        "\n",
        "    # Define region mapping for stations\n",
        "    region_mapping = {\n",
        "        'San Francisco': 'coastal',\n",
        "        'Mammoth Lakes': 'mountain',\n",
        "        'Palm Spring': 'desert',\n",
        "        'Fresno': 'valley',\n",
        "        'LA Downtown': 'urban'\n",
        "    }\n",
        "\n",
        "    # Add region information\n",
        "    if 'station_id' in df.columns:\n",
        "        df['region'] = df['station_id'].map(region_mapping)\n",
        "\n",
        "        # Filter to keep only one station per region (5 stations total)\n",
        "        kept_stations = list(region_mapping.keys())\n",
        "        df = df[df['station_id'].isin(kept_stations)]\n",
        "\n",
        "        # Convert region to numerical encoding\n",
        "        region_to_num = {region: i for i, region in enumerate(df['region'].unique())}\n",
        "        df['region_code'] = df['region'].map(region_to_num)\n",
        "\n",
        "        # Add elevation as a numerical topographic feature\n",
        "        elevation_mapping = {\n",
        "            'San Francisco': 16,      # meters\n",
        "            'Mammoth Lakes': 2500,    # meters\n",
        "            'Palm Spring': 146,      # meters\n",
        "            'Fresno': 99,             # meters\n",
        "            'LA Downtwon': 93         # meters\n",
        "        }\n",
        "        df['elevation'] = df['station_id'].map(elevation_mapping)\n",
        "        # Normalize elevation\n",
        "        df['elevation_norm'] = (df['elevation'] - df['elevation'].min()) / (df['elevation'].max() - df['elevation'].min())\n",
        "\n",
        "        print(f\"Filtered data to {len(kept_stations)} stations: {kept_stations}\")\n",
        "\n",
        "    # Interpolate missing values along the time dimension\n",
        "    df.interpolate(method='linear', limit_direction='both', inplace=True)\n",
        "\n",
        "    return df\n",
        "\n",
        "def extract_target_days(df):\n",
        "    \"\"\"\n",
        "    Extract the 4 specific target days (one per season) for predictions\n",
        "    \"\"\"\n",
        "    # Define target days\n",
        "    target_days = [\n",
        "        {'season': 'Spring', 'month': 4, 'day': 15},  # April 15\n",
        "        {'season': 'Summer', 'month': 7, 'day': 20},  # July 20\n",
        "        {'season': 'Fall', 'month': 10, 'day': 10},   # October 10\n",
        "        {'season': 'Winter', 'month': 1, 'day': 15}   # January 15\n",
        "    ]\n",
        "\n",
        "    # Filter for each target day\n",
        "    target_data = {}\n",
        "    for target in target_days:\n",
        "        # Filter by month and day\n",
        "        day_data = df[(df['timestamp'].dt.month == target['month']) &\n",
        "                       (df['timestamp'].dt.day == target['day'])]\n",
        "\n",
        "        # Get the most recent year that has data for this day\n",
        "        if not day_data.empty:\n",
        "            latest_year = day_data['timestamp'].dt.year.max()\n",
        "            target_day_data = day_data[day_data['timestamp'].dt.year == latest_year]\n",
        "            target_data[target['season']] = target_day_data\n",
        "            print(f\"Found {len(target_day_data)} records for {target['season']} target day ({target['month']}/{target['day']}/{latest_year})\")\n",
        "        else:\n",
        "            print(f\"WARNING: No data found for {target['season']} target day\")\n",
        "\n",
        "    return target_data\n",
        "\n",
        "def prepare_seasonal_training_data(df, target_days):\n",
        "    \"\"\"\n",
        "    For each target day, prepare all historical data for training\n",
        "    \"\"\"\n",
        "    training_sets = {}\n",
        "\n",
        "    for season, target_day_data in target_days.items():\n",
        "        if target_day_data.empty:\n",
        "            continue\n",
        "\n",
        "        # Get the date of this target\n",
        "        sample_date = target_day_data['timestamp'].iloc[0]\n",
        "        target_year = sample_date.year\n",
        "\n",
        "        # Use all historical data prior to the target year\n",
        "        historical_data = df[df['timestamp'].dt.year < target_year]\n",
        "\n",
        "        training_sets[season] = historical_data\n",
        "        print(f\"{season} training set: {len(historical_data)} samples from all historical data\")\n",
        "\n",
        "    return training_sets\n",
        "\n",
        "def normalize_features(train_df, val_df, feature_cols):\n",
        "    \"\"\"\n",
        "    Normalize features using StandardScaler fitted on training data\n",
        "    \"\"\"\n",
        "    scaler = StandardScaler()\n",
        "\n",
        "    # Fit on training data\n",
        "    scaler.fit(train_df[feature_cols])\n",
        "\n",
        "    # Transform datasets\n",
        "    train_scaled = scaler.transform(train_df[feature_cols])\n",
        "    val_scaled = scaler.transform(val_df[feature_cols])\n",
        "\n",
        "    # Convert back to DataFrames\n",
        "    train_norm = pd.DataFrame(train_scaled, columns=feature_cols, index=train_df.index)\n",
        "    val_norm = pd.DataFrame(val_scaled, columns=feature_cols, index=val_df.index)\n",
        "\n",
        "    return train_norm, val_norm, scaler\n",
        "\n",
        "class WeatherDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset for weather forecasting with sliding window approach.\n",
        "    Input: sequence of weather data\n",
        "    Output: next time step(s) temperature\n",
        "    \"\"\"\n",
        "    def __init__(self, df, station_ids, feature_cols, seq_length=24, forecast_horizon=24):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            df: DataFrame with weather data\n",
        "            station_ids: List of station IDs\n",
        "            feature_cols: List of feature columns to use as input\n",
        "            seq_length: Length of input sequence (in hours)\n",
        "            forecast_horizon: How many hours ahead to predict\n",
        "        \"\"\"\n",
        "        self.df = df\n",
        "        self.station_ids = station_ids\n",
        "        self.feature_cols = feature_cols\n",
        "        self.seq_length = seq_length\n",
        "        self.forecast_horizon = forecast_horizon\n",
        "        self.n_stations = len(station_ids)\n",
        "\n",
        "        # Group data by station_id for faster access\n",
        "        self.station_data = {station: df[df['station_id'] == station].sort_values('timestamp') \n",
        "                            for station in station_ids}\n",
        "        \n",
        "        # Get unique timestamps across all stations\n",
        "        all_timestamps = sorted(df['timestamp'].unique())\n",
        "        \n",
        "        # Find timestamps that have data for all stations\n",
        "        valid_timestamps = []\n",
        "        for ts in all_timestamps:\n",
        "            if all(len(self.station_data[station][self.station_data[station]['timestamp'] == ts]) > 0 \n",
        "                  for station in station_ids):\n",
        "                valid_timestamps.append(ts)\n",
        "        \n",
        "        self.timestamps = valid_timestamps\n",
        "        \n",
        "        # Find valid window starting indices\n",
        "        valid_idx = []\n",
        "        for i in range(len(self.timestamps) - (seq_length + forecast_horizon - 1)):\n",
        "            # Check if we have a continuous sequence\n",
        "            start_time = self.timestamps[i]\n",
        "            expected_times = [start_time + timedelta(hours=h) for h in range(seq_length + forecast_horizon)]\n",
        "            \n",
        "            # If all expected timestamps exist in our dataset\n",
        "            if all(t in self.timestamps for t in expected_times):\n",
        "                valid_idx.append(i)\n",
        "        \n",
        "        self.valid_indices = valid_idx\n",
        "        \n",
        "        if len(self.valid_indices) == 0:\n",
        "            print(f\"WARNING: No valid continuous windows found. Using relaxed requirements.\")\n",
        "            # Fall back to allowing any windows with at least input sequence length\n",
        "            valid_idx = []\n",
        "            for i in range(len(self.timestamps) - seq_length):\n",
        "                valid_idx.append(i)\n",
        "            self.valid_indices = valid_idx\n",
        "            self.fallback_mode = True\n",
        "            print(f\"Found {len(self.valid_indices)} windows with relaxed continuity requirements\")\n",
        "        else:\n",
        "            self.fallback_mode = False\n",
        "            print(f\"Created dataset with {len(self.valid_indices)} valid continuous windows\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return max(1, len(self.valid_indices))  # Ensure length is at least 1\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if len(self.valid_indices) == 0:\n",
        "            # Return dummy data if no valid indices\n",
        "            X = np.zeros((len(self.feature_cols), self.n_stations, self.seq_length))\n",
        "            y = np.zeros((self.n_stations, self.forecast_horizon))\n",
        "            static_features = np.zeros((self.n_stations, 2))  # region_code and elevation\n",
        "            return (torch.FloatTensor(X), torch.FloatTensor(static_features)), torch.FloatTensor(y)\n",
        "\n",
        "        # Get actual data when possible\n",
        "        start_idx = self.valid_indices[idx % len(self.valid_indices)]\n",
        "\n",
        "        # Get timestamps for input and output windows\n",
        "        input_timestamps = self.timestamps[start_idx:start_idx + self.seq_length]\n",
        "        output_timestamps = []\n",
        "        \n",
        "        # For fallback mode, just get as many valid output timestamps as possible\n",
        "        if self.fallback_mode:\n",
        "            next_idx = start_idx + self.seq_length\n",
        "            while len(output_timestamps) < self.forecast_horizon and next_idx < len(self.timestamps):\n",
        "                output_timestamps.append(self.timestamps[next_idx])\n",
        "                next_idx += 1\n",
        "        else:\n",
        "            # Normal mode - get consecutive timestamps\n",
        "            output_timestamps = self.timestamps[start_idx + self.seq_length:\n",
        "                                            start_idx + self.seq_length + self.forecast_horizon]\n",
        "\n",
        "        # Handle potential shortfall in output window\n",
        "        if len(output_timestamps) < self.forecast_horizon:\n",
        "            # Pad with repetition of last timestamp if needed\n",
        "            last_time = output_timestamps[-1] if len(output_timestamps) > 0 else input_timestamps[-1]\n",
        "            padding = [last_time] * (self.forecast_horizon - len(output_timestamps))\n",
        "            output_timestamps = list(output_timestamps) + padding\n",
        "\n",
        "        # Initialize tensors\n",
        "        # [features, stations, time]\n",
        "        X = np.zeros((len(self.feature_cols), self.n_stations, self.seq_length))\n",
        "        # [stations, forecast_horizon]\n",
        "        y = np.zeros((self.n_stations, self.forecast_horizon))\n",
        "\n",
        "        # Static features for each station\n",
        "        static_features = np.zeros((self.n_stations, 2))  # region_code and elevation\n",
        "\n",
        "        # Fill in data for each station\n",
        "        for s_idx, station_id in enumerate(self.station_ids):\n",
        "            station_df = self.station_data[station_id]\n",
        "            \n",
        "            # Get static features (same for all timestamps)\n",
        "            station_static = station_df.iloc[0]\n",
        "            static_features[s_idx, 0] = station_static['region_code']\n",
        "            static_features[s_idx, 1] = station_static['elevation_norm']\n",
        "            \n",
        "            # Input sequence\n",
        "            for t_idx, ts in enumerate(input_timestamps):\n",
        "                station_data = station_df[station_df['timestamp'] == ts]\n",
        "                \n",
        "                if not station_data.empty:\n",
        "                    for f_idx, feat in enumerate(self.feature_cols):\n",
        "                        X[f_idx, s_idx, t_idx] = station_data[feat].values[0]\n",
        "\n",
        "            # Target sequence (temperature only)\n",
        "            for t_idx, ts in enumerate(output_timestamps):\n",
        "                if t_idx < self.forecast_horizon:  # Safety check\n",
        "                    station_data = station_df[station_df['timestamp'] == ts]\n",
        "                    \n",
        "                    if not station_data.empty:\n",
        "                        y[s_idx, t_idx] = station_data['Temperature_C'].values[0]\n",
        "                    # If no data available, keep the initialized zero value\n",
        "\n",
        "        return (torch.FloatTensor(X), torch.FloatTensor(static_features)), torch.FloatTensor(y)\n",
        "# ============================================================\n",
        "# TEMPORAL FUSION TRANSFORMER IMPLEMENTATION\n",
        "# ============================================================\n",
        "class TemporalSelfAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Multi-head self-attention layer for temporal data.\n",
        "    Simplified from the original TFT paper.\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, n_heads=2, dropout=0.1):\n",
        "        super(TemporalSelfAttention, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = d_model // n_heads\n",
        "\n",
        "        self.query = nn.Linear(d_model, d_model)\n",
        "        self.key = nn.Linear(d_model, d_model)\n",
        "        self.value = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.out = nn.Linear(d_model, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_length, _ = x.size()\n",
        "\n",
        "        # Linear projections\n",
        "        queries = self.query(x).view(batch_size, seq_length, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        keys = self.key(x).view(batch_size, seq_length, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        values = self.value(x).view(batch_size, seq_length, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        # Attention scores\n",
        "        scores = torch.matmul(queries, keys.transpose(-2, -1)) / np.sqrt(self.head_dim)\n",
        "        attention = F.softmax(scores, dim=-1)\n",
        "        attention = self.dropout(attention)\n",
        "\n",
        "        # Apply attention to values\n",
        "        out = torch.matmul(attention, values)\n",
        "        out = out.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
        "\n",
        "        # Final linear layer\n",
        "        return self.out(out)\n",
        "\n",
        "class GatedResidualNetwork(nn.Module):\n",
        "    \"\"\"\n",
        "    Gated Residual Network as described in the TFT paper.\n",
        "    Simplified version with fewer layers.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size, hidden_size, output_size, dropout=0.1):\n",
        "        super(GatedResidualNetwork, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # If input and output sizes are different, apply a skip connection\n",
        "        self.skip_layer = None\n",
        "        if input_size != output_size:\n",
        "            self.skip_layer = nn.Linear(input_size, output_size)\n",
        "\n",
        "        # Main layers\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
        "        self.gate = nn.Linear(input_size + output_size, output_size)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.layer_norm = nn.LayerNorm(output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Main branch\n",
        "        hidden = F.elu(self.fc1(x))\n",
        "        hidden = self.dropout(hidden)\n",
        "        hidden = self.fc2(hidden)\n",
        "\n",
        "        # Skip connection\n",
        "        if self.skip_layer is not None:\n",
        "            skip = self.skip_layer(x)\n",
        "        else:\n",
        "            skip = x\n",
        "\n",
        "        # Gate mechanism\n",
        "        gate_input = torch.cat([x, hidden], dim=-1)\n",
        "        gate = torch.sigmoid(self.gate(gate_input))\n",
        "\n",
        "        # Combine using gate\n",
        "        output = gate * hidden + (1 - gate) * skip\n",
        "\n",
        "        # Layer normalization\n",
        "        return self.layer_norm(output)\n",
        "\n",
        "class VariableSelectionNetwork(nn.Module):\n",
        "    \"\"\"\n",
        "    Variable Selection Network for TFT.\n",
        "    Simplified version with fewer layers.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size_per_var, num_vars, hidden_size, output_size, dropout=0.1):\n",
        "        super(VariableSelectionNetwork, self).__init__()\n",
        "        self.input_size_per_var = input_size_per_var\n",
        "        self.num_vars = num_vars\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "\n",
        "        # GRN for variable weights\n",
        "        self.weight_grn = GatedResidualNetwork(\n",
        "            input_size=input_size_per_var * num_vars,\n",
        "            hidden_size=hidden_size,\n",
        "            output_size=num_vars,\n",
        "            dropout=dropout\n",
        "        )\n",
        "\n",
        "        # GRN for each variable\n",
        "        self.var_grns = nn.ModuleList([\n",
        "            GatedResidualNetwork(\n",
        "                input_size=input_size_per_var,\n",
        "                hidden_size=hidden_size,\n",
        "                output_size=output_size,\n",
        "                dropout=dropout\n",
        "            ) for _ in range(num_vars)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: [batch_size, num_vars, input_size_per_var]\n",
        "        batch_size = x.size(0)\n",
        "        flat_x = x.view(batch_size, -1)\n",
        "\n",
        "        # Calculate variable weights\n",
        "        var_weights = self.weight_grn(flat_x)\n",
        "        var_weights = F.softmax(var_weights, dim=-1).unsqueeze(-1)  # [batch_size, num_vars, 1]\n",
        "\n",
        "        # Transform each variable\n",
        "        var_outputs = []\n",
        "        for i in range(self.num_vars):\n",
        "            var_outputs.append(self.var_grns[i](x[:, i]))\n",
        "\n",
        "        var_outputs = torch.stack(var_outputs, dim=1)  # [batch_size, num_vars, output_size]\n",
        "\n",
        "        # Weighted combination\n",
        "        outputs = torch.sum(var_outputs * var_weights, dim=1)  # [batch_size, output_size]\n",
        "\n",
        "        return outputs, var_weights\n",
        "\n",
        "class TemporalFusionTransformer(nn.Module):\n",
        "    \"\"\"\n",
        "    Simplified Temporal Fusion Transformer for temperature forecasting.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_features, num_stations, hidden_size=64, num_heads=2, dropout=0.1, forecast_horizon=24):\n",
        "        super(TemporalFusionTransformer, self).__init__()\n",
        "        self.num_features = num_features\n",
        "        self.num_stations = num_stations\n",
        "        self.hidden_size = hidden_size\n",
        "        self.forecast_horizon = forecast_horizon  # Store forecast horizon\n",
        "\n",
        "        # Static variable processing (region_code, elevation)\n",
        "        self.static_var_processor = GatedResidualNetwork(\n",
        "            input_size=2,\n",
        "            hidden_size=hidden_size,\n",
        "            output_size=hidden_size,\n",
        "            dropout=dropout\n",
        "        )\n",
        "\n",
        "        # Variable selection for time-varying features\n",
        "        self.temporal_var_selection = VariableSelectionNetwork(\n",
        "            input_size_per_var=24,  # Sequence length per feature\n",
        "            num_vars=num_features,\n",
        "            hidden_size=hidden_size,\n",
        "            output_size=hidden_size,\n",
        "            dropout=dropout\n",
        "        )\n",
        "\n",
        "        # LSTM encoder\n",
        "        self.lstm_encoder = nn.LSTM(\n",
        "            input_size=hidden_size,\n",
        "            hidden_size=hidden_size,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        # Temporal self-attention\n",
        "        self.self_attention = TemporalSelfAttention(\n",
        "            d_model=hidden_size,\n",
        "            n_heads=num_heads,\n",
        "            dropout=dropout\n",
        "        )\n",
        "\n",
        "        # Final output layers\n",
        "        self.fc1 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, forecast_horizon)  # 24-hour forecast\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # Unpack inputs\n",
        "        temporal_features, static_features = inputs\n",
        "        batch_size = temporal_features.size(0)\n",
        "\n",
        "        # [batch, features, stations, time] -> [batch*stations, features, time]\n",
        "        temporal_features = temporal_features.permute(0, 2, 1, 3)\n",
        "        temporal_features = temporal_features.reshape(batch_size * self.num_stations, self.num_features, -1)\n",
        "\n",
        "        # Static features: [batch, stations, static_dims] -> [batch*stations, static_dims]\n",
        "        static_features = static_features.reshape(batch_size * self.num_stations, -1)\n",
        "\n",
        "        # Process static features\n",
        "        static_embeddings = self.static_var_processor(static_features)\n",
        "\n",
        "        # Process temporal features with variable selection\n",
        "        temporal_embeddings, temporal_weights = self.temporal_var_selection(temporal_features)\n",
        "\n",
        "        # Reshape to [batch*stations, seq_len, hidden]\n",
        "        temporal_embeddings = temporal_embeddings.unsqueeze(1).expand(-1, 24, -1)\n",
        "\n",
        "        # Add static embeddings to each timestep\n",
        "        temporal_embeddings = temporal_embeddings + static_embeddings.unsqueeze(1)\n",
        "\n",
        "        # LSTM encoding\n",
        "        lstm_out, _ = self.lstm_encoder(temporal_embeddings)\n",
        "\n",
        "        # Self-attention\n",
        "        attention_out = self.self_attention(lstm_out)\n",
        "\n",
        "        # Final prediction\n",
        "        outputs = F.relu(self.fc1(attention_out))\n",
        "        outputs = self.fc2(outputs)\n",
        "\n",
        "        # Take the last 24 timesteps for the forecast\n",
        "        forecast = outputs[:, -24:, 0]\n",
        "\n",
        "        # Reshape back to [batch, stations, horizon]\n",
        "        forecast = forecast.reshape(batch_size, self.num_stations, -1)\n",
        "\n",
        "        return forecast\n",
        "\n",
        "# ============================================================\n",
        "# TRAINING AND EVALUATION FUNCTIONS\n",
        "# ============================================================\n",
        "def train_model(model, train_loader, val_loader, learning_rate=0.001, epochs=20, patience=5):\n",
        "    \"\"\"\n",
        "    Train the model with early stopping based on validation loss.\n",
        "    \"\"\"\n",
        "    model.to(device)\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode='min', factor=0.5, patience=2, verbose=True\n",
        "    )\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    epoch_times = []\n",
        "\n",
        "    print(f\"Starting training for {epochs} epochs with patience {patience}...\")\n",
        "    total_start_time = time.time()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        epoch_start_time = time.time()\n",
        "        # Training\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        train_batches = 0\n",
        "\n",
        "        batch_times = []\n",
        "        for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "            batch_start = time.time()\n",
        "            # Move to device - handle different types of input structures\n",
        "            if isinstance(inputs, tuple):\n",
        "                inputs = tuple(x.to(device) for x in inputs)\n",
        "            elif isinstance(inputs, list):\n",
        "                inputs = [x.to(device) if isinstance(x, torch.Tensor) else x for x in inputs]\n",
        "            else:\n",
        "                inputs = inputs.to(device)\n",
        "\n",
        "            targets = targets.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            train_batches += 1\n",
        "            \n",
        "            batch_end = time.time()\n",
        "            batch_time = batch_end - batch_start\n",
        "            batch_times.append(batch_time)\n",
        "            \n",
        "            # Print progress every 10 batches\n",
        "            if (batch_idx + 1) % 10 == 0:\n",
        "                print(f\"  Batch {batch_idx+1}/{len(train_loader)}, Loss: {loss.item():.4f}, Time: {batch_time:.3f}s\")\n",
        "\n",
        "        avg_train_loss = train_loss / max(1, train_batches)\n",
        "        train_losses.append(avg_train_loss)\n",
        "        \n",
        "        avg_batch_time = sum(batch_times) / len(batch_times) if batch_times else 0\n",
        "        \n",
        "        # Validation\n",
        "        val_start_time = time.time()\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        val_batches = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, targets in val_loader:\n",
        "                # Move to device\n",
        "                if isinstance(inputs, tuple):\n",
        "                    inputs = tuple(x.to(device) for x in inputs)\n",
        "                else:\n",
        "                    inputs = inputs.to(device)\n",
        "                targets = targets.to(device)\n",
        "\n",
        "                # Forward pass\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, targets)\n",
        "\n",
        "                val_loss += loss.item()\n",
        "                val_batches += 1\n",
        "\n",
        "        avg_val_loss = val_loss / max(1, val_batches)\n",
        "        val_losses.append(avg_val_loss)\n",
        "        \n",
        "        val_time = time.time() - val_start_time\n",
        "        epoch_end_time = time.time()\n",
        "        epoch_time = epoch_end_time - epoch_start_time\n",
        "        epoch_times.append(epoch_time)\n",
        "        \n",
        "        # Calculate estimated time remaining\n",
        "        avg_epoch_time = sum(epoch_times) / len(epoch_times)\n",
        "        remaining_epochs = epochs - (epoch + 1)\n",
        "        est_time_remaining = avg_epoch_time * remaining_epochs\n",
        "        \n",
        "        # Print detailed progress\n",
        "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
        "        print(f\"  Train Loss: {avg_train_loss:.4f} ({train_batches} batches, avg batch time: {avg_batch_time:.3f}s)\")\n",
        "        print(f\"  Val Loss: {avg_val_loss:.4f} (validation time: {val_time:.2f}s)\")\n",
        "        print(f\"  Epoch Time: {epoch_time:.2f}s, Est. Remaining: {est_time_remaining/60:.2f} minutes\")\n",
        "        print(f\"  Elapsed Time: {(epoch_end_time - total_start_time)/60:.2f} minutes\")\n",
        "\n",
        "        # Learning rate scheduler\n",
        "        scheduler.step(avg_val_loss)\n",
        "\n",
        "        # Early stopping\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            patience_counter = 0\n",
        "            # Save best model\n",
        "            try:\n",
        "                torch.save(model.state_dict(), 'best_tft_model.pth')\n",
        "                print(f\"  Saved best model with val loss: {best_val_loss:.4f}\")\n",
        "            except Exception as e:\n",
        "                print(f\"  Error saving model: {e}\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            print(f\"  No improvement for {patience_counter}/{patience} epochs\")\n",
        "            if patience_counter >= patience:\n",
        "                print(f\"Early stopping at epoch {epoch+1}\")\n",
        "                break\n",
        "\n",
        "    total_time = time.time() - total_start_time\n",
        "    print(f\"Training completed in {total_time/60:.2f} minutes ({total_time:.1f} seconds)\")\n",
        "    print(f\"Average epoch time: {sum(epoch_times)/len(epoch_times):.2f} seconds\")\n",
        "\n",
        "    # Load best model\n",
        "    try:\n",
        "        model.load_state_dict(torch.load('best_tft_model.pth'))\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading best model: {e}\")\n",
        "\n",
        "    return model, train_losses, val_losses\n",
        "\n",
        "def evaluate_model(model, data_loader, station_ids, regions):\n",
        "    \"\"\"\n",
        "    Evaluate the model and calculate metrics.\n",
        "    \"\"\"\n",
        "    # Check if GPU is available\n",
        "    #device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "    station_preds = {station: [] for station in station_ids}\n",
        "    station_targets = {station: [] for station in station_ids}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in data_loader:\n",
        "            # Move to device - handle different types of input structures\n",
        "            if isinstance(inputs, tuple):\n",
        "                inputs = tuple(x.to(device) for x in inputs)\n",
        "            elif isinstance(inputs, list):\n",
        "                inputs = [x.to(device) if isinstance(x, torch.Tensor) else x for x in inputs]\n",
        "            else:\n",
        "                inputs = inputs.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Move to CPU for further processing\n",
        "            outputs = outputs.cpu().numpy()\n",
        "            targets = targets.numpy()\n",
        "\n",
        "            all_preds.append(outputs)\n",
        "            all_targets.append(targets)\n",
        "\n",
        "            # Store predictions by station\n",
        "            for i, station in enumerate(station_ids):\n",
        "                station_preds[station].append(outputs[:, i, :])\n",
        "                station_targets[station].append(targets[:, i, :])\n",
        "\n",
        "    # Concatenate predictions and targets\n",
        "    all_preds = np.concatenate(all_preds, axis=0)\n",
        "    all_targets = np.concatenate(all_targets, axis=0)\n",
        "\n",
        "    # Calculate overall metrics\n",
        "    rmse = np.sqrt(mean_squared_error(all_targets.flatten(), all_preds.flatten()))\n",
        "    mae = mean_absolute_error(all_targets.flatten(), all_preds.flatten())\n",
        "    r2 = r2_score(all_targets.flatten(), all_preds.flatten())\n",
        "\n",
        "    print(f\"Overall Metrics - RMSE: {rmse:.4f}, MAE: {mae:.4f}, R²: {r2:.4f}\")\n",
        "\n",
        "    # Calculate metrics by station/region\n",
        "    station_metrics = {}\n",
        "    for i, station in enumerate(station_ids):\n",
        "        station_pred = np.concatenate([p[:, i, :] for p in all_preds], axis=0).flatten()\n",
        "        station_target = np.concatenate([t[:, i, :] for t in all_targets], axis=0).flatten()\n",
        "\n",
        "        station_rmse = np.sqrt(mean_squared_error(station_target, station_pred))\n",
        "        station_mae = mean_absolute_error(station_target, station_pred)\n",
        "        station_r2 = r2_score(station_target, station_pred)\n",
        "\n",
        "        station_metrics[station] = {\n",
        "            'region': regions.get(station, 'Unknown'),\n",
        "            'rmse': station_rmse,\n",
        "            'mae': station_mae,\n",
        "            'r2': station_r2\n",
        "        }\n",
        "\n",
        "        print(f\"Station {station} ({regions.get(station, 'Unknown')}) - \"\n",
        "              f\"RMSE: {station_rmse:.4f}, MAE: {station_mae:.4f}, R²: {station_r2:.4f}\")\n",
        "\n",
        "    return rmse, mae, r2, station_metrics\n",
        "\n",
        "def visualize_predictions(model, data_loader, station_ids, regions, season):\n",
        "    \"\"\"\n",
        "    Visualize predictions for each station.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    if len(data_loader) == 0:\n",
        "        print(\"No data available for visualization\")\n",
        "        return\n",
        "    \n",
        "    # Get predictions\n",
        "    try:\n",
        "        for inputs, targets in data_loader:\n",
        "            # Only process one batch for visualization\n",
        "            if isinstance(inputs, tuple):\n",
        "                inputs = tuple(x.to(device) for x in inputs)\n",
        "            else:\n",
        "                inputs = inputs.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Move to CPU for plotting\n",
        "            outputs = outputs.cpu().numpy()\n",
        "            targets = targets.numpy()\n",
        "            break\n",
        "\n",
        "         # Check if we have data to plot\n",
        "        if 'outputs' not in locals():\n",
        "            print(\"No data was loaded from the dataloader\")\n",
        "            return\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing visualization data: {e}\")\n",
        "        return\n",
        "    \n",
        "    # Create subplots for each station\n",
        "    fig, axes = plt.subplots(len(station_ids), 1, figsize=(12, 3*len(station_ids)))\n",
        "    if len(station_ids) == 1:\n",
        "        axes = [axes]\n",
        "\n",
        "    hours = np.arange(24)\n",
        "\n",
        "    for i, station in enumerate(station_ids):\n",
        "        ax = axes[i]\n",
        "\n",
        "        # Plot actual vs predicted\n",
        "        ax.plot(hours, targets[0, i, :], 'b-', label='Actual')\n",
        "        ax.plot(hours, outputs[0, i, :], 'r--', label='Predicted')\n",
        "\n",
        "        ax.set_title(f\"{station} ({regions.get(station, 'Unknown')}) - {season}\")\n",
        "        ax.set_xlabel('Hour of Day')\n",
        "        ax.set_ylabel('Temperature (°C)')\n",
        "        ax.legend()\n",
        "        ax.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    plt.savefig(f\"{season}_predictions.png\")\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def analyze_topographic_performance(station_metrics, regions):\n",
        "    \"\"\"\n",
        "    Analyze model performance across different topographic regions.\n",
        "    \"\"\"\n",
        "    # Group metrics by region\n",
        "    region_metrics = {}\n",
        "    for station, metrics in station_metrics.items():\n",
        "        region = regions.get(station, 'Unknown')\n",
        "        if region not in region_metrics:\n",
        "            region_metrics[region] = []\n",
        "        region_metrics[region].append(metrics)\n",
        "\n",
        "    # Calculate average metrics by region\n",
        "    region_avg_metrics = {}\n",
        "    for region, metrics_list in region_metrics.items():\n",
        "        avg_rmse = np.mean([m['rmse'] for m in metrics_list])\n",
        "        avg_mae = np.mean([m['mae'] for m in metrics_list])\n",
        "        avg_r2 = np.mean([m['r2'] for m in metrics_list])\n",
        "\n",
        "        region_avg_metrics[region] = {\n",
        "            'avg_rmse': avg_rmse,\n",
        "            'avg_mae': avg_mae,\n",
        "            'avg_r2': avg_r2\n",
        "        }\n",
        "\n",
        "        print(f\"Region {region} - Avg RMSE: {avg_rmse:.4f}, Avg MAE: {avg_mae:.4f}, Avg R²: {avg_r2:.4f}\")\n",
        "\n",
        "        if not station_metrics:\n",
        "            print(\"No station metrics available for analysis\")\n",
        "            return {}\n",
        "\n",
        "    # Create bar chart comparing regions\n",
        "    regions = list(region_avg_metrics.keys())\n",
        "    rmse_values = [region_avg_metrics[r]['avg_rmse'] for r in regions]\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    bars = plt.bar(regions, rmse_values)\n",
        "\n",
        "    # Add styling\n",
        "    plt.title('RMSE by Topographic Region', fontsize=16)\n",
        "\n",
        "    plt.title('RMSE by Topographic Region', fontsize=16)\n",
        "    plt.ylabel('RMSE (°C)', fontsize=14)\n",
        "    plt.xlabel('Region', fontsize=14)\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # Add value labels on bars\n",
        "    for bar in bars:\n",
        "        height = bar.get_height()\n",
        "        plt.text(bar.get_x() + bar.get_width()/2., height + 0.1,\n",
        "                 f'{height:.2f}',\n",
        "                 ha='center', va='bottom', fontsize=12)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    plt.savefig('region_performance.png')\n",
        "    plt.close()\n",
        "\n",
        "    return region_avg_metrics\n",
        "\n",
        "def analyze_seasonal_performance(seasonal_results):\n",
        "    \"\"\"\n",
        "    Compare model performance across different seasons.\n",
        "    \"\"\"\n",
        "    seasons = list(seasonal_results.keys())\n",
        "    rmse_values = [results['rmse'] for results in seasonal_results.values()]\n",
        "    mae_values = [results['mae'] for results in seasonal_results.values()]\n",
        "\n",
        "    if not seasonal_results:\n",
        "        print(\"No seasonal results available for analysis\")\n",
        "        return [], [], []\n",
        "\n",
        "    # Create grouped bar chart\n",
        "    x = np.arange(len(seasons))\n",
        "    width = 0.35\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(12, 6))\n",
        "    ax.bar(x - width/2, rmse_values, width, label='RMSE')\n",
        "    ax.bar(x + width/2, mae_values, width, label='MAE')\n",
        "\n",
        "    ax.set_title('Model Performance by Season', fontsize=16)\n",
        "    ax.set_ylabel('Error (°C)', fontsize=14)\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(seasons)\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    # Add value labels\n",
        "    for i, v in enumerate(rmse_values):\n",
        "        ax.text(i - width/2, v + 0.1, f'{v:.2f}', ha='center', fontsize=10)\n",
        "\n",
        "    for i, v in enumerate(mae_values):\n",
        "        ax.text(i + width/2, v + 0.1, f'{v:.2f}', ha='center', fontsize=10)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    plt.savefig('seasonal_performance.png')\n",
        "    plt.close()\n",
        "\n",
        "    return seasons, rmse_values, mae_values\n",
        "\n",
        "# ============================================================\n",
        "# MAIN EXECUTION\n",
        "# ============================================================\n",
        "# ============================================================\n",
        "# MAIN EXECUTION\n",
        "# ============================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"California Weather Forecasting with Temporal Fusion Transformer\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "\n",
        "    df = load_data(DATA_PATH)\n",
        "    print(f\"Loaded data with {len(df)} records\")\n",
        "\n",
        "    # Extract target days (one per season) - keep this for evaluation\n",
        "    target_days = extract_target_days(df)\n",
        "\n",
        "    # Instead of preparing training data by season, use all data\n",
        "    print(\"\\nPreparing unified dataset from all historical data\")\n",
        "\n",
        "    # Get all stations that appear in the dataset\n",
        "    all_stations = df['station_id'].unique()\n",
        "    print(f\"Using {len(all_stations)} stations: {all_stations}\")\n",
        "\n",
        "    # Create region mapping\n",
        "    regions = {station: group for station, group in\n",
        "              zip(df['station_id'].unique(), df['region'].unique())}\n",
        "\n",
        "    # Define feature columns to use (unchanged)\n",
        "    feature_cols = [\n",
        "        'Temperature_C',\n",
        "        'HourlyRelativeHumidity',\n",
        "        'HourlyStationPressure',\n",
        "        'hour_sin', 'hour_cos',\n",
        "        'day_sin', 'day_cos'\n",
        "    ]\n",
        "\n",
        "    # Create a single unified dataset\n",
        "    full_dataset = WeatherDataset(\n",
        "        df=df,\n",
        "        station_ids=list(all_stations),\n",
        "        feature_cols=feature_cols,\n",
        "        seq_length=24,\n",
        "        forecast_horizon=24\n",
        "    )\n",
        "\n",
        "    # Split into train/val/test\n",
        "    dataset_size = len(full_dataset)\n",
        "    train_size = int(dataset_size * 0.7)\n",
        "    val_size = int(dataset_size * 0.15)\n",
        "    test_size = dataset_size - train_size - val_size\n",
        "\n",
        "    train_dataset, val_dataset, test_dataset = random_split(\n",
        "        full_dataset, [train_size, val_size, test_size])\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)  # For visualization\n",
        "\n",
        "    # Create and train model\n",
        "    model = TemporalFusionTransformer(\n",
        "        num_features=len(feature_cols),\n",
        "        num_stations=len(all_stations),\n",
        "        hidden_size=64,\n",
        "        num_heads=2,\n",
        "        dropout=0.1\n",
        "    )\n",
        "\n",
        "    # Train model\n",
        "    print(f\"Training unified model on all historical data...\")\n",
        "    model, train_losses, val_losses = train_model(\n",
        "        model=model,\n",
        "        train_loader=train_loader,\n",
        "        val_loader=val_loader,\n",
        "        learning_rate=0.001,\n",
        "        epochs=20,\n",
        "        patience=5\n",
        "    )\n",
        "\n",
        "    # Plot training curves\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(train_losses, label='Train Loss')\n",
        "    plt.plot(val_losses, label='Validation Loss')\n",
        "    plt.title('Training and Validation Loss - All Data')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss (MSE)')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.savefig('all_data_training_curve.png')\n",
        "    plt.close()\n",
        "\n",
        "    # Evaluate model\n",
        "    print(f\"Evaluating unified model...\")\n",
        "    rmse, mae, r2, station_metrics = evaluate_model(\n",
        "        model=model,\n",
        "        data_loader=test_loader,\n",
        "        station_ids=all_stations,\n",
        "        regions=regions\n",
        "    )\n",
        "\n",
        "    # Visualize predictions\n",
        "    visualize_predictions(\n",
        "        model=model,\n",
        "        data_loader=test_loader,\n",
        "        station_ids=all_stations,\n",
        "        regions=regions,\n",
        "        season=\"All Seasons\"\n",
        "    )\n",
        "\n",
        "    # Analyze performance by topography\n",
        "    region_metrics = analyze_topographic_performance(station_metrics, regions)\n",
        "\n",
        "    # Save final results to CSV\n",
        "    results_df = pd.DataFrame([{\n",
        "        'Dataset': 'All Data',\n",
        "        'RMSE': rmse,\n",
        "        'MAE': mae,\n",
        "        'R2': r2\n",
        "    }])\n",
        "\n",
        "    results_df.to_csv('unified_results.csv', index=False)\n",
        "    print(\"Summary results saved to unified_results.csv\")\n",
        "\n",
        "    # Generate detailed station-level results\n",
        "    station_results = []\n",
        "    for station, metrics in station_metrics.items():\n",
        "        station_results.append({\n",
        "            'Station': station,\n",
        "            'Region': metrics['region'],\n",
        "            'RMSE': metrics['rmse'],\n",
        "            'MAE': metrics['mae'],\n",
        "            'R2': metrics['r2']\n",
        "        })\n",
        "\n",
        "    station_results_df = pd.DataFrame(station_results)\n",
        "    station_results_df.to_csv('station_results.csv', index=False)\n",
        "    print(\"Station-level results saved to station_results.csv\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "pytorch_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
