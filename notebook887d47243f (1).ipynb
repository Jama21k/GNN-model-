{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11342745,"sourceType":"datasetVersion","datasetId":7096681},{"sourceId":11365940,"sourceType":"datasetVersion","datasetId":7114409},{"sourceId":11366236,"sourceType":"datasetVersion","datasetId":7114641},{"sourceId":11366666,"sourceType":"datasetVersion","datasetId":7114978},{"sourceId":11379404,"sourceType":"datasetVersion","datasetId":7124793}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# -*- coding: utf-8 -*-\n\"\"\"GNN Model.ipynb\n\nAutomatically generated by Colab.\n\nOriginal file is located at\n    https://colab.research.google.com/drive/1pSO-VEyn5Cywjw9sXKn2fjXdVbI3hAsH\n\"\"\"\n\n# -*- coding: utf-8 -*-\n\"\"\"Untitled2.ipynb\n\nAutomatically generated by Colab.\n\nOriginal file is located at\n    https://colab.research.google.com/drive/193StgLnr4doKklAxwBiQsVX3njEfb1oa\n\"\"\"\n\nDATA_PATH = \"/kaggle/input/final-data/Merged_Data.csv\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T12:46:55.710321Z","iopub.execute_input":"2025-04-14T12:46:55.711164Z","iopub.status.idle":"2025-04-14T12:46:55.716626Z","shell.execute_reply.started":"2025-04-14T12:46:55.711122Z","shell.execute_reply":"2025-04-14T12:46:55.715092Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"!pip install vmdpy\nfrom vmdpy import VMD ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T12:46:58.055802Z","iopub.execute_input":"2025-04-14T12:46:58.056079Z","iopub.status.idle":"2025-04-14T12:47:03.537527Z","shell.execute_reply.started":"2025-04-14T12:46:58.056061Z","shell.execute_reply":"2025-04-14T12:47:03.536243Z"}},"outputs":[{"name":"stdout","text":"Collecting vmdpy\n  Downloading vmdpy-0.2-py2.py3-none-any.whl.metadata (3.0 kB)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from vmdpy) (1.26.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->vmdpy) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->vmdpy) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->vmdpy) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->vmdpy) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->vmdpy) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->vmdpy) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->vmdpy) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->vmdpy) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->vmdpy) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->vmdpy) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->vmdpy) (2024.2.0)\nDownloading vmdpy-0.2-py2.py3-none-any.whl (6.5 kB)\nInstalling collected packages: vmdpy\nSuccessfully installed vmdpy-0.2\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader, random_split\nimport time\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nimport os\nfrom datetime import datetime, timedelta\nimport warnings\nimport sys\nsys.path.append('/kaggle/input/vmdpy')  # Adjust the path if needed\n\nwarnings.filterwarnings('ignore')\n\n\n# Check for GPU and set device appropriately for Kaggle T4\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\n# Set optimized CUDA options for T4 GPU\nif torch.cuda.is_available():\n    torch.backends.cudnn.benchmark = True\n    torch.backends.cudnn.deterministic = False  # Better performance but less reproducible\n    # Set to float16 precision for faster computation on T4 GPU\n    torch.set_float32_matmul_precision('high')\n\n# ============================================================\n# MODIFIED DATA LOADING AND PROCESSING FOR TARGETED SEASONAL ANALYSIS\n# ============================================================\n\ndef load_data(file_path):\n    \"\"\"\n    Load weather data, clean missing values, and filter to LA Downtown only.\n    \"\"\"\n    try:\n        # Determine file type and read\n        if file_path.endswith('.xlsx'):\n            df = pd.read_excel(file_path, engine='openpyxl')\n        else:\n            df = pd.read_csv(file_path, encoding='ISO-8859-1')\n    except Exception as e:\n        print(f\"Error loading data: {e}\")\n        return pd.DataFrame()  # Return empty DataFrame\n\n    # Convert timestamp\n    if 'timestamp' in df.columns:\n        df['timestamp'] = pd.to_datetime(df['timestamp'])\n    elif 'DATE' in df.columns:\n        df['timestamp'] = pd.to_datetime(df['DATE'])\n        df = df.rename(columns={'DATE': 'date_original'})\n\n    # Ensure time-based ordering before interpolation\n    df = df.sort_values(by='timestamp')\n\n    # Add hour of day feature - sine/cosine encoding for cyclical pattern\n    df['hour_sin'] = np.sin(2 * np.pi * df['timestamp'].dt.hour / 24)\n    df['hour_cos'] = np.cos(2 * np.pi * df['timestamp'].dt.hour / 24)\n\n    # Add day of year feature - sine/cosine encoding for cyclical pattern\n    df['day_sin'] = np.sin(2 * np.pi * df['timestamp'].dt.dayofyear / 365.25)\n    df['day_cos'] = np.cos(2 * np.pi * df['timestamp'].dt.dayofyear / 365.25)\n\n    # Define region mapping for LA Downtown only\n    region_mapping = {\n        'LA Downtown': 'urban'\n    }\n\n    # Filter to keep only LA Downtown\n    if 'station_id' in df.columns:\n        # Filter to LA Downtown only\n        df = df[df['station_id'] == 'LA Downtown']\n        \n        # Add region information\n        df['region'] = df['station_id'].map(region_mapping)\n\n        # Convert region to numerical encoding\n        region_to_num = {region: i for i, region in enumerate(df['region'].unique())}\n        df['region_code'] = df['region'].map(region_to_num)\n\n        # Add elevation for LA Downtown\n        elevation_mapping = {\n            'LA Downtown': 93         # meters\n        }\n        df['elevation'] = df['station_id'].map(elevation_mapping)\n        # Normalize elevation (will be constant for single station)\n        df['elevation_norm'] = 0.0  # Since we only have one station, just use 0 as normalized value\n\n        print(f\"Filtered data to LA Downtown only\")\n\n    # Interpolate missing values along the time dimension\n    df.interpolate(method='linear', limit_direction='both', inplace=True)\n\n    return df\n\n# ============================================================\n# VMD DECOMPOSITION\n# ============================================================\n\nimport multiprocessing\nimport joblib\nimport os\n\ndef perform_vmd_decomposition(signal, alpha=2000, tau=0, K=10, DC=0, init=1, tol=1e-7):\n    \"\"\"\n    Perform VMD decomposition on a signal.\n    \"\"\"\n    # Run VMD\n    u, u_hat, omega = VMD(signal, alpha, tau, K, DC, init, tol)\n    \n    # Calculate reconstruction residual\n    recon = np.sum(u, axis=0)\n    rres = np.sqrt(np.mean((signal - recon)**2)) / np.sqrt(np.mean(signal**2)) * 100\n    \n    return u, rres\n\ndef find_optimal_k(signal, k_range=range(6, 15)):\n    \"\"\"\n    Find optimal K where rres < 3% with no sharp drop.\n    \"\"\"\n    results = []\n    \n    for k in k_range:\n        print(f\"  Testing K={k}\")\n        _, rres = perform_vmd_decomposition(signal, K=k)\n        results.append((k, rres))\n        print(f\"  K={k}, rres={rres:.2f}%\")\n        if rres < 3.0:\n            return k, rres\n            \n    # If no K achieves rres < 3%, return the K with lowest rres\n    return min(results, key=lambda x: x[1])\n\ndef decompose_station_data(data, feature_name, station_id, cache_dir='vmd_cache'):\n    \"\"\"\n    Decompose time series data for a single station.\n    \"\"\"\n    print(f\"Starting decomposition for {station_id} - {feature_name}\")\n    \n    # Create cache directory if it doesn't exist\n    os.makedirs(cache_dir, exist_ok=True)\n    \n    # Create a unique cache filename\n    cache_file = f\"{cache_dir}/vmd_{station_id}_{feature_name}.joblib\"\n    \n    # Check if cached results exist\n    if os.path.exists(cache_file):\n        print(f\"Loading cached VMD decomposition for {station_id} - {feature_name}\")\n        return joblib.load(cache_file)\n    \n    # Get the time series to decompose\n    print(f\"  Extracting signal for {station_id}\")\n    signal = data[feature_name].values\n    \n    # Normalize signal\n    print(f\"  Normalizing signal\")\n    signal_norm = (signal - np.mean(signal)) / np.std(signal)\n    \n    # Find optimal K\n    print(f\"  Finding optimal K\")\n    k_opt, rres = find_optimal_k(signal_norm)\n    print(f\"Station {station_id}, feature {feature_name}: Optimal K={k_opt}, rres={rres:.2f}%\")\n    \n    # Perform VMD with optimal K\n    print(f\"  Performing VMD with K={k_opt}\")\n    u, _ = perform_vmd_decomposition(signal_norm, K=k_opt)\n    \n    # Denormalize modes\n    print(f\"  Denormalizing modes\")\n    u_denorm = u * np.std(signal) + np.mean(signal) / k_opt\n    \n    # Save to cache\n    print(f\"  Saving to cache: {cache_file}\")\n    joblib.dump(u_denorm, cache_file)\n    \n    print(f\"Completed decomposition for {station_id} - {feature_name}\")\n    return u_denorm\n\ndef parallel_vmd_decomposition(df, feature_cols_to_decompose=['Temperature_C']):\n    \"\"\"\n    Apply VMD decomposition to multiple stations serially to avoid multiprocessing issues.\n    \"\"\"\n    # Get the unique station IDs \n    station_ids = ['LA Downtown']  # We only have LA Downtown in this code\n    \n    decomposed_data = {}\n    \n    for feature in feature_cols_to_decompose:\n        print(f\"Performing VMD decomposition for feature: {feature}\")\n        \n        # Process each station serially instead of in parallel\n        results = []\n        for station_id in station_ids:\n            print(f\"  Processing station: {station_id}\")\n            result = decompose_station_data(df, feature, station_id)\n            results.append(result)\n            \n        # Store results\n        for i, station_id in enumerate(station_ids):\n            if station_id not in decomposed_data:\n                decomposed_data[station_id] = {}\n            decomposed_data[station_id][feature] = results[i]\n    \n    return decomposed_data\n\ndef extract_target_days(df):\n    \"\"\"\n    Extract the 4 specific target days (one per season) for predictions\n    \"\"\"\n    # Define target days\n    target_days = [\n        {'season': 'Spring', 'month': 4, 'day': 15},  # April 15\n        {'season': 'Summer', 'month': 7, 'day': 20},  # July 20\n        {'season': 'Fall', 'month': 10, 'day': 10},   # October 10\n        {'season': 'Winter', 'month': 1, 'day': 15}   # January 15\n    ]\n\n    # Filter for each target day\n    target_data = {}\n    for target in target_days:\n        # Filter by month and day\n        day_data = df[(df['timestamp'].dt.month == target['month']) &\n                       (df['timestamp'].dt.day == target['day'])]\n\n        # Get the most recent year that has data for this day\n        if not day_data.empty:\n            latest_year = day_data['timestamp'].dt.year.max()\n            target_day_data = day_data[day_data['timestamp'].dt.year == latest_year]\n            target_data[target['season']] = target_day_data\n            print(f\"Found {len(target_day_data)} records for {target['season']} target day ({target['month']}/{target['day']}/{latest_year})\")\n        else:\n            print(f\"WARNING: No data found for {target['season']} target day\")\n\n    return target_data\n\ndef prepare_seasonal_training_data(df, target_days):\n    \"\"\"\n    For each target day, prepare all historical data for training\n    \"\"\"\n    training_sets = {}\n\n    for season, target_day_data in target_days.items():\n        if target_day_data.empty:\n            continue\n\n        # Get the date of this target\n        sample_date = target_day_data['timestamp'].iloc[0]\n        target_year = sample_date.year\n\n        # Use all historical data prior to the target year\n        historical_data = df[df['timestamp'].dt.year < target_year]\n\n        training_sets[season] = historical_data\n        print(f\"{season} training set: {len(historical_data)} samples from all historical data\")\n\n    return training_sets\n\ndef normalize_features(train_df, val_df, feature_cols):\n    \"\"\"\n    Normalize features using StandardScaler fitted on training data\n    \"\"\"\n    scaler = StandardScaler()\n\n    # Fit on training data\n    scaler.fit(train_df[feature_cols])\n\n    # Transform datasets\n    train_scaled = scaler.transform(train_df[feature_cols])\n    val_scaled = scaler.transform(val_df[feature_cols])\n\n    # Convert back to DataFrames\n    train_norm = pd.DataFrame(train_scaled, columns=feature_cols, index=train_df.index)\n    val_norm = pd.DataFrame(val_scaled, columns=feature_cols, index=val_df.index)\n\n    return train_norm, val_norm, scaler\n\nclass WeatherDataset(Dataset):\n    \"\"\"\n    Dataset for weather forecasting with sliding window approach.\n    Modified to work efficiently with single station (LA Downtown).\n    \"\"\"\n    def __init__(self, df, station_ids, feature_cols, seq_length=24, forecast_horizon=24):\n        \"\"\"\n        Args:\n            df: DataFrame with weather data (filtered to LA Downtown only)\n            station_ids: List containing only 'LA Downtown'\n            feature_cols: List of feature columns to use as input\n            seq_length: Length of input sequence (in hours)\n            forecast_horizon: How many hours ahead to predict\n        \"\"\"\n        self.df = df\n        self.station_ids = station_ids\n        self.feature_cols = feature_cols\n        self.seq_length = seq_length\n        self.forecast_horizon = forecast_horizon\n        self.n_stations = len(station_ids)  # Should be 1\n\n        # Get unique timestamps (now all from one station)\n        self.timestamps = sorted(df['timestamp'].unique())\n\n        # Filter valid timestamps (those that have enough history and future data)\n        valid_idx = []\n        for i in range(len(self.timestamps) - (seq_length + forecast_horizon - 1)):\n            # Check if we have continuous data for this window\n            current_time = self.timestamps[i]\n            end_time = self.timestamps[i + seq_length + forecast_horizon - 1]\n            expected_duration = timedelta(hours=seq_length + forecast_horizon - 1)\n\n            if (end_time - current_time) == expected_duration:\n                valid_idx.append(i)\n\n        self.valid_indices = valid_idx\n\n        # Add a safety check to ensure we have at least one valid window\n        if len(self.valid_indices) == 0:\n            print(f\"WARNING: No valid windows found in dataset. Using reduced requirements.\")\n            # Fall back to allowing any windows where we have both input and output data\n            valid_idx = []\n            for i in range(len(self.timestamps) - seq_length):\n                if i + seq_length < len(self.timestamps):\n                    valid_idx.append(i)\n            self.valid_indices = valid_idx\n            self.fallback_mode = True\n            print(f\"Found {len(self.valid_indices)} windows with relaxed continuity requirements\")\n        else:\n            self.fallback_mode = False\n            print(f\"Created dataset with {len(self.valid_indices)} valid windows\")\n\n    def __len__(self):\n        return max(1, len(self.valid_indices))  # Ensure length is at least 1\n\n    def __getitem__(self, idx):\n        if len(self.valid_indices) == 0:\n            # Return dummy data if no valid indices\n            X = np.zeros((len(self.feature_cols), self.n_stations, self.seq_length))\n            y = np.zeros((self.n_stations, self.forecast_horizon))\n            static_features = np.zeros((self.n_stations, 2))  # region_code and elevation\n            return (torch.FloatTensor(X), torch.FloatTensor(static_features)), torch.FloatTensor(y)\n\n        # Get actual data when possible\n        start_idx = self.valid_indices[idx % len(self.valid_indices)]\n\n        # Get timestamps for input and output windows\n        input_timestamps = self.timestamps[start_idx:start_idx + self.seq_length]\n        output_timestamps = self.timestamps[start_idx + self.seq_length:\n                                            start_idx + self.seq_length + self.forecast_horizon]\n\n        # Handle potential shortfall in output window (fallback mode)\n        if self.fallback_mode and len(output_timestamps) < self.forecast_horizon:\n            # Pad with repetition of last timestamp if needed\n            last_time = output_timestamps[-1] if len(output_timestamps) > 0 else input_timestamps[-1]\n            padding = [last_time] * (self.forecast_horizon - len(output_timestamps))\n            output_timestamps = list(output_timestamps) + padding\n\n        # Initialize tensors - simpler now with just one station\n        X = np.zeros((len(self.feature_cols), 1, self.seq_length))  # Only one station\n        y = np.zeros((1, self.forecast_horizon))  # Only one station\n        static_features = np.zeros((1, 2))  # region_code and elevation\n\n        # LA Downtown is our only station\n        station_id = 'LA Downtown'\n        \n        # Input sequence\n        for t_idx, ts in enumerate(input_timestamps):\n            station_data = self.df[self.df['timestamp'] == ts]\n\n            if not station_data.empty:\n                for f_idx, feat in enumerate(self.feature_cols):\n                    X[f_idx, 0, t_idx] = station_data[feat].values[0]\n\n                # Store static features (same for all timestamps)\n                if t_idx == 0:\n                    static_features[0, 0] = station_data['region_code'].values[0]\n                    static_features[0, 1] = station_data['elevation_norm'].values[0]\n\n        # Target sequence (temperature only)\n        for t_idx, ts in enumerate(output_timestamps):\n            if t_idx < self.forecast_horizon:  # Safety check\n                station_data = self.df[self.df['timestamp'] == ts]\n\n                if not station_data.empty:\n                    y[0, t_idx] = station_data['Temperature_C'].values[0]\n\n        return (torch.FloatTensor(X), torch.FloatTensor(static_features)), torch.FloatTensor(y)\n# ============================================================\n# TEMPORAL FUSION TRANSFORMER IMPLEMENTATION\n# ============================================================\nclass TemporalSelfAttention(nn.Module):\n    \"\"\"\n    Multi-head self-attention layer for temporal data.\n    Simplified from the original TFT paper.\n    \"\"\"\n    def __init__(self, d_model, n_heads=2, dropout=0.1):\n        super(TemporalSelfAttention, self).__init__()\n        self.d_model = d_model\n        self.n_heads = n_heads\n        self.head_dim = d_model // n_heads\n\n        self.query = nn.Linear(d_model, d_model)\n        self.key = nn.Linear(d_model, d_model)\n        self.value = nn.Linear(d_model, d_model)\n\n        self.out = nn.Linear(d_model, d_model)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        batch_size, seq_length, _ = x.size()\n\n        # Linear projections\n        queries = self.query(x).view(batch_size, seq_length, self.n_heads, self.head_dim).transpose(1, 2)\n        keys = self.key(x).view(batch_size, seq_length, self.n_heads, self.head_dim).transpose(1, 2)\n        values = self.value(x).view(batch_size, seq_length, self.n_heads, self.head_dim).transpose(1, 2)\n\n        # Attention scores\n        scores = torch.matmul(queries, keys.transpose(-2, -1)) / np.sqrt(self.head_dim)\n        attention = F.softmax(scores, dim=-1)\n        attention = self.dropout(attention)\n\n        # Apply attention to values\n        out = torch.matmul(attention, values)\n        out = out.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n\n        # Final linear layer\n        return self.out(out)\n\nclass GatedResidualNetwork(nn.Module):\n    \"\"\"\n    Gated Residual Network as described in the TFT paper.\n    Simplified version with fewer layers.\n    \"\"\"\n    def __init__(self, input_size, hidden_size, output_size, dropout=0.1):\n        super(GatedResidualNetwork, self).__init__()\n        self.input_size = input_size\n        self.output_size = output_size\n        self.hidden_size = hidden_size\n\n        # If input and output sizes are different, apply a skip connection\n        self.skip_layer = None\n        if input_size != output_size:\n            self.skip_layer = nn.Linear(input_size, output_size)\n\n        # Main layers\n        self.fc1 = nn.Linear(input_size, hidden_size)\n        self.fc2 = nn.Linear(hidden_size, output_size)\n        self.gate = nn.Linear(input_size + output_size, output_size)\n\n        self.dropout = nn.Dropout(dropout)\n        self.layer_norm = nn.LayerNorm(output_size)\n\n    def forward(self, x):\n        # Main branch\n        hidden = F.elu(self.fc1(x))\n        hidden = self.dropout(hidden)\n        hidden = self.fc2(hidden)\n\n        # Skip connection\n        if self.skip_layer is not None:\n            skip = self.skip_layer(x)\n        else:\n            skip = x\n\n        # Gate mechanism\n        gate_input = torch.cat([x, hidden], dim=-1)\n        gate = torch.sigmoid(self.gate(gate_input))\n\n        # Combine using gate\n        output = gate * hidden + (1 - gate) * skip\n\n        # Layer normalization\n        return self.layer_norm(output)\n\nclass VariableSelectionNetwork(nn.Module):\n    \"\"\"\n    Variable Selection Network for TFT.\n    Simplified version with fewer layers.\n    \"\"\"\n    def __init__(self, input_size_per_var, num_vars, hidden_size, output_size, dropout=0.1):\n        super(VariableSelectionNetwork, self).__init__()\n        self.input_size_per_var = input_size_per_var\n        self.num_vars = num_vars\n        self.hidden_size = hidden_size\n        self.output_size = output_size\n\n        # GRN for variable weights\n        self.weight_grn = GatedResidualNetwork(\n            input_size=input_size_per_var * num_vars,\n            hidden_size=hidden_size,\n            output_size=num_vars,\n            dropout=dropout\n        )\n\n        # GRN for each variable\n        self.var_grns = nn.ModuleList([\n            GatedResidualNetwork(\n                input_size=input_size_per_var,\n                hidden_size=hidden_size,\n                output_size=output_size,\n                dropout=dropout\n            ) for _ in range(num_vars)\n        ])\n\n    def forward(self, x):\n        # x shape: [batch_size, num_vars, input_size_per_var]\n        batch_size = x.size(0)\n        flat_x = x.view(batch_size, -1)\n\n        # Calculate variable weights\n        var_weights = self.weight_grn(flat_x)\n        var_weights = F.softmax(var_weights, dim=-1).unsqueeze(-1)  # [batch_size, num_vars, 1]\n\n        # Transform each variable\n        var_outputs = []\n        for i in range(self.num_vars):\n            var_outputs.append(self.var_grns[i](x[:, i]))\n\n        var_outputs = torch.stack(var_outputs, dim=1)  # [batch_size, num_vars, output_size]\n\n        # Weighted combination\n        outputs = torch.sum(var_outputs * var_weights, dim=1)  # [batch_size, output_size]\n\n        return outputs, var_weights\n\nclass TemporalFusionTransformer(nn.Module):\n    \"\"\"\n    Temporal Fusion Transformer with single predictions (no quantiles).\n    \"\"\"\n    def __init__(self, num_features, num_stations, hidden_size=64, num_heads=1, \n                 dropout=0.1, forecast_horizon=24, hidden_layers=2):\n        super(TemporalFusionTransformer, self).__init__()\n        self.num_features = num_features\n        self.num_stations = num_stations\n        self.hidden_size = hidden_size\n        self.forecast_horizon = forecast_horizon\n        self.hidden_layers = hidden_layers\n        \n        # Static variable processing\n        self.static_var_processor = GatedResidualNetwork(\n            input_size=2,  # region_code, elevation\n            hidden_size=hidden_size,\n            output_size=hidden_size,\n            dropout=dropout\n        )\n        \n        # Variable selection for time-varying features\n        self.temporal_var_selection = VariableSelectionNetwork(\n            input_size_per_var=24,  # Sequence length per feature\n            num_vars=num_features,\n            hidden_size=hidden_size,\n            output_size=hidden_size,\n            dropout=dropout\n        )\n        \n        # LSTM encoder layers\n        self.lstm_layers = nn.ModuleList([\n            nn.LSTM(\n                input_size=hidden_size if i == 0 else hidden_size,\n                hidden_size=hidden_size,\n                batch_first=True\n            ) for i in range(hidden_layers)\n        ])\n        \n        # Temporal self-attention\n        self.self_attention = TemporalSelfAttention(\n            d_model=hidden_size,\n            n_heads=num_heads,\n            dropout=dropout\n        )\n        \n        # Final output layer for forecasting (single prediction)\n        self.forecast_projection = nn.Sequential(\n            nn.Linear(hidden_size, hidden_size),\n            nn.ReLU(),\n            nn.Linear(hidden_size, forecast_horizon)\n        )\n        \n    def forward(self, inputs):\n        # Unpack inputs\n        temporal_features, static_features = inputs\n        batch_size = temporal_features.size(0)\n        \n        # [batch, features, stations, time] -> [batch*stations, features, time]\n        temporal_features = temporal_features.permute(0, 2, 1, 3)\n        temporal_features = temporal_features.reshape(batch_size * self.num_stations, self.num_features, -1)\n        \n        # Static features: [batch, stations, static_dims] -> [batch*stations, static_dims]\n        static_features = static_features.reshape(batch_size * self.num_stations, -1)\n        \n        # Process static features\n        static_embeddings = self.static_var_processor(static_features)\n        \n        # Process temporal features with variable selection\n        temporal_embeddings, temporal_weights = self.temporal_var_selection(temporal_features)\n        \n        # Reshape to [batch*stations, seq_len, hidden]\n        temporal_embeddings = temporal_embeddings.unsqueeze(1).expand(-1, 24, -1)\n        \n        # Add static embeddings to each timestep\n        temporal_embeddings = temporal_embeddings + static_embeddings.unsqueeze(1)\n        \n        # Pass through LSTM layers\n        lstm_out = temporal_embeddings\n        for lstm_layer in self.lstm_layers:\n            lstm_out, _ = lstm_layer(lstm_out)\n        \n        # Self-attention\n        attention_out = self.self_attention(lstm_out)\n        \n        # Generate forecast\n        forecast = self.forecast_projection(attention_out)\n        \n        # Take the last timesteps for the forecast horizon\n        forecast = forecast[:, -self.forecast_horizon:, 0]\n        \n        # Reshape back to [batch, stations, horizon]\n        forecast = forecast.reshape(batch_size, self.num_stations, -1)\n        \n        return forecast\n        \n# ============================================================\n# ADE OPTIMIZATION\n# ============================================================\nimport random\nfrom functools import partial\n\nclass ADE:\n    \"\"\"\n    Adaptive Differential Evolution for hyperparameter optimization.\n    \"\"\"\n    def __init__(self, obj_func, bounds, pop_size=10, max_iter=15, \n                 F=0.5, CR=0.7, F_range=(0.1, 1.0), CR_range=(0.2, 0.9)):\n        self.obj_func = obj_func\n        self.bounds = bounds\n        self.pop_size = pop_size\n        self.max_iter = max_iter\n        self.F = F\n        self.CR = CR\n        self.F_range = F_range\n        self.CR_range = CR_range\n        \n        # Initialize population\n        self.pop = self._init_population()\n        self.fitness = np.array([self.obj_func(ind) for ind in self.pop])\n        \n        # Track best solution\n        self.best_idx = np.argmin(self.fitness)\n        self.best = self.pop[self.best_idx]\n        self.best_fitness = self.fitness[self.best_idx]\n        \n    def _init_population(self):\n        \"\"\"\n        Initialize random population within bounds.\n        \"\"\"\n        pop = np.zeros((self.pop_size, len(self.bounds)))\n        for i in range(self.pop_size):\n            for j in range(len(self.bounds)):\n                pop[i, j] = random.uniform(self.bounds[j][0], self.bounds[j][1])\n        return pop\n    \n    def run(self):\n        \"\"\"\n        Run optimization process.\n        \"\"\"\n        for gen in range(self.max_iter):\n            for i in range(self.pop_size):\n                # Select three individuals\n                indices = list(range(self.pop_size))\n                indices.remove(i)\n                a, b, c = random.sample(indices, 3)\n                \n                # Adaptive F and CR\n                F = random.uniform(*self.F_range)\n                CR = random.uniform(*self.CR_range)\n                \n                # Mutation\n                mutant = self.pop[a] + F * (self.pop[b] - self.pop[c])\n                \n                # Constrain within bounds\n                for j in range(len(self.bounds)):\n                    mutant[j] = max(min(mutant[j], self.bounds[j][1]), self.bounds[j][0])\n                \n                # Crossover\n                trial = np.copy(self.pop[i])\n                for j in range(len(self.bounds)):\n                    if random.random() < CR:\n                        trial[j] = mutant[j]\n                \n                # Selection\n                trial_fitness = self.obj_func(trial)\n                if trial_fitness < self.fitness[i]:\n                    self.pop[i] = trial\n                    self.fitness[i] = trial_fitness\n                    \n                    # Update best\n                    if trial_fitness < self.best_fitness:\n                        self.best = trial\n                        self.best_fitness = trial_fitness\n            \n            print(f\"Generation {gen+1}/{self.max_iter}, Best fitness: {self.best_fitness:.4f}\")\n            \n        return self.best, self.best_fitness\n\ndef evaluate_tft_params(params, train_loader, val_loader, feature_cols, num_stations):\n    \"\"\"\n    Evaluate TFT model with given parameters.\n    \"\"\"\n    # Extract parameters\n    time_steps = int(params[0])\n    batch_size = int(params[1])\n    learning_rate = params[2]\n    hidden_size = int(params[3])\n    hidden_layers = int(params[4])\n    \n    # Create model\n    model = TemporalFusionTransformer(\n        num_features=len(feature_cols),\n        num_stations=num_stations,\n        hidden_size=hidden_size,\n        num_heads=1,  # Fixed as per requirements\n        dropout=0.1,\n        forecast_horizon=24\n    )\n    \n    # Train model with early stopping\n    model, train_losses, val_losses = train_model(\n        model=model,\n        train_loader=train_loader,\n        val_loader=val_loader,\n        learning_rate=learning_rate,\n        epochs=20,  # Max epochs as per requirements\n        patience=5   # Early stopping patience\n    )\n    \n    # Evaluate on validation set\n    model.eval()\n    val_loss = 0\n    val_batches = 0\n    \n    with torch.no_grad():\n        for inputs, targets in val_loader:\n            if isinstance(inputs, tuple):\n                inputs = tuple(x.to(device) for x in inputs)\n            elif isinstance(inputs, list):\n                inputs = [x.to(device) for x in inputs]\n            else:\n                inputs = inputs.to(device)\n\n            targets = targets.to(device)\n            \n            outputs = model(inputs)\n            \n            # Calculate MAPE\n            epsilon = 1e-10  # Small constant to avoid division by zero\n            mape = torch.mean(torch.abs((targets - outputs) / (targets + epsilon))) * 100\n            \n            val_loss += mape.item()\n            val_batches += 1\n    \n    avg_val_mape = val_loss / max(1, val_batches)\n    print(f\"Params: {params}, Validation MAPE: {avg_val_mape:.4f}%\")\n    \n    return avg_val_mape\n\ndef optimize_hyperparameters(train_loader, val_loader, feature_cols, num_stations):\n    \"\"\"\n    Use ADE to optimize hyperparameters.\n    \"\"\"\n    # Define parameter bounds - adjusted for T4 GPU efficiency\n    bounds = [\n        (12, 24),      # time_steps\n        (32, 64),      # batch_size - increased for better GPU utilization\n        (0.01, 0.05),  # learning_rate\n        (16, 32),      # hidden_size - increased for T4 capacity\n        (2, 3)         # hidden_layers\n    ]\n    \n    # Create objective function\n    obj_func = partial(evaluate_tft_params, train_loader=train_loader, \n                      val_loader=val_loader, feature_cols=feature_cols, \n                      num_stations=num_stations)\n    \n    # Run ADE optimization with smaller population for faster convergence\n    optimizer = ADE(obj_func=obj_func, bounds=bounds, pop_size=8, max_iter=10)\n    best_params, best_mape = optimizer.run()\n    \n    \n    # Convert parameters to appropriate types\n    time_steps = int(best_params[0])\n    batch_size = int(best_params[1])\n    learning_rate = best_params[2]\n    hidden_size = int(best_params[3])\n    hidden_layers = int(best_params[4])\n    \n    print(\"=\" * 50)\n    print(\"Optimal Parameters:\")\n    print(f\"Time Steps: {time_steps}\")\n    print(f\"Batch Size: {batch_size}\")\n    print(f\"Learning Rate: {learning_rate:.5f}\")\n    print(f\"Hidden Size: {hidden_size}\")\n    print(f\"Hidden Layers: {hidden_layers}\")\n    print(f\"Validation MAPE: {best_mape:.4f}%\")\n    print(\"=\" * 50)\n    \n    return {\n        'time_steps': time_steps,\n        'batch_size': batch_size,\n        'learning_rate': learning_rate,\n        'hidden_size': hidden_size,\n        'hidden_layers': hidden_layers\n    }\n\n# ============================================================\n# TRAINING AND EVALUATION FUNCTIONS\n# ============================================================\ndef mse_loss(preds, targets):\n    \"\"\"\n    Calculate mean squared error loss.\n    \"\"\"\n    return torch.mean((preds - targets) ** 2)\n\ndef train_model(model, train_loader, val_loader, learning_rate=0.001, epochs=20, patience=5):\n    \"\"\"\n    Train the model with MSE loss and early stopping.\n    \"\"\"\n    model.to(device)\n    \n    checkpoint_path = 'checkpoint.pth'\n    start_epoch = 0\n    best_val_loss = float('inf')\n    patience_counter = 0\n    \n    if os.path.exists(checkpoint_path):\n        checkpoint = torch.load(checkpoint_path)\n        model.load_state_dict(checkpoint['model_state_dict'])\n        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n        start_epoch = checkpoint['epoch'] + 1\n        patience_counter = checkpoint['patience_counter']\n        best_val_loss = checkpoint['best_val_loss']\n        print(f\"Resuming training from epoch {start_epoch}\")\n    else:\n        print(\"Starting training from scratch.\")\n\n    # REMOVED: quantiles = [0.1, 0.5, 0.9]  # P10, P50, P90\n    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer, mode='min', factor=0.5, patience=2, verbose=True\n    )\n    \n    # Add mixed precision training with autocast\n    scaler = torch.cuda.amp.GradScaler()\n\n    best_val_loss = float('inf')\n    patience_counter = 0\n    train_losses = []\n    val_losses = []\n    epoch_times = []\n\n    print(f\"Starting training for {epochs} epochs with patience {patience}...\")\n    total_start_time = time.time()\n\n    for epoch in range(start_epoch, epochs):\n        epoch_start_time = time.time()\n        # Training\n        model.train()\n        train_loss = 0\n        train_batches = 0\n\n        batch_times = []\n        for batch_idx, (inputs, targets) in enumerate(train_loader):\n            batch_start = time.time()\n            \n            # Move to device\n            if isinstance(inputs, tuple):\n                inputs = tuple(x.to(device) for x in inputs)\n            elif isinstance(inputs, list):\n                inputs = [x.to(device) if isinstance(x, torch.Tensor) else x for x in inputs]\n            else:\n                inputs = inputs.to(device)\n\n            targets = targets.to(device)\n\n            # Forward pass with mixed precision\n            optimizer.zero_grad()\n            \n            with torch.cuda.amp.autocast():\n                outputs = model(inputs)\n                # CHANGED: Use MSE loss instead of quantile loss\n                loss = mse_loss(outputs, targets)\n\n            # Backward pass with gradient scaling\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n\n            train_loss += loss.item()\n            train_batches += 1\n            \n            batch_end = time.time()\n            batch_time = batch_end - batch_start\n            batch_times.append(batch_time)\n            \n            # Print progress every 10 batches\n            if (batch_idx + 1) % 10 == 0:\n                print(f\"  Batch {batch_idx+1}/{len(train_loader)}, Loss: {loss.item():.4f}, Time: {batch_time:.3f}s\")\n\n        avg_train_loss = train_loss / max(1, train_batches)\n        train_losses.append(avg_train_loss)\n        \n        avg_batch_time = sum(batch_times) / len(batch_times) if batch_times else 0\n        \n        # Validation\n        val_start_time = time.time()\n        model.eval()\n        val_loss = 0\n        val_batches = 0\n\n        with torch.no_grad():\n            for inputs, targets in val_loader:\n                # Move to device\n                if isinstance(inputs, tuple):\n                    inputs = tuple(x.to(device) for x in inputs)\n                elif isinstance(inputs, list):\n                    inputs = [x.to(device) if isinstance(x, torch.Tensor) else x for x in inputs]\n                else:\n                    inputs = inputs.to(device)\n                    \n                targets = targets.to(device)\n\n                # Forward pass\n                outputs = model(inputs)\n                # CHANGED: Use MSE loss instead of quantile loss\n                loss = mse_loss(outputs, targets)\n\n                val_loss += loss.item()\n                val_batches += 1\n\n        avg_val_loss = val_loss / max(1, val_batches)\n        val_losses.append(avg_val_loss)\n        \n        val_time = time.time() - val_start_time\n        epoch_end_time = time.time()\n        epoch_time = epoch_end_time - epoch_start_time\n        epoch_times.append(epoch_time)\n        \n        # Calculate estimated time remaining\n        avg_epoch_time = sum(epoch_times) / len(epoch_times)\n        remaining_epochs = epochs - (epoch + 1)\n        est_time_remaining = avg_epoch_time * remaining_epochs\n        \n        # Print detailed progress\n        print(f\"Epoch {epoch+1}/{epochs}\")\n        print(f\"  Train Loss: {avg_train_loss:.4f} ({train_batches} batches, avg batch time: {avg_batch_time:.3f}s)\")\n        print(f\"  Val Loss: {avg_val_loss:.4f} (validation time: {val_time:.2f}s)\")\n        print(f\"  Epoch Time: {epoch_time:.2f}s, Est. Remaining: {est_time_remaining/60:.2f} minutes\")\n        \n        # Learning rate scheduler\n        scheduler.step(avg_val_loss)\n\n        # Early stopping\n        if avg_val_loss < best_val_loss:\n            best_val_loss = avg_val_loss\n            patience_counter = 0\n            # Save best model\n            try:\n                torch.save({\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'scheduler_state_dict': scheduler.state_dict(),\n                'epoch': epoch,\n                'patience_counter': patience_counter,\n                'best_val_loss': best_val_loss\n                }, 'checkpoint.pth')\n\n                print(f\"  Saved best model with val loss: {best_val_loss:.4f}\")\n            except Exception as e:\n                print(f\"  Error saving model: {e}\")\n        else:\n            patience_counter += 1\n            print(f\"  No improvement for {patience_counter}/{patience} epochs\")\n            if patience_counter >= patience:\n                print(f\"Early stopping at epoch {epoch+1}\")\n                break\n\n    total_time = time.time() - total_start_time\n    print(f\"Training completed in {total_time/60:.2f} minutes ({total_time:.1f} seconds)\")\n\n    # Load best model\n    try:\n        if os.path.exists('checkpoint.pth'):\n            checkpoint = torch.load('checkpoint.pth')\n            model.load_state_dict(checkpoint['model_state_dict'])\n\n    except Exception as e:\n        print(f\"Error loading best model: {e}\")\n\n    return model, train_losses, val_losses\n\ndef evaluate_model(model, data_loader, station_ids, regions):\n    \"\"\"\n    Evaluate the model and calculate metrics with robust dimension handling.\n    \"\"\"\n    model.eval()\n    all_predictions = []\n    all_actuals = []\n\n    with torch.no_grad():\n        for inputs, targets in data_loader:\n            # Move to device - handle ALL possible input types\n            if isinstance(inputs, tuple):\n                inputs = tuple(x.to(device) for x in inputs)\n            elif isinstance(inputs, list):\n                inputs = [x.to(device) if isinstance(x, torch.Tensor) else x for x in inputs]\n            else:\n                inputs = inputs.to(device)\n\n            # Forward pass with mixed precision for evaluation\n            with torch.cuda.amp.autocast():\n                outputs = model(inputs)\n\n            # CHANGED: Store predictions directly instead of extracting median predictions\n            preds = outputs.detach().cpu().numpy()\n            targets_cpu = targets.numpy()\n            \n            # Append batch predictions and targets\n            all_predictions.append(median_preds)\n            all_actuals.append(targets_cpu)\n            \n            # Clear GPU cache periodically\n            if torch.cuda.is_available():\n                torch.cuda.empty_cache()\n\n    # Process predictions in chunks to save memory\n    rmse_sum = 0\n    mae_sum = 0\n    r2_sum = 0\n    sample_count = 0\n    \n    # Process in smaller chunks\n    for preds, acts in zip(all_predictions, all_actuals):\n        # Flatten current batch\n        preds_flat = preds.flatten()  \n        acts_flat = acts.flatten()\n        \n        # Update metrics\n        rmse_sum += np.sum((preds_flat - acts_flat) ** 2)\n        mae_sum += np.sum(np.abs(preds_flat - acts_flat))\n        sample_count += len(preds_flat)\n    \n    # Calculate final metrics\n    rmse = np.sqrt(rmse_sum / sample_count)\n    mae = mae_sum / sample_count\n    \n    # For R², we need all data (this is an approximation)\n    all_preds_concat = np.concatenate([p.flatten() for p in all_predictions])\n    all_acts_concat = np.concatenate([a.flatten() for a in all_actuals])\n    r2 = r2_score(all_acts_concat, all_preds_concat)\n\n    print(f\"Overall Metrics - RMSE: {rmse:.4f}, MAE: {mae:.4f}, R²: {r2:.4f}\")\n\n    # Since we only have one station (LA Downtown), simplify the station metrics\n    station_metrics = {\n        station_ids[0]: {\n            'region': regions.get(station_ids[0], 'Unknown'),\n            'rmse': rmse,\n            'mae': mae,\n            'r2': r2\n        }\n    }\n    \n    print(f\"Station {station_ids[0]} ({regions.get(station_ids[0], 'Unknown')}) - \"\n          f\"RMSE: {rmse:.4f}, MAE: {mae:.4f}, R²: {r2:.4f}\")\n\n    return rmse, mae, r2, station_metrics\n\ndef visualize_predictions(model, data_loader, station_ids, regions, season):\n    \"\"\"\n    Visualize predictions for each station.\n    \"\"\"\n    model.eval()\n    if len(data_loader) == 0:\n        print(\"No data available for visualization\")\n        return\n    \n    # Get predictions\n    try:\n        for inputs, targets in data_loader:\n            # Only process one batch for visualization\n            if isinstance(inputs, tuple):\n                inputs = tuple(x.to(device) for x in inputs)\n            else:\n                inputs = inputs.to(device)\n\n            # Forward pass\n            outputs = model(inputs)\n\n            # Move to CPU for plotting\n            outputs = outputs.cpu().numpy()\n            targets = targets.numpy()\n            break\n\n        # Check if we have data to plot\n        if 'outputs' not in locals():\n            print(\"No data was loaded from the dataloader\")\n            return\n    except Exception as e:\n        print(f\"Error processing visualization data: {e}\")\n        return\n    \n    # Create subplots for each station\n    fig, axes = plt.subplots(len(station_ids), 1, figsize=(12, 3*len(station_ids)))\n    if len(station_ids) == 1:\n        axes = [axes]\n\n    hours = np.arange(24)\n\n    for i, station in enumerate(station_ids):\n        ax = axes[i]\n\n        # Plot actual vs predicted\n        ax.plot(hours, targets[0, i, :], 'b-', label='Actual')\n        ax.plot(hours, outputs[0, i, :], 'r--', label='Predicted')\n\n        ax.set_title(f\"{station} ({regions.get(station, 'Unknown')}) - {season}\")\n        ax.set_xlabel('Hour of Day')\n        ax.set_ylabel('Temperature (°C)')\n        ax.legend()\n        ax.grid(True, alpha=0.3)\n\n    plt.tight_layout()\n    plt.savefig(f\"{season}_predictions.png\")\n    plt.show()  # Add this line to display the plot\n    plt.close()\n\ndef create_mirror_plot(model, data_loader, station_ids, regions, season):\n    \"\"\"\n    Create a mirror plot with actual temperature on left side and predicted on right side.\n    \"\"\"\n    model.eval()\n    if len(data_loader) == 0:\n        print(f\"No data available for {season} mirror plot\")\n        return\n    \n    # Get predictions\n    try:\n        for inputs, targets in data_loader:\n            # Move inputs to device - handle ALL possible input types\n            if isinstance(inputs, tuple):\n                inputs = tuple(x.to(device) for x in inputs)\n            elif isinstance(inputs, list):\n                inputs = [x.to(device) if isinstance(x, torch.Tensor) else x for x in inputs]\n            else:\n                inputs = inputs.to(device)\n\n            # Forward pass\n            outputs = model(inputs)\n\n            # Move to CPU for plotting\n            outputs = outputs.cpu().numpy()\n            targets = targets.numpy()\n            break\n            \n        if 'outputs' not in locals():\n            print(f\"No data was loaded for {season} mirror plot\")\n            return\n    except Exception as e:\n        print(f\"Error processing mirror plot data for {season}: {e}\")\n        import traceback\n        traceback.print_exc()  # Print full error details\n        return\n    \n    # Create a figure for each station\n    for i, station in enumerate(station_ids):\n        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6), sharey=True)\n        \n        hours = np.arange(24)\n        \n        # Left plot - Actual temperatures\n        ax1.plot(hours, targets[0, i, :], 'b-o', linewidth=2)\n        ax1.set_title(f\"Actual Temperature - {season}\", fontsize=14)\n        ax1.set_xlabel('Hour of Day', fontsize=12)\n        ax1.set_ylabel('Temperature (°C)', fontsize=12)\n        ax1.set_xlim(0, 23)\n        ax1.grid(True, alpha=0.3)\n        ax1.invert_xaxis()  # Invert x-axis for mirror effect\n        \n        # Right plot - Predicted temperatures\n        ax2.plot(hours, outputs[0, i, :], 'r-o', linewidth=2)\n        ax2.set_title(f\"Predicted Temperature - {season}\", fontsize=14)\n        ax2.set_xlabel('Hour of Day', fontsize=12)\n        ax2.set_xlim(0, 23)\n        ax2.grid(True, alpha=0.3)\n        \n        # Add overall title\n        plt.suptitle(f\"{station} ({regions.get(station, 'Unknown')}) - {season} Day Comparison\", \n                    fontsize=16, y=1.05)\n        \n        # Add a line in the middle\n        fig.tight_layout()\n        plt.subplots_adjust(wspace=0.05)  # Reduce space between subplots\n        \n        # Save and display\n        plt.savefig(f\"{season}_mirror_comparison.png\", bbox_inches='tight')\n        plt.show()\n        plt.close()\n\ndef create_seasonal_datasets(df, target_days, all_stations, feature_cols):\n    \"\"\"\n    Create datasets for each seasonal day and visualize predictions.\n    \"\"\"\n    seasonal_datasets = {}\n    \n    for season, day_data in target_days.items():\n        if day_data.empty:\n            print(f\"No data available for {season}\")\n            continue\n            \n        # Get the date of this target day\n        sample_date = day_data['timestamp'].iloc[0]\n        print(f\"Creating dataset for {season}: {sample_date.strftime('%Y-%m-%d')}\")\n        \n        try:\n            # Prepare the data in the format expected by the model\n            # Format should be: temporal_features [batch, features, stations, time], static_features [batch, stations, static_features]\n            \n            # Reshape features to match expected format\n            temporal_data = np.zeros((1, len(feature_cols), 1, 24))  # [batch=1, features, stations=1, time=24]\n            \n            # Fill in the temporal features\n            for f_idx, feat in enumerate(feature_cols):\n                if feat in day_data.columns:\n                    temporal_data[0, f_idx, 0, :] = day_data[feat].values[:24]  # Using first 24 records\n            \n            # Create static features (region_code and elevation)\n            static_data = np.zeros((1, 1, 2))  # [batch=1, stations=1, static_features=2]\n            static_data[0, 0, 0] = day_data['region_code'].iloc[0] \n            static_data[0, 0, 1] = day_data['elevation_norm'].iloc[0]\n            \n            # Create target (actual temperatures)\n            target_data = np.zeros((1, 1, 24))  # [batch=1, stations=1, time=24]\n            target_data[0, 0, :] = day_data['Temperature_C'].values[:24]  # Using first 24 records\n            \n            # Convert to tensors\n            X_temporal = torch.FloatTensor(temporal_data)\n            X_static = torch.FloatTensor(static_data)\n            y = torch.FloatTensor(target_data)\n            \n            # Store as a list containing a single data point\n            seasonal_datasets[season] = [((X_temporal, X_static), y)]\n            print(f\"Created {season} dataset with shapes: X_temporal={X_temporal.shape}, X_static={X_static.shape}, y={y.shape}\")\n            \n        except Exception as e:\n            print(f\"Error creating {season} dataset: {e}\")\n            import traceback\n            traceback.print_exc()\n    \n    return seasonal_datasets\n\n\ndef analyze_topographic_performance(station_metrics, regions):\n    \"\"\"\n    Analyze model performance across different topographic regions.\n    \"\"\"\n    # Group metrics by region\n    region_metrics = {}\n    for station, metrics in station_metrics.items():\n        region = regions.get(station, 'Unknown')\n        if region not in region_metrics:\n            region_metrics[region] = []\n        region_metrics[region].append(metrics)\n\n    # Calculate average metrics by region\n    region_avg_metrics = {}\n    for region, metrics_list in region_metrics.items():\n        avg_rmse = np.mean([m['rmse'] for m in metrics_list])\n        avg_mae = np.mean([m['mae'] for m in metrics_list])\n        avg_r2 = np.mean([m['r2'] for m in metrics_list])\n\n        region_avg_metrics[region] = {\n            'avg_rmse': avg_rmse,\n            'avg_mae': avg_mae,\n            'avg_r2': avg_r2\n        }\n\n        print(f\"Region {region} - Avg RMSE: {avg_rmse:.4f}, Avg MAE: {avg_mae:.4f}, Avg R²: {avg_r2:.4f}\")\n\n        if not station_metrics:\n            print(\"No station metrics available for analysis\")\n            return {}\n\n    # Create bar chart comparing regions\n    regions = list(region_avg_metrics.keys())\n    rmse_values = [region_avg_metrics[r]['avg_rmse'] for r in regions]\n\n    plt.figure(figsize=(10, 6))\n    bars = plt.bar(regions, rmse_values)\n\n    # Add styling\n    plt.title('RMSE by Topographic Region', fontsize=16)\n\n    plt.title('RMSE by Topographic Region', fontsize=16)\n    plt.ylabel('RMSE (°C)', fontsize=14)\n    plt.xlabel('Region', fontsize=14)\n    plt.xticks(rotation=45)\n    plt.grid(True, alpha=0.3)\n\n    # Add value labels on bars\n    for bar in bars:\n        height = bar.get_height()\n        plt.text(bar.get_x() + bar.get_width()/2., height + 0.1,\n                 f'{height:.2f}',\n                 ha='center', va='bottom', fontsize=12)\n\n    plt.tight_layout()\n    plt.show()\n    plt.savefig('region_performance.png')\n    plt.close()\n\n    return region_avg_metrics\n\ndef analyze_seasonal_performance(seasonal_results):\n    \"\"\"\n    Compare model performance across different seasons.\n    \"\"\"\n    seasons = list(seasonal_results.keys())\n    rmse_values = [results['rmse'] for results in seasonal_results.values()]\n    mae_values = [results['mae'] for results in seasonal_results.values()]\n\n    if not seasonal_results:\n        print(\"No seasonal results available for analysis\")\n        return [], [], []\n\n    # Create grouped bar chart\n    x = np.arange(len(seasons))\n    width = 0.35\n\n    fig, ax = plt.subplots(figsize=(12, 6))\n    ax.bar(x - width/2, rmse_values, width, label='RMSE')\n    ax.bar(x + width/2, mae_values, width, label='MAE')\n\n    ax.set_title('Model Performance by Season', fontsize=16)\n    ax.set_ylabel('Error (°C)', fontsize=14)\n    ax.set_xticks(x)\n    ax.set_xticklabels(seasons)\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n\n    # Add value labels\n    for i, v in enumerate(rmse_values):\n        ax.text(i - width/2, v + 0.1, f'{v:.2f}', ha='center', fontsize=10)\n\n    for i, v in enumerate(mae_values):\n        ax.text(i + width/2, v + 0.1, f'{v:.2f}', ha='center', fontsize=10)\n\n    plt.tight_layout()\n    plt.show()\n    plt.savefig('seasonal_performance.png')\n    plt.close()\n\n    return seasons, rmse_values, mae_values\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T13:01:14.550370Z","iopub.execute_input":"2025-04-14T13:01:14.551014Z","iopub.status.idle":"2025-04-14T13:01:14.726196Z","shell.execute_reply.started":"2025-04-14T13:01:14.550974Z","shell.execute_reply":"2025-04-14T13:01:14.724767Z"}},"outputs":[{"name":"stdout","text":"Using device: cpu\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# ============================================================\n# BLOCK 1: INITIALIZATION AND DATA LOADING\n# ============================================================\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nimport torch\nfrom torch.utils.data import DataLoader, random_split\n\nprint(\"LA Downtown Weather Forecasting with VMD + TFT\")\nprint(\"=\" * 70)\n\n# Load and prepare data\ndf = load_data(DATA_PATH)\nprint(f\"Loaded data with {len(df)} records\")\n\n# All stations - only LA Downtown in this case\nall_stations = ['LA Downtown']\nregions = {'LA Downtown': 'urban'}\n\n# Define feature columns to use\nfeature_cols = [\n    'Temperature_C',\n    'HourlyRelativeHumidity',\n    'HourlyStationPressure',\n    'hour_sin', 'hour_cos',\n    'day_sin', 'day_cos'\n]\n\nprint(\"BLOCK 1 COMPLETED: Initialization and data loading successful.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T13:01:25.571917Z","iopub.execute_input":"2025-04-14T13:01:25.572414Z","iopub.status.idle":"2025-04-14T13:01:26.814637Z","shell.execute_reply.started":"2025-04-14T13:01:25.572375Z","shell.execute_reply":"2025-04-14T13:01:26.813492Z"}},"outputs":[{"name":"stdout","text":"LA Downtown Weather Forecasting with VMD + TFT\n======================================================================\nFiltered data to LA Downtown only\nLoaded data with 35040 records\nBLOCK 1 COMPLETED: Initialization and data loading successful.\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# ============================================================\n# BLOCK 2: VMD DECOMPOSITION\n# ============================================================\n\n# Apply VMD decomposition to the Temperature_C feature\nprint(\"Performing VMD decomposition...\")\ndecomposed_data = parallel_vmd_decomposition(df, feature_cols_to_decompose=['Temperature_C'])\n\n# Expand feature columns with decomposed modes\nexpanded_feature_cols = feature_cols.copy()\n\n# For LA Downtown, add decomposed temperature modes to feature columns\nla_downtown_modes = decomposed_data['LA Downtown']['Temperature_C']\nn_modes = la_downtown_modes.shape[0]  # Number of VMD modes\n\nprint(f\"Found {n_modes} VMD modes for Temperature_C\")\n\n# Create expanded dataset with VMD modes\nvmd_df = df.copy()\n\n# Add VMD modes as new features\nfor i in range(n_modes):\n    mode_name = f'Temp_Mode_{i+1}'\n    vmd_df[mode_name] = la_downtown_modes[i, :]\n    expanded_feature_cols.append(mode_name)\n\nprint(\"BLOCK 2 COMPLETED: VMD decomposition successful.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T13:01:39.196203Z","iopub.execute_input":"2025-04-14T13:01:39.196635Z","iopub.status.idle":"2025-04-14T13:01:39.219883Z","shell.execute_reply.started":"2025-04-14T13:01:39.196604Z","shell.execute_reply":"2025-04-14T13:01:39.218818Z"}},"outputs":[{"name":"stdout","text":"Performing VMD decomposition...\nPerforming VMD decomposition for feature: Temperature_C\n  Processing station: LA Downtown\nStarting decomposition for LA Downtown - Temperature_C\nLoading cached VMD decomposition for LA Downtown - Temperature_C\nFound 14 VMD modes for Temperature_C\nBLOCK 2 COMPLETED: VMD decomposition successful.\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# ============================================================\n# BLOCK 3: DATASET CREATION AND SPLITTING\n# ============================================================\n\n# Create dataset with expanded features\nfull_dataset = WeatherDataset(\n    df=vmd_df,\n    station_ids=all_stations,\n    feature_cols=expanded_feature_cols,\n    seq_length=24,\n    forecast_horizon=24\n)\n\n# Split into train/val/test\ndataset_size = len(full_dataset)\ntrain_size = int(dataset_size * 0.7)\nval_size = int(dataset_size * 0.15)\ntest_size = dataset_size - train_size - val_size\n\ntrain_dataset, val_dataset, test_dataset = random_split(\n    full_dataset, [train_size, val_size, test_size])\n\nprint(f\"Dataset split completed - Train: {train_size}, Validation: {val_size}, Test: {test_size}\")\n\n# First, create data loaders with default batch size\ninitial_batch_size = 32  # Default value before optimization\n\ntrain_loader = DataLoader(\n    train_dataset, \n    batch_size=initial_batch_size, \n    shuffle=True,\n    pin_memory=True,\n    num_workers=2,\n    persistent_workers=True\n)\n\nval_loader = DataLoader(\n    val_dataset, \n    batch_size=initial_batch_size, \n    shuffle=False,\n    pin_memory=True,\n    num_workers=2,\n    persistent_workers=True\n)\n\nprint(\"BLOCK 3 COMPLETED: Dataset creation and splitting successful.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T13:01:42.434981Z","iopub.execute_input":"2025-04-14T13:01:42.435341Z","iopub.status.idle":"2025-04-14T13:01:42.655329Z","shell.execute_reply.started":"2025-04-14T13:01:42.435283Z","shell.execute_reply":"2025-04-14T13:01:42.654332Z"}},"outputs":[{"name":"stdout","text":"Created dataset with 34946 valid windows\nDataset split completed - Train: 24462, Validation: 5241, Test: 5243\nBLOCK 3 COMPLETED: Dataset creation and splitting successful.\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# ============================================================\n# BLOCK 4: HYPERPARAMETER OPTIMIZATION\n# ============================================================\n\n# Run ADE optimization to find best hyperparameters\nprint(\"Running ADE optimization for hyperparameters...\")\nbest_params = optimize_hyperparameters(\n    train_loader=train_loader,\n    val_loader=val_loader,\n    feature_cols=expanded_feature_cols,\n    num_stations=len(all_stations)\n)\n\nprint(\"Optimized hyperparameters:\")\nfor param, value in best_params.items():\n    print(f\"- {param}: {value}\")\n    \nprint(\"BLOCK 4 COMPLETED: Hyperparameter optimization successful.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T13:01:45.883322Z","iopub.execute_input":"2025-04-14T13:01:45.883685Z","iopub.status.idle":"2025-04-14T16:05:32.670234Z","shell.execute_reply.started":"2025-04-14T13:01:45.883660Z","shell.execute_reply":"2025-04-14T16:05:32.667572Z"}},"outputs":[{"name":"stdout","text":"Running ADE optimization for hyperparameters...\nStarting training from scratch.\nStarting training for 20 epochs with patience 5...\n  Batch 10/765, Loss: 23.6946, Time: 0.083s\n  Batch 20/765, Loss: 58.9403, Time: 0.086s\n  Batch 30/765, Loss: 24.0082, Time: 0.085s\n  Batch 40/765, Loss: 20.9242, Time: 0.114s\n  Batch 50/765, Loss: 21.8134, Time: 0.084s\n  Batch 60/765, Loss: 18.8995, Time: 0.092s\n  Batch 70/765, Loss: 20.8797, Time: 0.084s\n  Batch 80/765, Loss: 27.2562, Time: 0.092s\n  Batch 90/765, Loss: 29.5925, Time: 0.087s\n  Batch 100/765, Loss: 23.6042, Time: 0.084s\n  Batch 110/765, Loss: 19.6169, Time: 0.086s\n  Batch 120/765, Loss: 16.4102, Time: 0.089s\n  Batch 130/765, Loss: 26.6869, Time: 0.085s\n  Batch 140/765, Loss: 26.7750, Time: 0.084s\n  Batch 150/765, Loss: 18.7962, Time: 0.083s\n  Batch 160/765, Loss: 15.7471, Time: 0.089s\n  Batch 170/765, Loss: 16.8617, Time: 0.116s\n  Batch 180/765, Loss: 14.1968, Time: 0.089s\n  Batch 190/765, Loss: 13.0765, Time: 0.060s\n  Batch 200/765, Loss: 12.7836, Time: 0.089s\n  Batch 210/765, Loss: 16.4167, Time: 0.186s\n  Batch 220/765, Loss: 13.1449, Time: 0.083s\n  Batch 230/765, Loss: 14.0427, Time: 0.088s\n  Batch 240/765, Loss: 12.2406, Time: 0.084s\n  Batch 250/765, Loss: 9.7375, Time: 0.091s\n  Batch 260/765, Loss: 12.6598, Time: 0.084s\n  Batch 270/765, Loss: 13.2748, Time: 0.087s\n  Batch 280/765, Loss: 6.1950, Time: 0.108s\n  Batch 290/765, Loss: 13.7786, Time: 0.083s\n  Batch 300/765, Loss: 13.4515, Time: 0.189s\n  Batch 310/765, Loss: 13.5711, Time: 0.115s\n  Batch 320/765, Loss: 8.1506, Time: 0.102s\n  Batch 330/765, Loss: 10.1114, Time: 0.088s\n  Batch 340/765, Loss: 10.0965, Time: 0.062s\n  Batch 350/765, Loss: 11.0180, Time: 0.100s\n  Batch 360/765, Loss: 10.9633, Time: 0.132s\n  Batch 370/765, Loss: 10.5851, Time: 0.085s\n  Batch 380/765, Loss: 12.1205, Time: 0.084s\n  Batch 390/765, Loss: 8.7031, Time: 0.083s\n  Batch 400/765, Loss: 11.0307, Time: 0.136s\n  Batch 410/765, Loss: 7.7189, Time: 0.114s\n  Batch 420/765, Loss: 11.5592, Time: 0.090s\n  Batch 430/765, Loss: 9.3267, Time: 0.252s\n  Batch 440/765, Loss: 7.2709, Time: 0.088s\n  Batch 450/765, Loss: 9.4852, Time: 0.057s\n  Batch 460/765, Loss: 10.7421, Time: 0.088s\n  Batch 470/765, Loss: 9.5181, Time: 0.082s\n  Batch 480/765, Loss: 11.3343, Time: 0.090s\n  Batch 490/765, Loss: 8.9197, Time: 0.090s\n  Batch 500/765, Loss: 8.2362, Time: 0.055s\n  Batch 510/765, Loss: 9.9964, Time: 0.058s\n  Batch 520/765, Loss: 11.7697, Time: 0.060s\n  Batch 530/765, Loss: 9.6589, Time: 0.085s\n  Batch 540/765, Loss: 12.8422, Time: 0.090s\n  Batch 550/765, Loss: 9.9967, Time: 0.088s\n  Batch 560/765, Loss: 10.0635, Time: 0.094s\n  Batch 570/765, Loss: 8.0340, Time: 0.086s\n  Batch 580/765, Loss: 10.7386, Time: 0.089s\n  Batch 590/765, Loss: 7.6056, Time: 0.089s\n  Batch 600/765, Loss: 12.6223, Time: 0.084s\n  Batch 610/765, Loss: 6.9446, Time: 0.090s\n  Batch 620/765, Loss: 11.4431, Time: 0.087s\n  Batch 630/765, Loss: 11.2036, Time: 0.088s\n  Batch 640/765, Loss: 9.8437, Time: 0.087s\n  Batch 650/765, Loss: 7.7773, Time: 0.065s\n  Batch 660/765, Loss: 8.5948, Time: 0.087s\n  Batch 670/765, Loss: 8.2269, Time: 0.085s\n  Batch 680/765, Loss: 9.2597, Time: 0.091s\n  Batch 690/765, Loss: 10.7522, Time: 0.083s\n  Batch 700/765, Loss: 7.7599, Time: 0.094s\n  Batch 710/765, Loss: 9.5110, Time: 0.091s\n  Batch 720/765, Loss: 9.3012, Time: 0.090s\n  Batch 730/765, Loss: 10.2730, Time: 0.090s\n  Batch 740/765, Loss: 10.4233, Time: 0.111s\n  Batch 750/765, Loss: 9.0978, Time: 0.092s\n  Batch 760/765, Loss: 12.8899, Time: 0.057s\nEpoch 1/20\n  Train Loss: 16.7541 (765 batches, avg batch time: 0.092s)\n  Val Loss: 9.1921 (validation time: 115.75s)\n  Epoch Time: 677.26s, Est. Remaining: 214.47 minutes\n  Saved best model with val loss: 9.1921\n  Batch 10/765, Loss: 12.9303, Time: 0.087s\n  Batch 20/765, Loss: 9.0485, Time: 0.086s\n  Batch 30/765, Loss: 9.5530, Time: 0.091s\n  Batch 40/765, Loss: 7.9583, Time: 0.087s\n  Batch 50/765, Loss: 9.5248, Time: 0.091s\n  Batch 60/765, Loss: 8.9611, Time: 0.085s\n  Batch 70/765, Loss: 10.0397, Time: 0.086s\n  Batch 80/765, Loss: 9.2348, Time: 0.091s\n  Batch 90/765, Loss: 10.4898, Time: 0.093s\n  Batch 100/765, Loss: 10.3524, Time: 0.090s\n  Batch 110/765, Loss: 9.1220, Time: 0.084s\n  Batch 120/765, Loss: 9.7245, Time: 0.090s\n  Batch 130/765, Loss: 11.2689, Time: 0.087s\n  Batch 140/765, Loss: 8.5388, Time: 0.090s\n  Batch 150/765, Loss: 11.2501, Time: 0.089s\n  Batch 160/765, Loss: 11.2480, Time: 0.090s\n  Batch 170/765, Loss: 9.4172, Time: 0.084s\n  Batch 180/765, Loss: 8.4205, Time: 0.092s\n  Batch 190/765, Loss: 7.8720, Time: 0.085s\n  Batch 200/765, Loss: 10.3319, Time: 0.085s\n  Batch 210/765, Loss: 6.8730, Time: 0.091s\n  Batch 220/765, Loss: 8.3190, Time: 0.062s\n  Batch 230/765, Loss: 8.7486, Time: 0.090s\n  Batch 240/765, Loss: 9.9805, Time: 0.084s\n  Batch 250/765, Loss: 9.7682, Time: 0.090s\n  Batch 260/765, Loss: 8.0930, Time: 0.094s\n  Batch 270/765, Loss: 9.1805, Time: 0.092s\n  Batch 280/765, Loss: 7.4004, Time: 0.089s\n  Batch 290/765, Loss: 11.5020, Time: 0.090s\n  Batch 300/765, Loss: 10.7262, Time: 0.085s\n  Batch 310/765, Loss: 9.0520, Time: 0.090s\n  Batch 320/765, Loss: 12.7699, Time: 0.084s\n  Batch 330/765, Loss: 11.6202, Time: 0.085s\n  Batch 340/765, Loss: 11.2515, Time: 0.086s\n  Batch 350/765, Loss: 9.6811, Time: 0.090s\n  Batch 360/765, Loss: 8.1426, Time: 0.090s\n  Batch 370/765, Loss: 8.0018, Time: 0.085s\n  Batch 380/765, Loss: 7.4899, Time: 0.091s\n  Batch 390/765, Loss: 9.1863, Time: 0.085s\n  Batch 400/765, Loss: 11.5547, Time: 0.085s\n  Batch 410/765, Loss: 11.3535, Time: 0.091s\n  Batch 420/765, Loss: 8.4239, Time: 0.091s\n  Batch 430/765, Loss: 6.7976, Time: 0.085s\n  Batch 440/765, Loss: 7.7664, Time: 0.140s\n  Batch 450/765, Loss: 12.3243, Time: 0.091s\n  Batch 460/765, Loss: 8.0439, Time: 0.106s\n  Batch 470/765, Loss: 7.5303, Time: 0.084s\n  Batch 480/765, Loss: 12.1936, Time: 0.090s\n  Batch 490/765, Loss: 9.5765, Time: 0.085s\n  Batch 500/765, Loss: 6.5432, Time: 0.090s\n  Batch 510/765, Loss: 9.3705, Time: 0.094s\n  Batch 520/765, Loss: 6.5318, Time: 0.084s\n  Batch 530/765, Loss: 11.2223, Time: 0.282s\n  Batch 540/765, Loss: 7.1103, Time: 0.089s\n  Batch 550/765, Loss: 10.9278, Time: 0.085s\n  Batch 560/765, Loss: 8.1352, Time: 0.091s\n  Batch 570/765, Loss: 11.6143, Time: 0.087s\n  Batch 580/765, Loss: 8.6548, Time: 0.101s\n  Batch 590/765, Loss: 8.2212, Time: 0.084s\n  Batch 600/765, Loss: 8.5371, Time: 0.085s\n  Batch 610/765, Loss: 8.3454, Time: 0.089s\n  Batch 620/765, Loss: 9.0293, Time: 0.097s\n  Batch 630/765, Loss: 8.8494, Time: 0.090s\n  Batch 640/765, Loss: 10.3544, Time: 0.090s\n  Batch 650/765, Loss: 11.5720, Time: 0.090s\n  Batch 660/765, Loss: 8.5469, Time: 0.084s\n  Batch 670/765, Loss: 10.7644, Time: 0.115s\n  Batch 680/765, Loss: 9.2949, Time: 0.086s\n  Batch 690/765, Loss: 9.4511, Time: 0.085s\n  Batch 700/765, Loss: 8.7580, Time: 0.092s\n  Batch 710/765, Loss: 9.1707, Time: 0.100s\n  Batch 720/765, Loss: 9.9271, Time: 0.087s\n  Batch 730/765, Loss: 9.7852, Time: 0.087s\n  Batch 740/765, Loss: 8.0180, Time: 0.088s\n  Batch 750/765, Loss: 7.1210, Time: 0.085s\n  Batch 760/765, Loss: 6.7123, Time: 0.083s\nEpoch 2/20\n  Train Loss: 9.3142 (765 batches, avg batch time: 0.093s)\n  Val Loss: 9.3285 (validation time: 114.96s)\n  Epoch Time: 670.14s, Est. Remaining: 202.11 minutes\n  No improvement for 1/5 epochs\n  Batch 10/765, Loss: 8.0650, Time: 0.118s\n  Batch 20/765, Loss: 7.3960, Time: 0.060s\n  Batch 30/765, Loss: 8.9260, Time: 0.089s\n  Batch 40/765, Loss: 9.8726, Time: 0.092s\n  Batch 50/765, Loss: 8.2656, Time: 0.179s\n  Batch 60/765, Loss: 7.7572, Time: 0.086s\n  Batch 70/765, Loss: 8.5667, Time: 0.086s\n  Batch 80/765, Loss: 11.0979, Time: 0.087s\n  Batch 90/765, Loss: 9.7101, Time: 0.058s\n  Batch 100/765, Loss: 9.7276, Time: 0.087s\n  Batch 110/765, Loss: 10.0927, Time: 0.085s\n  Batch 120/765, Loss: 10.0025, Time: 0.090s\n  Batch 130/765, Loss: 9.5292, Time: 0.088s\n  Batch 140/765, Loss: 10.9795, Time: 0.146s\n  Batch 150/765, Loss: 9.1355, Time: 0.087s\n  Batch 160/765, Loss: 8.9775, Time: 0.090s\n  Batch 170/765, Loss: 10.0056, Time: 0.091s\n  Batch 180/765, Loss: 8.6763, Time: 0.090s\n  Batch 190/765, Loss: 8.8397, Time: 0.101s\n  Batch 200/765, Loss: 8.4421, Time: 0.089s\n  Batch 210/765, Loss: 10.1510, Time: 0.088s\n  Batch 220/765, Loss: 8.8292, Time: 0.063s\n  Batch 230/765, Loss: 6.5937, Time: 0.093s\n  Batch 240/765, Loss: 7.9723, Time: 0.089s\n  Batch 250/765, Loss: 9.7949, Time: 0.091s\n  Batch 260/765, Loss: 9.8984, Time: 0.092s\n  Batch 270/765, Loss: 8.3757, Time: 0.203s\n  Batch 280/765, Loss: 9.5291, Time: 0.091s\n  Batch 290/765, Loss: 8.6830, Time: 0.089s\n  Batch 300/765, Loss: 8.0316, Time: 0.088s\n  Batch 310/765, Loss: 7.9948, Time: 0.089s\n  Batch 320/765, Loss: 12.3713, Time: 0.091s\n  Batch 330/765, Loss: 6.7204, Time: 0.091s\n  Batch 340/765, Loss: 8.9641, Time: 0.087s\n  Batch 350/765, Loss: 7.7148, Time: 0.086s\n  Batch 360/765, Loss: 9.0458, Time: 0.088s\n  Batch 370/765, Loss: 10.0466, Time: 0.088s\n  Batch 380/765, Loss: 12.0093, Time: 0.087s\n  Batch 390/765, Loss: 8.4516, Time: 0.088s\n  Batch 400/765, Loss: 9.4400, Time: 0.163s\n  Batch 410/765, Loss: 9.0920, Time: 0.089s\n  Batch 420/765, Loss: 8.8451, Time: 0.091s\n  Batch 430/765, Loss: 14.2155, Time: 0.094s\n  Batch 440/765, Loss: 10.4075, Time: 0.088s\n  Batch 450/765, Loss: 9.0501, Time: 0.089s\n  Batch 460/765, Loss: 9.7802, Time: 0.084s\n  Batch 470/765, Loss: 6.9853, Time: 0.061s\n  Batch 480/765, Loss: 9.1913, Time: 0.088s\n  Batch 490/765, Loss: 7.2616, Time: 0.089s\n  Batch 500/765, Loss: 8.6402, Time: 0.087s\n  Batch 510/765, Loss: 9.4913, Time: 0.090s\n  Batch 520/765, Loss: 6.9202, Time: 0.092s\n  Batch 530/765, Loss: 8.6794, Time: 0.091s\n  Batch 540/765, Loss: 5.9906, Time: 0.090s\n  Batch 550/765, Loss: 9.5151, Time: 0.088s\n  Batch 560/765, Loss: 8.0197, Time: 0.092s\n  Batch 570/765, Loss: 10.0244, Time: 0.090s\n  Batch 580/765, Loss: 8.5730, Time: 0.089s\n  Batch 590/765, Loss: 8.0369, Time: 0.097s\n  Batch 600/765, Loss: 10.3998, Time: 0.089s\n  Batch 610/765, Loss: 10.1347, Time: 0.088s\n  Batch 620/765, Loss: 7.0517, Time: 0.211s\n  Batch 630/765, Loss: 7.2303, Time: 0.091s\n  Batch 640/765, Loss: 8.7824, Time: 0.092s\n  Batch 650/765, Loss: 9.1531, Time: 0.109s\n  Batch 660/765, Loss: 9.6024, Time: 0.088s\n  Batch 670/765, Loss: 9.7480, Time: 0.103s\n  Batch 680/765, Loss: 7.5433, Time: 0.089s\n  Batch 690/765, Loss: 9.9421, Time: 0.062s\n  Batch 700/765, Loss: 7.5879, Time: 0.091s\n  Batch 710/765, Loss: 10.1507, Time: 0.090s\n  Batch 720/765, Loss: 9.1786, Time: 0.091s\n  Batch 730/765, Loss: 11.9350, Time: 0.090s\n  Batch 740/765, Loss: 8.4987, Time: 0.141s\n  Batch 750/765, Loss: 9.7762, Time: 0.127s\n  Batch 760/765, Loss: 9.7375, Time: 0.087s\nEpoch 3/20\n  Train Loss: 9.3142 (765 batches, avg batch time: 0.093s)\n  Val Loss: 8.8461 (validation time: 118.27s)\n  Epoch Time: 684.19s, Est. Remaining: 191.87 minutes\n  Saved best model with val loss: 8.8461\n  Batch 10/765, Loss: 11.1405, Time: 0.088s\n  Batch 20/765, Loss: 9.2743, Time: 0.089s\n  Batch 30/765, Loss: 9.2003, Time: 0.090s\n  Batch 40/765, Loss: 13.0929, Time: 0.061s\n  Batch 50/765, Loss: 9.1038, Time: 0.089s\n  Batch 60/765, Loss: 6.8677, Time: 0.088s\n  Batch 70/765, Loss: 10.0167, Time: 0.087s\n  Batch 80/765, Loss: 9.9372, Time: 0.113s\n  Batch 90/765, Loss: 9.0247, Time: 0.109s\n  Batch 100/765, Loss: 7.7811, Time: 0.090s\n  Batch 110/765, Loss: 11.0743, Time: 0.089s\n  Batch 120/765, Loss: 8.8828, Time: 0.088s\n  Batch 130/765, Loss: 8.8580, Time: 0.089s\n  Batch 140/765, Loss: 8.3471, Time: 0.089s\n  Batch 150/765, Loss: 11.7792, Time: 0.090s\n  Batch 160/765, Loss: 8.2685, Time: 0.089s\n  Batch 170/765, Loss: 12.4212, Time: 0.089s\n  Batch 180/765, Loss: 8.1314, Time: 0.087s\n  Batch 190/765, Loss: 10.9199, Time: 0.092s\n  Batch 200/765, Loss: 10.1521, Time: 0.087s\n  Batch 210/765, Loss: 6.4239, Time: 0.089s\n  Batch 220/765, Loss: 9.3086, Time: 0.090s\n  Batch 230/765, Loss: 9.0352, Time: 0.114s\n  Batch 240/765, Loss: 10.9701, Time: 0.086s\n  Batch 250/765, Loss: 9.3079, Time: 0.087s\n  Batch 260/765, Loss: 9.7318, Time: 0.089s\n  Batch 270/765, Loss: 10.3232, Time: 0.087s\n  Batch 280/765, Loss: 10.2587, Time: 0.088s\n  Batch 290/765, Loss: 10.9810, Time: 0.089s\n  Batch 300/765, Loss: 10.4036, Time: 0.088s\n  Batch 310/765, Loss: 8.1927, Time: 0.061s\n  Batch 320/765, Loss: 10.3577, Time: 0.088s\n  Batch 330/765, Loss: 8.5642, Time: 0.090s\n  Batch 340/765, Loss: 11.8752, Time: 0.088s\n  Batch 350/765, Loss: 8.3329, Time: 0.089s\n  Batch 360/765, Loss: 7.2833, Time: 0.089s\n  Batch 370/765, Loss: 10.3748, Time: 0.089s\n  Batch 380/765, Loss: 7.7170, Time: 0.090s\n  Batch 390/765, Loss: 9.6245, Time: 0.088s\n  Batch 400/765, Loss: 10.7516, Time: 0.088s\n  Batch 410/765, Loss: 10.9026, Time: 0.062s\n  Batch 420/765, Loss: 10.2265, Time: 0.090s\n  Batch 430/765, Loss: 8.9518, Time: 0.062s\n  Batch 440/765, Loss: 9.7404, Time: 0.069s\n  Batch 450/765, Loss: 9.1265, Time: 0.089s\n  Batch 460/765, Loss: 9.2751, Time: 0.088s\n  Batch 470/765, Loss: 8.5781, Time: 0.090s\n  Batch 480/765, Loss: 8.6585, Time: 0.087s\n  Batch 490/765, Loss: 10.0589, Time: 0.087s\n  Batch 500/765, Loss: 9.4258, Time: 0.088s\n  Batch 510/765, Loss: 8.6053, Time: 0.063s\n  Batch 520/765, Loss: 12.5765, Time: 0.088s\n  Batch 530/765, Loss: 8.2193, Time: 0.091s\n  Batch 540/765, Loss: 9.6746, Time: 0.089s\n  Batch 550/765, Loss: 10.7025, Time: 0.089s\n  Batch 560/765, Loss: 8.5658, Time: 0.088s\n  Batch 570/765, Loss: 10.0923, Time: 0.199s\n  Batch 580/765, Loss: 7.7530, Time: 0.090s\n  Batch 590/765, Loss: 7.2296, Time: 0.087s\n  Batch 600/765, Loss: 10.2252, Time: 0.088s\n  Batch 610/765, Loss: 11.4027, Time: 0.090s\n  Batch 620/765, Loss: 10.0542, Time: 0.089s\n  Batch 630/765, Loss: 10.6657, Time: 0.088s\n  Batch 640/765, Loss: 9.4441, Time: 0.087s\n  Batch 650/765, Loss: 10.5935, Time: 0.089s\n  Batch 660/765, Loss: 7.9635, Time: 0.091s\n  Batch 670/765, Loss: 5.9757, Time: 0.088s\n  Batch 680/765, Loss: 7.9698, Time: 0.098s\n  Batch 690/765, Loss: 8.3990, Time: 0.089s\n  Batch 700/765, Loss: 7.0473, Time: 0.151s\n  Batch 710/765, Loss: 8.5756, Time: 0.115s\n  Batch 720/765, Loss: 7.0806, Time: 0.117s\n  Batch 730/765, Loss: 11.0696, Time: 0.088s\n  Batch 740/765, Loss: 8.6598, Time: 0.063s\n  Batch 750/765, Loss: 8.5820, Time: 0.089s\n  Batch 760/765, Loss: 13.2017, Time: 0.087s\nEpoch 4/20\n  Train Loss: 9.1553 (765 batches, avg batch time: 0.095s)\n  Val Loss: 9.7882 (validation time: 118.74s)\n  Epoch Time: 685.13s, Est. Remaining: 181.11 minutes\n  No improvement for 1/5 epochs\n  Batch 10/765, Loss: 7.7807, Time: 0.090s\n  Batch 20/765, Loss: 8.5554, Time: 0.089s\n  Batch 30/765, Loss: 9.0061, Time: 0.089s\n  Batch 40/765, Loss: 7.6998, Time: 0.117s\n  Batch 50/765, Loss: 9.0328, Time: 0.089s\n  Batch 60/765, Loss: 10.2173, Time: 0.091s\n  Batch 70/765, Loss: 11.2937, Time: 0.088s\n  Batch 80/765, Loss: 9.7874, Time: 0.089s\n  Batch 90/765, Loss: 7.7270, Time: 0.090s\n  Batch 100/765, Loss: 9.5072, Time: 0.090s\n  Batch 110/765, Loss: 10.6034, Time: 0.090s\n  Batch 120/765, Loss: 8.5015, Time: 0.107s\n  Batch 130/765, Loss: 10.3966, Time: 0.088s\n  Batch 140/765, Loss: 10.1550, Time: 0.093s\n  Batch 150/765, Loss: 8.0461, Time: 0.088s\n  Batch 160/765, Loss: 8.2914, Time: 0.087s\n  Batch 170/765, Loss: 7.2456, Time: 0.317s\n  Batch 180/765, Loss: 12.0469, Time: 0.089s\n  Batch 190/765, Loss: 7.3247, Time: 0.091s\n  Batch 200/765, Loss: 8.5340, Time: 0.094s\n  Batch 210/765, Loss: 10.7888, Time: 0.088s\n  Batch 220/765, Loss: 8.5211, Time: 0.090s\n  Batch 230/765, Loss: 8.5609, Time: 0.062s\n  Batch 240/765, Loss: 11.6436, Time: 0.089s\n  Batch 250/765, Loss: 10.0571, Time: 0.089s\n  Batch 260/765, Loss: 6.5112, Time: 0.097s\n  Batch 270/765, Loss: 7.5986, Time: 0.063s\n  Batch 280/765, Loss: 8.9943, Time: 0.090s\n  Batch 290/765, Loss: 8.8341, Time: 0.088s\n  Batch 300/765, Loss: 8.4413, Time: 0.111s\n  Batch 310/765, Loss: 8.2238, Time: 0.090s\n  Batch 320/765, Loss: 10.5754, Time: 0.088s\n  Batch 330/765, Loss: 9.0101, Time: 0.088s\n  Batch 340/765, Loss: 8.0024, Time: 0.089s\n  Batch 350/765, Loss: 8.4464, Time: 0.090s\n  Batch 360/765, Loss: 8.5740, Time: 0.086s\n  Batch 370/765, Loss: 8.9505, Time: 0.087s\n  Batch 380/765, Loss: 10.8298, Time: 0.116s\n  Batch 390/765, Loss: 7.3139, Time: 0.062s\n  Batch 400/765, Loss: 7.3283, Time: 0.087s\n  Batch 410/765, Loss: 9.8768, Time: 0.087s\n  Batch 420/765, Loss: 8.6964, Time: 0.090s\n  Batch 430/765, Loss: 8.2969, Time: 0.060s\n  Batch 440/765, Loss: 11.9568, Time: 0.088s\n  Batch 450/765, Loss: 8.0276, Time: 0.089s\n  Batch 460/765, Loss: 7.2082, Time: 0.091s\n  Batch 470/765, Loss: 7.5528, Time: 0.090s\n  Batch 480/765, Loss: 8.6672, Time: 0.089s\n  Batch 490/765, Loss: 12.8014, Time: 0.088s\n  Batch 500/765, Loss: 8.7348, Time: 0.089s\n  Batch 510/765, Loss: 7.8287, Time: 0.088s\n  Batch 520/765, Loss: 8.8568, Time: 0.088s\n  Batch 530/765, Loss: 9.7982, Time: 0.087s\n  Batch 540/765, Loss: 7.2567, Time: 0.089s\n  Batch 550/765, Loss: 12.1860, Time: 0.090s\n  Batch 560/765, Loss: 9.5706, Time: 0.093s\n  Batch 570/765, Loss: 11.4489, Time: 0.089s\n  Batch 580/765, Loss: 11.1284, Time: 0.089s\n  Batch 590/765, Loss: 7.9591, Time: 0.091s\n  Batch 600/765, Loss: 9.9095, Time: 0.088s\n  Batch 610/765, Loss: 9.2123, Time: 0.087s\n  Batch 620/765, Loss: 6.7984, Time: 0.106s\n  Batch 630/765, Loss: 6.4840, Time: 0.088s\n  Batch 640/765, Loss: 9.3295, Time: 0.091s\n  Batch 650/765, Loss: 6.8284, Time: 0.103s\n  Batch 660/765, Loss: 8.9651, Time: 0.094s\n  Batch 670/765, Loss: 7.1893, Time: 0.089s\n  Batch 680/765, Loss: 8.0177, Time: 0.088s\n  Batch 690/765, Loss: 8.6388, Time: 0.088s\n  Batch 700/765, Loss: 8.3102, Time: 0.090s\n  Batch 710/765, Loss: 9.9043, Time: 0.091s\n  Batch 720/765, Loss: 6.9886, Time: 0.088s\n  Batch 730/765, Loss: 9.9342, Time: 0.087s\n  Batch 740/765, Loss: 8.8044, Time: 0.114s\n  Batch 750/765, Loss: 8.7143, Time: 0.091s\n  Batch 760/765, Loss: 11.0692, Time: 0.089s\nEpoch 5/20\n  Train Loss: 9.1791 (765 batches, avg batch time: 0.094s)\n  Val Loss: 8.8103 (validation time: 118.72s)\n  Epoch Time: 685.48s, Est. Remaining: 170.11 minutes\n  Saved best model with val loss: 8.8103\n  Batch 10/765, Loss: 10.4880, Time: 0.088s\n  Batch 20/765, Loss: 10.2980, Time: 0.089s\n  Batch 30/765, Loss: 7.9530, Time: 0.089s\n  Batch 40/765, Loss: 7.0561, Time: 0.087s\n  Batch 50/765, Loss: 10.3537, Time: 0.088s\n  Batch 60/765, Loss: 9.2514, Time: 0.089s\n  Batch 70/765, Loss: 7.9275, Time: 0.090s\n  Batch 80/765, Loss: 8.2344, Time: 0.089s\n  Batch 90/765, Loss: 10.4530, Time: 0.088s\n  Batch 100/765, Loss: 8.2047, Time: 0.091s\n  Batch 110/765, Loss: 16.1285, Time: 0.088s\n  Batch 120/765, Loss: 8.1164, Time: 0.235s\n  Batch 130/765, Loss: 9.1115, Time: 0.089s\n  Batch 140/765, Loss: 7.7629, Time: 0.090s\n  Batch 150/765, Loss: 10.3887, Time: 0.088s\n  Batch 160/765, Loss: 10.6026, Time: 0.089s\n  Batch 170/765, Loss: 9.7689, Time: 0.089s\n  Batch 180/765, Loss: 9.4496, Time: 0.061s\n  Batch 190/765, Loss: 7.9290, Time: 0.090s\n  Batch 200/765, Loss: 8.7037, Time: 0.091s\n  Batch 210/765, Loss: 9.6203, Time: 0.088s\n  Batch 220/765, Loss: 8.7935, Time: 0.089s\n  Batch 230/765, Loss: 7.8197, Time: 0.088s\n  Batch 240/765, Loss: 9.8326, Time: 0.088s\n  Batch 250/765, Loss: 11.1759, Time: 0.101s\n  Batch 260/765, Loss: 9.0669, Time: 0.090s\n  Batch 270/765, Loss: 10.0741, Time: 0.092s\n  Batch 280/765, Loss: 8.6239, Time: 0.094s\n  Batch 290/765, Loss: 8.1115, Time: 0.089s\n  Batch 300/765, Loss: 8.2201, Time: 0.061s\n  Batch 310/765, Loss: 9.6905, Time: 0.092s\n  Batch 320/765, Loss: 10.0203, Time: 0.091s\n  Batch 330/765, Loss: 10.3981, Time: 0.089s\n  Batch 340/765, Loss: 10.4468, Time: 0.063s\n  Batch 350/765, Loss: 10.5499, Time: 0.092s\n  Batch 360/765, Loss: 8.0273, Time: 0.090s\n  Batch 370/765, Loss: 6.4405, Time: 0.103s\n  Batch 380/765, Loss: 8.0176, Time: 0.114s\n  Batch 390/765, Loss: 10.2246, Time: 0.091s\n  Batch 400/765, Loss: 8.2713, Time: 0.089s\n  Batch 410/765, Loss: 7.9896, Time: 0.094s\n  Batch 420/765, Loss: 9.6238, Time: 0.063s\n  Batch 430/765, Loss: 9.3942, Time: 0.088s\n  Batch 440/765, Loss: 9.0986, Time: 0.088s\n  Batch 450/765, Loss: 8.4210, Time: 0.089s\n  Batch 460/765, Loss: 9.9384, Time: 0.114s\n  Batch 470/765, Loss: 10.7387, Time: 0.089s\n  Batch 480/765, Loss: 8.3178, Time: 0.088s\n  Batch 490/765, Loss: 9.4301, Time: 0.062s\n  Batch 500/765, Loss: 9.4025, Time: 0.095s\n  Batch 510/765, Loss: 6.7679, Time: 0.114s\n  Batch 520/765, Loss: 10.4361, Time: 0.094s\n  Batch 530/765, Loss: 9.4168, Time: 0.086s\n  Batch 540/765, Loss: 8.7126, Time: 0.095s\n  Batch 550/765, Loss: 8.8446, Time: 0.089s\n  Batch 560/765, Loss: 7.5534, Time: 0.089s\n  Batch 570/765, Loss: 7.9141, Time: 0.089s\n  Batch 580/765, Loss: 8.1491, Time: 0.089s\n  Batch 590/765, Loss: 6.8635, Time: 0.088s\n  Batch 600/765, Loss: 11.2227, Time: 0.092s\n  Batch 610/765, Loss: 8.9535, Time: 0.089s\n  Batch 620/765, Loss: 8.5730, Time: 0.089s\n  Batch 630/765, Loss: 9.4949, Time: 0.089s\n  Batch 640/765, Loss: 8.5913, Time: 0.113s\n  Batch 650/765, Loss: 9.6813, Time: 0.089s\n  Batch 660/765, Loss: 8.4864, Time: 0.089s\n  Batch 670/765, Loss: 10.2666, Time: 0.115s\n  Batch 680/765, Loss: 8.7366, Time: 0.088s\n  Batch 690/765, Loss: 9.5294, Time: 0.091s\n  Batch 700/765, Loss: 7.2824, Time: 0.091s\n  Batch 710/765, Loss: 9.3187, Time: 0.090s\n  Batch 720/765, Loss: 9.4466, Time: 0.092s\n  Batch 730/765, Loss: 11.4054, Time: 0.099s\n  Batch 740/765, Loss: 6.9251, Time: 0.089s\n  Batch 750/765, Loss: 13.3941, Time: 0.089s\n  Batch 760/765, Loss: 8.8887, Time: 0.087s\nEpoch 6/20\n  Train Loss: 9.0672 (765 batches, avg batch time: 0.094s)\n  Val Loss: 8.6973 (validation time: 118.47s)\n  Epoch Time: 688.34s, Est. Remaining: 159.08 minutes\n  Saved best model with val loss: 8.6973\n  Batch 10/765, Loss: 9.0637, Time: 0.089s\n  Batch 20/765, Loss: 9.2533, Time: 0.264s\n  Batch 30/765, Loss: 9.1751, Time: 0.094s\n  Batch 40/765, Loss: 6.1943, Time: 0.089s\n  Batch 50/765, Loss: 6.4843, Time: 0.088s\n  Batch 60/765, Loss: 9.8378, Time: 0.090s\n  Batch 70/765, Loss: 10.2830, Time: 0.090s\n  Batch 80/765, Loss: 9.9038, Time: 0.090s\n  Batch 90/765, Loss: 10.6136, Time: 0.087s\n  Batch 100/765, Loss: 9.8902, Time: 0.089s\n  Batch 110/765, Loss: 10.3470, Time: 0.089s\n  Batch 120/765, Loss: 7.6792, Time: 0.088s\n  Batch 130/765, Loss: 10.4605, Time: 0.090s\n  Batch 140/765, Loss: 8.7352, Time: 0.089s\n  Batch 150/765, Loss: 6.2558, Time: 0.187s\n  Batch 160/765, Loss: 9.3818, Time: 0.087s\n  Batch 170/765, Loss: 11.6020, Time: 0.088s\n  Batch 180/765, Loss: 8.6422, Time: 0.088s\n  Batch 190/765, Loss: 10.2196, Time: 0.090s\n  Batch 200/765, Loss: 11.7583, Time: 0.090s\n  Batch 210/765, Loss: 10.5501, Time: 0.088s\n  Batch 220/765, Loss: 25.9611, Time: 0.088s\n  Batch 230/765, Loss: 19.4682, Time: 0.091s\n  Batch 240/765, Loss: 16.4190, Time: 0.090s\n  Batch 250/765, Loss: 11.1097, Time: 0.091s\n  Batch 260/765, Loss: 10.9091, Time: 0.091s\n  Batch 270/765, Loss: 14.3847, Time: 0.092s\n  Batch 280/765, Loss: 13.2792, Time: 0.177s\n  Batch 290/765, Loss: 12.2104, Time: 0.062s\n  Batch 300/765, Loss: 8.8255, Time: 0.091s\n  Batch 310/765, Loss: 9.5022, Time: 0.091s\n  Batch 320/765, Loss: 13.1725, Time: 0.091s\n  Batch 330/765, Loss: 14.1491, Time: 0.094s\n  Batch 340/765, Loss: 11.0954, Time: 0.091s\n  Batch 350/765, Loss: 13.9609, Time: 0.092s\n  Batch 360/765, Loss: 16.3190, Time: 0.092s\n  Batch 370/765, Loss: 9.8104, Time: 0.065s\n  Batch 380/765, Loss: 12.3457, Time: 0.064s\n  Batch 390/765, Loss: 14.7479, Time: 0.090s\n  Batch 400/765, Loss: 9.7561, Time: 0.089s\n  Batch 410/765, Loss: 8.6765, Time: 0.083s\n  Batch 420/765, Loss: 9.1523, Time: 0.092s\n  Batch 430/765, Loss: 11.0430, Time: 0.091s\n  Batch 440/765, Loss: 11.4695, Time: 0.091s\n  Batch 450/765, Loss: 9.3214, Time: 0.092s\n  Batch 460/765, Loss: 14.6895, Time: 0.092s\n  Batch 470/765, Loss: 11.7519, Time: 0.091s\n  Batch 480/765, Loss: 10.9018, Time: 0.089s\n  Batch 490/765, Loss: 8.9721, Time: 0.063s\n  Batch 500/765, Loss: 8.3382, Time: 0.091s\n  Batch 510/765, Loss: 10.7339, Time: 0.093s\n  Batch 520/765, Loss: 8.9505, Time: 0.090s\n  Batch 530/765, Loss: 9.2368, Time: 0.090s\n  Batch 540/765, Loss: 10.9220, Time: 0.088s\n  Batch 550/765, Loss: 8.5587, Time: 0.095s\n  Batch 560/765, Loss: 9.1251, Time: 0.090s\n  Batch 570/765, Loss: 10.3165, Time: 0.096s\n  Batch 580/765, Loss: 10.8547, Time: 0.092s\n  Batch 590/765, Loss: 9.1138, Time: 0.093s\n  Batch 600/765, Loss: 7.9685, Time: 0.092s\n  Batch 610/765, Loss: 10.9633, Time: 0.120s\n  Batch 620/765, Loss: 8.0095, Time: 0.091s\n  Batch 630/765, Loss: 9.8026, Time: 0.091s\n  Batch 640/765, Loss: 11.3834, Time: 0.092s\n  Batch 650/765, Loss: 7.4723, Time: 0.095s\n  Batch 660/765, Loss: 14.4520, Time: 0.086s\n  Batch 670/765, Loss: 10.6051, Time: 0.086s\n  Batch 680/765, Loss: 12.2397, Time: 0.089s\n  Batch 690/765, Loss: 11.3764, Time: 0.092s\n  Batch 700/765, Loss: 10.1082, Time: 0.095s\n  Batch 710/765, Loss: 9.4727, Time: 0.086s\n  Batch 720/765, Loss: 7.2333, Time: 0.086s\n  Batch 730/765, Loss: 8.2456, Time: 0.090s\n  Batch 740/765, Loss: 8.1583, Time: 0.086s\n  Batch 750/765, Loss: 10.7244, Time: 0.092s\n  Batch 760/765, Loss: 10.7718, Time: 0.168s\nEpoch 7/20\n  Train Loss: 10.4216 (765 batches, avg batch time: 0.096s)\n  Val Loss: 9.4615 (validation time: 115.42s)\n  Epoch Time: 684.30s, Est. Remaining: 147.79 minutes\n  No improvement for 1/5 epochs\n  Batch 10/765, Loss: 9.0537, Time: 0.091s\n  Batch 20/765, Loss: 8.5198, Time: 0.087s\n  Batch 30/765, Loss: 9.2010, Time: 0.104s\n  Batch 40/765, Loss: 10.1314, Time: 0.092s\n  Batch 50/765, Loss: 11.0093, Time: 0.086s\n  Batch 60/765, Loss: 10.4277, Time: 0.092s\n  Batch 70/765, Loss: 11.3203, Time: 0.099s\n  Batch 80/765, Loss: 10.5239, Time: 0.068s\n  Batch 90/765, Loss: 11.7111, Time: 0.087s\n  Batch 100/765, Loss: 9.7311, Time: 0.087s\n  Batch 110/765, Loss: 11.6027, Time: 0.104s\n  Batch 120/765, Loss: 7.5052, Time: 0.087s\n  Batch 130/765, Loss: 10.2710, Time: 0.093s\n  Batch 140/765, Loss: 9.9532, Time: 0.087s\n  Batch 150/765, Loss: 10.0384, Time: 0.090s\n  Batch 160/765, Loss: 8.3921, Time: 0.098s\n  Batch 170/765, Loss: 8.7726, Time: 0.088s\n  Batch 180/765, Loss: 11.6156, Time: 0.095s\n  Batch 190/765, Loss: 10.3993, Time: 0.086s\n  Batch 200/765, Loss: 10.3637, Time: 0.093s\n  Batch 210/765, Loss: 8.5424, Time: 0.088s\n  Batch 220/765, Loss: 10.3361, Time: 0.062s\n  Batch 230/765, Loss: 9.3569, Time: 0.063s\n  Batch 240/765, Loss: 11.5939, Time: 0.086s\n  Batch 250/765, Loss: 11.2289, Time: 0.086s\n  Batch 260/765, Loss: 8.6566, Time: 0.090s\n  Batch 270/765, Loss: 9.7256, Time: 0.090s\n  Batch 280/765, Loss: 8.3318, Time: 0.154s\n  Batch 290/765, Loss: 9.0557, Time: 0.092s\n  Batch 300/765, Loss: 7.8076, Time: 0.091s\n  Batch 310/765, Loss: 12.3628, Time: 0.086s\n  Batch 320/765, Loss: 10.5174, Time: 0.091s\n  Batch 330/765, Loss: 11.0740, Time: 0.090s\n  Batch 340/765, Loss: 8.4230, Time: 0.087s\n  Batch 350/765, Loss: 9.3429, Time: 0.086s\n  Batch 360/765, Loss: 9.1840, Time: 0.089s\n  Batch 370/765, Loss: 11.6522, Time: 0.122s\n  Batch 380/765, Loss: 10.9973, Time: 0.087s\n  Batch 390/765, Loss: 9.3002, Time: 0.090s\n  Batch 400/765, Loss: 9.1833, Time: 0.059s\n  Batch 410/765, Loss: 9.9851, Time: 0.092s\n  Batch 420/765, Loss: 5.8807, Time: 0.087s\n  Batch 430/765, Loss: 9.8308, Time: 0.087s\n  Batch 440/765, Loss: 9.6793, Time: 0.094s\n  Batch 450/765, Loss: 8.2542, Time: 0.094s\n  Batch 460/765, Loss: 8.7045, Time: 0.088s\n  Batch 470/765, Loss: 10.6095, Time: 0.095s\n  Batch 480/765, Loss: 9.7557, Time: 0.087s\n  Batch 490/765, Loss: 10.2648, Time: 0.092s\n  Batch 500/765, Loss: 9.2705, Time: 0.090s\n  Batch 510/765, Loss: 9.2473, Time: 0.091s\n  Batch 520/765, Loss: 9.8238, Time: 0.087s\n  Batch 530/765, Loss: 10.2489, Time: 0.086s\n  Batch 540/765, Loss: 8.7070, Time: 0.087s\n  Batch 550/765, Loss: 9.3758, Time: 0.063s\n  Batch 560/765, Loss: 9.9633, Time: 0.089s\n  Batch 570/765, Loss: 8.0315, Time: 0.094s\n  Batch 580/765, Loss: 8.3483, Time: 0.089s\n  Batch 590/765, Loss: 9.6980, Time: 0.087s\n  Batch 600/765, Loss: 9.0421, Time: 0.093s\n  Batch 610/765, Loss: 9.3655, Time: 0.090s\n  Batch 620/765, Loss: 7.0314, Time: 0.094s\n  Batch 630/765, Loss: 8.7319, Time: 0.088s\n  Batch 640/765, Loss: 9.8968, Time: 0.066s\n  Batch 650/765, Loss: 8.4608, Time: 0.095s\n  Batch 660/765, Loss: 11.3731, Time: 0.105s\n  Batch 670/765, Loss: 9.9994, Time: 0.062s\n  Batch 680/765, Loss: 9.3806, Time: 0.093s\n  Batch 690/765, Loss: 7.0081, Time: 0.094s\n  Batch 700/765, Loss: 6.6101, Time: 0.088s\n  Batch 710/765, Loss: 10.8568, Time: 0.085s\n  Batch 720/765, Loss: 8.0500, Time: 0.086s\n  Batch 730/765, Loss: 10.2121, Time: 0.108s\n  Batch 740/765, Loss: 10.5412, Time: 0.087s\n  Batch 750/765, Loss: 9.3056, Time: 0.093s\n  Batch 760/765, Loss: 9.9854, Time: 0.070s\nEpoch 8/20\n  Train Loss: 9.4002 (765 batches, avg batch time: 0.095s)\n  Val Loss: 9.0053 (validation time: 116.82s)\n  Epoch Time: 672.89s, Est. Remaining: 136.19 minutes\n  No improvement for 2/5 epochs\n  Batch 10/765, Loss: 9.1615, Time: 0.091s\n  Batch 20/765, Loss: 8.8774, Time: 0.087s\n  Batch 30/765, Loss: 9.7605, Time: 0.087s\n  Batch 40/765, Loss: 10.2115, Time: 0.091s\n  Batch 50/765, Loss: 10.6921, Time: 0.092s\n  Batch 60/765, Loss: 8.4887, Time: 0.087s\n  Batch 70/765, Loss: 9.4663, Time: 0.086s\n  Batch 80/765, Loss: 8.1518, Time: 0.096s\n  Batch 90/765, Loss: 9.5954, Time: 0.088s\n  Batch 100/765, Loss: 8.5393, Time: 0.091s\n  Batch 110/765, Loss: 10.7461, Time: 0.091s\n  Batch 120/765, Loss: 8.5208, Time: 0.087s\n  Batch 130/765, Loss: 9.2504, Time: 0.093s\n  Batch 140/765, Loss: 5.7940, Time: 0.085s\n  Batch 150/765, Loss: 10.3715, Time: 0.092s\n  Batch 160/765, Loss: 11.5525, Time: 0.085s\n  Batch 170/765, Loss: 9.1697, Time: 0.090s\n  Batch 180/765, Loss: 12.0182, Time: 0.085s\n  Batch 190/765, Loss: 8.2015, Time: 0.060s\n  Batch 200/765, Loss: 9.2173, Time: 0.141s\n  Batch 210/765, Loss: 8.8839, Time: 0.094s\n  Batch 220/765, Loss: 10.9008, Time: 0.104s\n  Batch 230/765, Loss: 10.3709, Time: 0.088s\n  Batch 240/765, Loss: 12.0036, Time: 0.100s\n  Batch 250/765, Loss: 10.0185, Time: 0.091s\n  Batch 260/765, Loss: 9.2996, Time: 0.101s\n  Batch 270/765, Loss: 8.8397, Time: 0.096s\n  Batch 280/765, Loss: 9.4515, Time: 0.085s\n  Batch 290/765, Loss: 11.1512, Time: 0.087s\n  Batch 300/765, Loss: 11.0249, Time: 0.092s\n  Batch 310/765, Loss: 10.2144, Time: 0.148s\n  Batch 320/765, Loss: 8.7930, Time: 0.094s\n  Batch 330/765, Loss: 12.2972, Time: 0.092s\n  Batch 340/765, Loss: 8.0952, Time: 0.092s\n  Batch 350/765, Loss: 10.0493, Time: 0.090s\n  Batch 360/765, Loss: 11.3915, Time: 0.086s\n  Batch 370/765, Loss: 10.1797, Time: 0.110s\n  Batch 380/765, Loss: 11.0458, Time: 0.062s\n  Batch 390/765, Loss: 10.2220, Time: 0.085s\n  Batch 400/765, Loss: 9.6504, Time: 0.093s\n  Batch 410/765, Loss: 7.2391, Time: 0.086s\n  Batch 420/765, Loss: 10.0920, Time: 0.085s\n  Batch 430/765, Loss: 7.6817, Time: 0.101s\n  Batch 440/765, Loss: 11.3431, Time: 0.090s\n  Batch 450/765, Loss: 6.9213, Time: 0.087s\n  Batch 460/765, Loss: 10.0727, Time: 0.062s\n  Batch 470/765, Loss: 10.2438, Time: 0.092s\n  Batch 480/765, Loss: 6.6618, Time: 0.094s\n  Batch 490/765, Loss: 10.4440, Time: 0.089s\n  Batch 500/765, Loss: 9.6743, Time: 0.090s\n  Batch 510/765, Loss: 10.3479, Time: 0.090s\n  Batch 520/765, Loss: 12.0529, Time: 0.088s\n  Batch 530/765, Loss: 8.0663, Time: 0.090s\n  Batch 540/765, Loss: 9.9789, Time: 0.091s\n  Batch 550/765, Loss: 7.0496, Time: 0.088s\n  Batch 560/765, Loss: 8.0337, Time: 0.090s\n  Batch 570/765, Loss: 8.3358, Time: 0.089s\n  Batch 580/765, Loss: 10.8298, Time: 0.091s\n  Batch 590/765, Loss: 13.8629, Time: 0.090s\n  Batch 600/765, Loss: 9.5150, Time: 0.088s\n  Batch 610/765, Loss: 9.2060, Time: 0.089s\n  Batch 620/765, Loss: 13.3388, Time: 0.090s\n  Batch 630/765, Loss: 7.3053, Time: 0.091s\n  Batch 640/765, Loss: 8.8583, Time: 0.074s\n  Batch 650/765, Loss: 6.9956, Time: 0.088s\n  Batch 660/765, Loss: 9.1642, Time: 0.091s\n  Batch 670/765, Loss: 9.2807, Time: 0.089s\n  Batch 680/765, Loss: 9.0344, Time: 0.088s\n  Batch 690/765, Loss: 10.0432, Time: 0.098s\n  Batch 700/765, Loss: 9.0923, Time: 0.089s\n  Batch 710/765, Loss: 8.6233, Time: 0.089s\n  Batch 720/765, Loss: 12.0704, Time: 0.115s\n  Batch 730/765, Loss: 14.8352, Time: 0.092s\n  Batch 740/765, Loss: 8.8530, Time: 0.099s\n  Batch 750/765, Loss: 8.9124, Time: 0.090s\n  Batch 760/765, Loss: 10.8908, Time: 0.091s\nEpoch 9/20\n  Train Loss: 9.3662 (765 batches, avg batch time: 0.094s)\n  Val Loss: 9.1658 (validation time: 118.31s)\n  Epoch Time: 680.95s, Est. Remaining: 124.84 minutes\n  No improvement for 3/5 epochs\n  Batch 10/765, Loss: 9.3280, Time: 0.087s\n  Batch 20/765, Loss: 11.0548, Time: 0.089s\n  Batch 30/765, Loss: 9.9376, Time: 0.090s\n  Batch 40/765, Loss: 10.4238, Time: 0.087s\n  Batch 50/765, Loss: 9.0032, Time: 0.088s\n  Batch 60/765, Loss: 8.0253, Time: 0.100s\n  Batch 70/765, Loss: 7.7382, Time: 0.090s\n  Batch 80/765, Loss: 9.5916, Time: 0.092s\n  Batch 90/765, Loss: 9.2792, Time: 0.091s\n  Batch 100/765, Loss: 11.2920, Time: 0.090s\n  Batch 110/765, Loss: 10.2196, Time: 0.180s\n  Batch 120/765, Loss: 9.5162, Time: 0.090s\n  Batch 130/765, Loss: 7.8328, Time: 0.089s\n  Batch 140/765, Loss: 9.1716, Time: 0.117s\n  Batch 150/765, Loss: 7.5311, Time: 0.091s\n  Batch 160/765, Loss: 10.4671, Time: 0.063s\n  Batch 170/765, Loss: 6.9272, Time: 0.091s\n  Batch 180/765, Loss: 10.0232, Time: 0.090s\n  Batch 190/765, Loss: 7.3793, Time: 0.091s\n  Batch 200/765, Loss: 8.8488, Time: 0.090s\n  Batch 210/765, Loss: 9.1151, Time: 0.092s\n  Batch 220/765, Loss: 9.3057, Time: 0.096s\n  Batch 230/765, Loss: 8.9761, Time: 0.089s\n  Batch 240/765, Loss: 8.3173, Time: 0.205s\n  Batch 250/765, Loss: 8.5365, Time: 0.115s\n  Batch 260/765, Loss: 10.7553, Time: 0.088s\n  Batch 270/765, Loss: 6.3320, Time: 0.094s\n  Batch 280/765, Loss: 9.2495, Time: 0.088s\n  Batch 290/765, Loss: 9.6183, Time: 0.089s\n  Batch 300/765, Loss: 11.3653, Time: 0.088s\n  Batch 310/765, Loss: 8.1407, Time: 0.090s\n  Batch 320/765, Loss: 6.9465, Time: 0.094s\n  Batch 330/765, Loss: 11.9467, Time: 0.089s\n  Batch 340/765, Loss: 8.7905, Time: 0.089s\n  Batch 350/765, Loss: 12.5669, Time: 0.094s\n  Batch 360/765, Loss: 7.6240, Time: 0.089s\n  Batch 370/765, Loss: 4.5880, Time: 0.138s\n  Batch 380/765, Loss: 8.9112, Time: 0.089s\n  Batch 390/765, Loss: 7.8297, Time: 0.089s\n  Batch 400/765, Loss: 8.7306, Time: 0.088s\n  Batch 410/765, Loss: 9.7128, Time: 0.090s\n  Batch 420/765, Loss: 11.0829, Time: 0.088s\n  Batch 430/765, Loss: 7.6440, Time: 0.089s\n  Batch 440/765, Loss: 11.5862, Time: 0.095s\n  Batch 450/765, Loss: 8.6176, Time: 0.089s\n  Batch 460/765, Loss: 8.8665, Time: 0.089s\n  Batch 470/765, Loss: 8.4466, Time: 0.094s\n  Batch 480/765, Loss: 9.1371, Time: 0.090s\n  Batch 490/765, Loss: 10.5508, Time: 0.087s\n  Batch 500/765, Loss: 8.9950, Time: 0.089s\n  Batch 510/765, Loss: 7.5307, Time: 0.089s\n  Batch 520/765, Loss: 9.1470, Time: 0.088s\n  Batch 530/765, Loss: 8.3628, Time: 0.089s\n  Batch 540/765, Loss: 6.8629, Time: 0.087s\n  Batch 550/765, Loss: 5.7262, Time: 0.090s\n  Batch 560/765, Loss: 8.9262, Time: 0.089s\n  Batch 570/765, Loss: 11.5243, Time: 0.089s\n  Batch 580/765, Loss: 7.0297, Time: 0.087s\n  Batch 590/765, Loss: 7.4544, Time: 0.133s\n  Batch 600/765, Loss: 7.9733, Time: 0.092s\n  Batch 610/765, Loss: 11.3031, Time: 0.088s\n  Batch 620/765, Loss: 9.9730, Time: 0.087s\n  Batch 630/765, Loss: 9.9324, Time: 0.139s\n  Batch 640/765, Loss: 9.0095, Time: 0.088s\n  Batch 650/765, Loss: 11.6787, Time: 0.088s\n  Batch 660/765, Loss: 10.2329, Time: 0.088s\n  Batch 670/765, Loss: 9.4496, Time: 0.099s\n  Batch 680/765, Loss: 9.5743, Time: 0.089s\n  Batch 690/765, Loss: 8.5418, Time: 0.090s\n  Batch 700/765, Loss: 8.2659, Time: 0.088s\n  Batch 710/765, Loss: 10.4897, Time: 0.091s\n  Batch 720/765, Loss: 10.2849, Time: 0.185s\n  Batch 730/765, Loss: 9.4280, Time: 0.091s\n  Batch 740/765, Loss: 8.3586, Time: 0.059s\n  Batch 750/765, Loss: 7.5462, Time: 0.090s\n  Batch 760/765, Loss: 7.8824, Time: 0.087s\nEpoch 10/20\n  Train Loss: 9.1687 (765 batches, avg batch time: 0.094s)\n  Val Loss: 8.7361 (validation time: 119.78s)\n  Epoch Time: 686.49s, Est. Remaining: 113.59 minutes\n  No improvement for 4/5 epochs\n  Batch 10/765, Loss: 10.9394, Time: 0.089s\n  Batch 20/765, Loss: 8.6953, Time: 0.087s\n  Batch 30/765, Loss: 11.0859, Time: 0.088s\n  Batch 40/765, Loss: 9.7322, Time: 0.062s\n  Batch 50/765, Loss: 9.6186, Time: 0.088s\n  Batch 60/765, Loss: 9.3891, Time: 0.088s\n  Batch 70/765, Loss: 9.9039, Time: 0.087s\n  Batch 80/765, Loss: 9.8809, Time: 0.093s\n  Batch 90/765, Loss: 11.8318, Time: 0.088s\n  Batch 100/765, Loss: 12.4095, Time: 0.088s\n  Batch 110/765, Loss: 9.8519, Time: 0.115s\n  Batch 120/765, Loss: 8.2063, Time: 0.087s\n  Batch 130/765, Loss: 9.8410, Time: 0.094s\n  Batch 140/765, Loss: 8.8336, Time: 0.093s\n  Batch 150/765, Loss: 11.0745, Time: 0.088s\n  Batch 160/765, Loss: 7.9873, Time: 0.088s\n  Batch 170/765, Loss: 9.5495, Time: 0.089s\n  Batch 180/765, Loss: 11.0877, Time: 0.085s\n  Batch 190/765, Loss: 10.0523, Time: 0.088s\n  Batch 200/765, Loss: 8.6515, Time: 0.090s\n  Batch 210/765, Loss: 7.2385, Time: 0.087s\n  Batch 220/765, Loss: 9.8475, Time: 0.088s\n  Batch 230/765, Loss: 9.8316, Time: 0.119s\n  Batch 240/765, Loss: 8.5200, Time: 0.089s\n  Batch 250/765, Loss: 11.4480, Time: 0.090s\n  Batch 260/765, Loss: 6.5098, Time: 0.087s\n  Batch 270/765, Loss: 8.7334, Time: 0.088s\n  Batch 280/765, Loss: 10.4881, Time: 0.089s\n  Batch 290/765, Loss: 8.4047, Time: 0.063s\n  Batch 300/765, Loss: 8.6117, Time: 0.087s\n  Batch 310/765, Loss: 7.9505, Time: 0.088s\n  Batch 320/765, Loss: 8.0613, Time: 0.111s\n  Batch 330/765, Loss: 7.7994, Time: 0.061s\n  Batch 340/765, Loss: 7.2445, Time: 0.092s\n  Batch 350/765, Loss: 9.5152, Time: 0.090s\n  Batch 360/765, Loss: 9.5279, Time: 0.088s\n  Batch 370/765, Loss: 9.4347, Time: 0.085s\n  Batch 380/765, Loss: 7.1563, Time: 0.088s\n  Batch 390/765, Loss: 8.9294, Time: 0.088s\n  Batch 400/765, Loss: 10.6033, Time: 0.088s\n  Batch 410/765, Loss: 7.8723, Time: 0.091s\n  Batch 420/765, Loss: 8.3703, Time: 0.087s\n  Batch 430/765, Loss: 9.5947, Time: 0.088s\n  Batch 440/765, Loss: 9.2227, Time: 0.063s\n  Batch 450/765, Loss: 9.3393, Time: 0.108s\n  Batch 460/765, Loss: 10.1921, Time: 0.088s\n  Batch 470/765, Loss: 8.4985, Time: 0.089s\n  Batch 480/765, Loss: 8.9373, Time: 0.089s\n  Batch 490/765, Loss: 7.5571, Time: 0.096s\n  Batch 500/765, Loss: 6.0532, Time: 0.089s\n  Batch 510/765, Loss: 7.5709, Time: 0.092s\n  Batch 520/765, Loss: 8.3506, Time: 0.063s\n  Batch 530/765, Loss: 10.3853, Time: 0.089s\n  Batch 540/765, Loss: 9.1771, Time: 0.088s\n  Batch 550/765, Loss: 7.1443, Time: 0.090s\n  Batch 560/765, Loss: 8.9190, Time: 0.092s\n  Batch 570/765, Loss: 10.3232, Time: 0.090s\n  Batch 580/765, Loss: 9.9510, Time: 0.090s\n  Batch 590/765, Loss: 6.1894, Time: 0.087s\n  Batch 600/765, Loss: 7.8052, Time: 0.090s\n  Batch 610/765, Loss: 6.2463, Time: 0.066s\n  Batch 620/765, Loss: 10.7893, Time: 0.088s\n  Batch 630/765, Loss: 7.7667, Time: 0.088s\n  Batch 640/765, Loss: 8.0763, Time: 0.088s\n  Batch 650/765, Loss: 7.9834, Time: 0.137s\n  Batch 660/765, Loss: 13.5698, Time: 0.093s\n  Batch 670/765, Loss: 9.4457, Time: 0.176s\n  Batch 680/765, Loss: 9.1150, Time: 0.088s\n  Batch 690/765, Loss: 8.6999, Time: 0.061s\n  Batch 700/765, Loss: 10.6774, Time: 0.087s\n  Batch 710/765, Loss: 7.1994, Time: 0.089s\n  Batch 720/765, Loss: 10.0577, Time: 0.093s\n  Batch 730/765, Loss: 7.1807, Time: 0.088s\n  Batch 740/765, Loss: 9.3526, Time: 0.087s\n  Batch 750/765, Loss: 8.8455, Time: 0.087s\n  Batch 760/765, Loss: 8.2154, Time: 0.089s\nEpoch 11/20\n  Train Loss: 9.0316 (765 batches, avg batch time: 0.094s)\n  Val Loss: 8.6696 (validation time: 117.93s)\n  Epoch Time: 683.20s, Est. Remaining: 102.25 minutes\n  Saved best model with val loss: 8.6696\n  Batch 10/765, Loss: 9.8721, Time: 0.089s\n  Batch 20/765, Loss: 9.5506, Time: 0.089s\n  Batch 30/765, Loss: 8.5112, Time: 0.088s\n  Batch 40/765, Loss: 9.6375, Time: 0.093s\n  Batch 50/765, Loss: 6.1997, Time: 0.172s\n  Batch 60/765, Loss: 7.4688, Time: 0.087s\n  Batch 70/765, Loss: 9.3769, Time: 0.089s\n  Batch 80/765, Loss: 7.5684, Time: 0.090s\n  Batch 90/765, Loss: 8.5111, Time: 0.089s\n  Batch 100/765, Loss: 9.1755, Time: 0.089s\n  Batch 110/765, Loss: 8.5900, Time: 0.088s\n  Batch 120/765, Loss: 11.0161, Time: 0.089s\n  Batch 130/765, Loss: 8.3720, Time: 0.089s\n  Batch 140/765, Loss: 7.3375, Time: 0.090s\n  Batch 150/765, Loss: 6.3277, Time: 0.089s\n  Batch 160/765, Loss: 9.5516, Time: 0.114s\n  Batch 170/765, Loss: 9.3210, Time: 0.088s\n  Batch 180/765, Loss: 8.0755, Time: 0.064s\n  Batch 190/765, Loss: 8.9185, Time: 0.089s\n  Batch 200/765, Loss: 9.2168, Time: 0.091s\n  Batch 210/765, Loss: 7.5476, Time: 0.089s\n  Batch 220/765, Loss: 9.3104, Time: 0.064s\n  Batch 230/765, Loss: 9.6915, Time: 0.089s\n  Batch 240/765, Loss: 8.1374, Time: 0.090s\n  Batch 250/765, Loss: 6.8453, Time: 0.089s\n  Batch 260/765, Loss: 10.1253, Time: 0.091s\n  Batch 270/765, Loss: 10.9819, Time: 0.111s\n  Batch 280/765, Loss: 9.7745, Time: 0.091s\n  Batch 290/765, Loss: 8.1930, Time: 0.091s\n  Batch 300/765, Loss: 7.9601, Time: 0.089s\n  Batch 310/765, Loss: 12.2018, Time: 0.089s\n  Batch 320/765, Loss: 9.0217, Time: 0.091s\n  Batch 330/765, Loss: 11.7497, Time: 0.090s\n  Batch 340/765, Loss: 10.1913, Time: 0.088s\n  Batch 350/765, Loss: 10.3973, Time: 0.090s\n  Batch 360/765, Loss: 7.2824, Time: 0.105s\n  Batch 370/765, Loss: 10.0998, Time: 0.061s\n  Batch 380/765, Loss: 7.2214, Time: 0.093s\n  Batch 390/765, Loss: 6.4314, Time: 0.088s\n  Batch 400/765, Loss: 7.8628, Time: 0.185s\n  Batch 410/765, Loss: 7.0250, Time: 0.088s\n  Batch 420/765, Loss: 6.1204, Time: 0.088s\n  Batch 430/765, Loss: 8.8991, Time: 0.090s\n  Batch 440/765, Loss: 8.6392, Time: 0.091s\n  Batch 450/765, Loss: 9.1418, Time: 0.092s\n  Batch 460/765, Loss: 12.7649, Time: 0.087s\n  Batch 470/765, Loss: 10.8826, Time: 0.088s\n  Batch 480/765, Loss: 11.6135, Time: 0.092s\n  Batch 490/765, Loss: 10.6192, Time: 0.089s\n  Batch 500/765, Loss: 10.1799, Time: 0.085s\n  Batch 510/765, Loss: 9.5854, Time: 0.090s\n  Batch 520/765, Loss: 11.2483, Time: 0.095s\n  Batch 530/765, Loss: 9.3520, Time: 0.086s\n  Batch 540/765, Loss: 9.9273, Time: 0.092s\n  Batch 550/765, Loss: 9.8341, Time: 0.087s\n  Batch 560/765, Loss: 10.7711, Time: 0.112s\n  Batch 570/765, Loss: 8.7101, Time: 0.090s\n  Batch 580/765, Loss: 9.3457, Time: 0.088s\n  Batch 590/765, Loss: 7.7351, Time: 0.145s\n  Batch 600/765, Loss: 10.1779, Time: 0.085s\n  Batch 610/765, Loss: 12.3104, Time: 0.086s\n  Batch 620/765, Loss: 9.3699, Time: 0.108s\n  Batch 630/765, Loss: 10.1045, Time: 0.085s\n  Batch 640/765, Loss: 10.0578, Time: 0.093s\n  Batch 650/765, Loss: 7.0612, Time: 0.095s\n  Batch 660/765, Loss: 9.6101, Time: 0.084s\n  Batch 670/765, Loss: 9.6719, Time: 0.100s\n  Batch 680/765, Loss: 10.2906, Time: 0.087s\n  Batch 690/765, Loss: 7.9215, Time: 0.086s\n  Batch 700/765, Loss: 7.0781, Time: 0.099s\n  Batch 710/765, Loss: 10.0054, Time: 0.093s\n  Batch 720/765, Loss: 9.9390, Time: 0.091s\n  Batch 730/765, Loss: 6.7107, Time: 0.090s\n  Batch 740/765, Loss: 8.2882, Time: 0.115s\n  Batch 750/765, Loss: 10.4220, Time: 0.085s\n  Batch 760/765, Loss: 9.8335, Time: 0.107s\nEpoch 12/20\n  Train Loss: 9.0170 (765 batches, avg batch time: 0.095s)\n  Val Loss: 8.8153 (validation time: 114.67s)\n  Epoch Time: 678.61s, Est. Remaining: 90.86 minutes\n  No improvement for 1/5 epochs\n  Batch 10/765, Loss: 9.2162, Time: 0.090s\n  Batch 20/765, Loss: 8.2477, Time: 0.091s\n  Batch 30/765, Loss: 6.5329, Time: 0.091s\n  Batch 40/765, Loss: 8.1384, Time: 0.093s\n  Batch 50/765, Loss: 8.5216, Time: 0.091s\n  Batch 60/765, Loss: 9.4272, Time: 0.091s\n  Batch 70/765, Loss: 9.9545, Time: 0.088s\n  Batch 80/765, Loss: 8.4532, Time: 0.085s\n  Batch 90/765, Loss: 7.5239, Time: 0.083s\n  Batch 100/765, Loss: 7.5623, Time: 0.100s\n  Batch 110/765, Loss: 8.9134, Time: 0.090s\n  Batch 120/765, Loss: 9.4636, Time: 0.088s\n  Batch 130/765, Loss: 10.2632, Time: 0.090s\n  Batch 140/765, Loss: 10.9800, Time: 0.090s\n  Batch 150/765, Loss: 8.6470, Time: 0.111s\n  Batch 160/765, Loss: 7.9651, Time: 0.088s\n  Batch 170/765, Loss: 9.0690, Time: 0.069s\n  Batch 180/765, Loss: 8.0234, Time: 0.090s\n  Batch 190/765, Loss: 10.0394, Time: 0.095s\n  Batch 200/765, Loss: 8.2140, Time: 0.087s\n  Batch 210/765, Loss: 6.0088, Time: 0.098s\n  Batch 220/765, Loss: 8.1968, Time: 0.089s\n  Batch 230/765, Loss: 9.1367, Time: 0.087s\n  Batch 240/765, Loss: 10.6778, Time: 0.087s\n  Batch 250/765, Loss: 9.2502, Time: 0.101s\n  Batch 260/765, Loss: 11.1390, Time: 0.089s\n  Batch 270/765, Loss: 10.9785, Time: 0.089s\n  Batch 280/765, Loss: 9.6041, Time: 0.090s\n  Batch 290/765, Loss: 6.8316, Time: 0.089s\n  Batch 300/765, Loss: 10.5486, Time: 0.151s\n  Batch 310/765, Loss: 8.0983, Time: 0.113s\n  Batch 320/765, Loss: 8.5804, Time: 0.115s\n  Batch 330/765, Loss: 10.8472, Time: 0.091s\n  Batch 340/765, Loss: 8.6032, Time: 0.062s\n  Batch 350/765, Loss: 8.3154, Time: 0.087s\n  Batch 360/765, Loss: 8.8917, Time: 0.088s\n  Batch 370/765, Loss: 9.8273, Time: 0.088s\n  Batch 380/765, Loss: 11.5534, Time: 0.093s\n  Batch 390/765, Loss: 8.8478, Time: 0.061s\n  Batch 400/765, Loss: 6.1845, Time: 0.067s\n  Batch 410/765, Loss: 8.5267, Time: 0.087s\n  Batch 420/765, Loss: 7.8132, Time: 0.089s\n  Batch 430/765, Loss: 8.0529, Time: 0.093s\n  Batch 440/765, Loss: 9.9225, Time: 0.114s\n  Batch 450/765, Loss: 10.9247, Time: 0.088s\n  Batch 460/765, Loss: 8.8002, Time: 0.118s\n  Batch 470/765, Loss: 11.2277, Time: 0.088s\n  Batch 480/765, Loss: 9.2151, Time: 0.088s\n  Batch 490/765, Loss: 10.2939, Time: 0.086s\n  Batch 500/765, Loss: 8.6348, Time: 0.087s\n  Batch 510/765, Loss: 10.7922, Time: 0.088s\n  Batch 520/765, Loss: 9.0148, Time: 0.149s\n  Batch 530/765, Loss: 7.6389, Time: 0.165s\n  Batch 540/765, Loss: 11.7217, Time: 0.088s\n  Batch 550/765, Loss: 7.2725, Time: 0.089s\n  Batch 560/765, Loss: 7.8929, Time: 0.088s\n  Batch 570/765, Loss: 9.5753, Time: 0.090s\n  Batch 580/765, Loss: 10.1747, Time: 0.090s\n  Batch 590/765, Loss: 10.6583, Time: 0.092s\n  Batch 600/765, Loss: 9.7558, Time: 0.093s\n  Batch 610/765, Loss: 11.5278, Time: 0.090s\n  Batch 620/765, Loss: 9.4280, Time: 0.062s\n  Batch 630/765, Loss: 11.2757, Time: 0.062s\n  Batch 640/765, Loss: 9.1883, Time: 0.087s\n  Batch 650/765, Loss: 9.7553, Time: 0.112s\n  Batch 660/765, Loss: 11.1153, Time: 0.086s\n  Batch 670/765, Loss: 8.3378, Time: 0.090s\n  Batch 680/765, Loss: 9.3479, Time: 0.088s\n  Batch 690/765, Loss: 9.5059, Time: 0.086s\n  Batch 700/765, Loss: 8.3447, Time: 0.087s\n  Batch 710/765, Loss: 9.2511, Time: 0.088s\n  Batch 720/765, Loss: 10.7325, Time: 0.091s\n  Batch 730/765, Loss: 8.6682, Time: 0.092s\n  Batch 740/765, Loss: 6.7495, Time: 0.087s\n  Batch 750/765, Loss: 12.7918, Time: 0.087s\n  Batch 760/765, Loss: 8.5525, Time: 0.097s\nEpoch 13/20\n  Train Loss: 9.0149 (765 batches, avg batch time: 0.091s)\n  Val Loss: 8.6867 (validation time: 118.27s)\n  Epoch Time: 680.01s, Est. Remaining: 79.49 minutes\n  No improvement for 2/5 epochs\n  Batch 10/765, Loss: 9.7490, Time: 0.089s\n  Batch 20/765, Loss: 8.9255, Time: 0.067s\n  Batch 30/765, Loss: 11.9345, Time: 0.088s\n  Batch 40/765, Loss: 6.6198, Time: 0.088s\n  Batch 50/765, Loss: 8.2492, Time: 0.091s\n  Batch 60/765, Loss: 11.2879, Time: 0.088s\n  Batch 70/765, Loss: 8.7622, Time: 0.089s\n  Batch 80/765, Loss: 6.2777, Time: 0.061s\n  Batch 90/765, Loss: 8.3876, Time: 0.111s\n  Batch 100/765, Loss: 9.2125, Time: 0.087s\n  Batch 110/765, Loss: 10.1364, Time: 0.088s\n  Batch 120/765, Loss: 8.3968, Time: 0.089s\n  Batch 130/765, Loss: 8.3048, Time: 0.060s\n  Batch 140/765, Loss: 8.5936, Time: 0.087s\n  Batch 150/765, Loss: 10.7154, Time: 0.087s\n  Batch 160/765, Loss: 7.1878, Time: 0.105s\n  Batch 170/765, Loss: 9.5023, Time: 0.089s\n  Batch 180/765, Loss: 7.1872, Time: 0.087s\n  Batch 190/765, Loss: 10.4661, Time: 0.089s\n  Batch 200/765, Loss: 8.3069, Time: 0.088s\n  Batch 210/765, Loss: 11.6309, Time: 0.138s\n  Batch 220/765, Loss: 10.2475, Time: 0.090s\n  Batch 230/765, Loss: 9.8953, Time: 0.089s\n  Batch 240/765, Loss: 9.3375, Time: 0.088s\n  Batch 250/765, Loss: 9.6614, Time: 0.088s\n  Batch 260/765, Loss: 9.8873, Time: 0.087s\n  Batch 270/765, Loss: 7.2660, Time: 0.089s\n  Batch 280/765, Loss: 8.9284, Time: 0.089s\n  Batch 290/765, Loss: 8.7270, Time: 0.088s\n  Batch 300/765, Loss: 9.6071, Time: 0.088s\n  Batch 310/765, Loss: 9.5345, Time: 0.098s\n  Batch 320/765, Loss: 8.9483, Time: 0.090s\n  Batch 330/765, Loss: 7.2387, Time: 0.089s\n  Batch 340/765, Loss: 11.5380, Time: 0.086s\n  Batch 350/765, Loss: 11.3925, Time: 0.091s\n  Batch 360/765, Loss: 8.6893, Time: 0.090s\n  Batch 370/765, Loss: 10.5015, Time: 0.093s\n  Batch 380/765, Loss: 7.9524, Time: 0.088s\n  Batch 390/765, Loss: 9.8530, Time: 0.094s\n  Batch 400/765, Loss: 10.1760, Time: 0.088s\n  Batch 410/765, Loss: 8.3593, Time: 0.090s\n  Batch 420/765, Loss: 10.1064, Time: 0.086s\n  Batch 430/765, Loss: 9.2049, Time: 0.140s\n  Batch 440/765, Loss: 10.3474, Time: 0.088s\n  Batch 450/765, Loss: 8.4632, Time: 0.089s\n  Batch 460/765, Loss: 8.7769, Time: 0.093s\n  Batch 470/765, Loss: 12.7430, Time: 0.089s\n  Batch 480/765, Loss: 8.8650, Time: 0.089s\n  Batch 490/765, Loss: 9.0245, Time: 0.089s\n  Batch 500/765, Loss: 7.3222, Time: 0.089s\n  Batch 510/765, Loss: 9.6333, Time: 0.091s\n  Batch 520/765, Loss: 6.7426, Time: 0.162s\n  Batch 530/765, Loss: 10.6070, Time: 0.104s\n  Batch 540/765, Loss: 10.7334, Time: 0.087s\n  Batch 550/765, Loss: 7.7649, Time: 0.091s\n  Batch 560/765, Loss: 8.3589, Time: 0.089s\n  Batch 570/765, Loss: 8.3191, Time: 0.087s\n  Batch 580/765, Loss: 6.3789, Time: 0.090s\n  Batch 590/765, Loss: 8.6550, Time: 0.090s\n  Batch 600/765, Loss: 10.3079, Time: 0.063s\n  Batch 610/765, Loss: 8.4923, Time: 0.088s\n  Batch 620/765, Loss: 10.5936, Time: 0.112s\n  Batch 630/765, Loss: 7.2364, Time: 0.090s\n  Batch 640/765, Loss: 9.2710, Time: 0.088s\n  Batch 650/765, Loss: 9.4199, Time: 0.087s\n  Batch 660/765, Loss: 8.2160, Time: 0.116s\n  Batch 670/765, Loss: 8.9185, Time: 0.089s\n  Batch 680/765, Loss: 8.9477, Time: 0.114s\n  Batch 690/765, Loss: 7.2420, Time: 0.087s\n  Batch 700/765, Loss: 9.4874, Time: 0.112s\n  Batch 710/765, Loss: 10.9099, Time: 0.090s\n  Batch 720/765, Loss: 8.2874, Time: 0.090s\n  Batch 730/765, Loss: 9.8831, Time: 0.088s\n  Batch 740/765, Loss: 9.1944, Time: 0.166s\n  Batch 750/765, Loss: 10.6969, Time: 0.090s\n  Batch 760/765, Loss: 10.2160, Time: 0.092s\nEpoch 14/20\n  Train Loss: 9.0338 (765 batches, avg batch time: 0.093s)\n  Val Loss: 8.6930 (validation time: 118.50s)\n  Epoch Time: 684.93s, Est. Remaining: 68.16 minutes\n  No improvement for 3/5 epochs\n  Batch 10/765, Loss: 7.5149, Time: 0.088s\n  Batch 20/765, Loss: 9.5447, Time: 0.088s\n  Batch 30/765, Loss: 9.6231, Time: 0.088s\n  Batch 40/765, Loss: 8.8424, Time: 0.089s\n  Batch 50/765, Loss: 9.4678, Time: 0.090s\n  Batch 60/765, Loss: 7.7971, Time: 0.093s\n  Batch 70/765, Loss: 7.8883, Time: 0.115s\n  Batch 80/765, Loss: 7.9482, Time: 0.090s\n  Batch 90/765, Loss: 9.6011, Time: 0.091s\n  Batch 100/765, Loss: 12.7010, Time: 0.087s\n  Batch 110/765, Loss: 7.3797, Time: 0.089s\n  Batch 120/765, Loss: 8.5553, Time: 0.091s\n  Batch 130/765, Loss: 6.7670, Time: 0.090s\n  Batch 140/765, Loss: 9.2851, Time: 0.086s\n  Batch 150/765, Loss: 11.1251, Time: 0.090s\n  Batch 160/765, Loss: 7.7734, Time: 0.088s\n  Batch 170/765, Loss: 7.4786, Time: 0.089s\n  Batch 180/765, Loss: 7.9286, Time: 0.091s\n  Batch 190/765, Loss: 8.9397, Time: 0.088s\n  Batch 200/765, Loss: 8.3095, Time: 0.092s\n  Batch 210/765, Loss: 7.5108, Time: 0.295s\n  Batch 220/765, Loss: 7.3520, Time: 0.092s\n  Batch 230/765, Loss: 7.9782, Time: 0.090s\n  Batch 240/765, Loss: 9.8716, Time: 0.088s\n  Batch 250/765, Loss: 8.7911, Time: 0.088s\n  Batch 260/765, Loss: 8.6351, Time: 0.088s\n  Batch 270/765, Loss: 9.4028, Time: 0.088s\n  Batch 280/765, Loss: 8.6636, Time: 0.090s\n  Batch 290/765, Loss: 8.3408, Time: 0.088s\n  Batch 300/765, Loss: 9.3406, Time: 0.088s\n  Batch 310/765, Loss: 9.3416, Time: 0.089s\n  Batch 320/765, Loss: 9.2420, Time: 0.089s\n  Batch 330/765, Loss: 8.1629, Time: 0.097s\n  Batch 340/765, Loss: 6.6295, Time: 0.089s\n  Batch 350/765, Loss: 7.2294, Time: 0.091s\n  Batch 360/765, Loss: 5.8238, Time: 0.089s\n  Batch 370/765, Loss: 7.2269, Time: 0.098s\n  Batch 380/765, Loss: 10.2238, Time: 0.095s\n  Batch 390/765, Loss: 10.6958, Time: 0.086s\n  Batch 400/765, Loss: 8.7523, Time: 0.089s\n  Batch 410/765, Loss: 9.2383, Time: 0.092s\n  Batch 420/765, Loss: 7.3555, Time: 0.088s\n  Batch 430/765, Loss: 8.9871, Time: 0.109s\n  Batch 440/765, Loss: 7.0689, Time: 0.089s\n  Batch 450/765, Loss: 7.8877, Time: 0.088s\n  Batch 460/765, Loss: 11.1526, Time: 0.061s\n  Batch 470/765, Loss: 9.7806, Time: 0.113s\n  Batch 480/765, Loss: 9.4378, Time: 0.088s\n  Batch 490/765, Loss: 9.9079, Time: 0.093s\n  Batch 500/765, Loss: 11.4674, Time: 0.089s\n  Batch 510/765, Loss: 8.0338, Time: 0.089s\n  Batch 520/765, Loss: 10.6003, Time: 0.088s\n  Batch 530/765, Loss: 10.2395, Time: 0.088s\n  Batch 540/765, Loss: 10.1132, Time: 0.089s\n  Batch 550/765, Loss: 6.7522, Time: 0.088s\n  Batch 560/765, Loss: 6.9548, Time: 0.062s\n  Batch 570/765, Loss: 9.7618, Time: 0.061s\n  Batch 580/765, Loss: 8.2420, Time: 0.089s\n  Batch 590/765, Loss: 9.6806, Time: 0.090s\n  Batch 600/765, Loss: 9.1990, Time: 0.093s\n  Batch 610/765, Loss: 11.0912, Time: 0.089s\n  Batch 620/765, Loss: 7.4986, Time: 0.087s\n  Batch 630/765, Loss: 9.5996, Time: 0.092s\n  Batch 640/765, Loss: 11.6450, Time: 0.061s\n  Batch 650/765, Loss: 9.6001, Time: 0.135s\n  Batch 660/765, Loss: 9.8942, Time: 0.089s\n  Batch 670/765, Loss: 9.7427, Time: 0.087s\n  Batch 680/765, Loss: 9.8493, Time: 0.063s\n  Batch 690/765, Loss: 9.0862, Time: 0.091s\n  Batch 700/765, Loss: 8.9206, Time: 0.087s\n  Batch 710/765, Loss: 9.5875, Time: 0.089s\n  Batch 720/765, Loss: 6.7939, Time: 0.089s\n  Batch 730/765, Loss: 12.1100, Time: 0.088s\n  Batch 740/765, Loss: 10.8657, Time: 0.086s\n  Batch 750/765, Loss: 7.5405, Time: 0.090s\n  Batch 760/765, Loss: 8.2132, Time: 0.090s\nEpoch 15/20\n  Train Loss: 8.9603 (765 batches, avg batch time: 0.093s)\n  Val Loss: 8.6855 (validation time: 117.92s)\n  Epoch Time: 682.81s, Est. Remaining: 56.80 minutes\n  No improvement for 4/5 epochs\n  Batch 10/765, Loss: 7.4268, Time: 0.089s\n  Batch 20/765, Loss: 7.6383, Time: 0.089s\n  Batch 30/765, Loss: 7.3472, Time: 0.112s\n  Batch 40/765, Loss: 8.5669, Time: 0.070s\n  Batch 50/765, Loss: 7.6386, Time: 0.090s\n  Batch 60/765, Loss: 8.0327, Time: 0.092s\n  Batch 70/765, Loss: 7.7650, Time: 0.087s\n  Batch 80/765, Loss: 6.7546, Time: 0.088s\n  Batch 90/765, Loss: 6.5628, Time: 0.089s\n  Batch 100/765, Loss: 8.8460, Time: 0.088s\n  Batch 110/765, Loss: 6.9470, Time: 0.088s\n  Batch 120/765, Loss: 8.5892, Time: 0.133s\n  Batch 130/765, Loss: 10.7774, Time: 0.088s\n  Batch 140/765, Loss: 9.3993, Time: 0.088s\n  Batch 150/765, Loss: 7.8343, Time: 0.087s\n  Batch 160/765, Loss: 9.8080, Time: 0.092s\n  Batch 170/765, Loss: 7.1878, Time: 0.089s\n  Batch 180/765, Loss: 11.4226, Time: 0.087s\n  Batch 190/765, Loss: 9.1960, Time: 0.087s\n  Batch 200/765, Loss: 9.7190, Time: 0.088s\n  Batch 210/765, Loss: 9.6473, Time: 0.109s\n  Batch 220/765, Loss: 10.9424, Time: 0.086s\n  Batch 230/765, Loss: 9.9596, Time: 0.088s\n  Batch 240/765, Loss: 12.4138, Time: 0.093s\n  Batch 250/765, Loss: 7.2169, Time: 0.093s\n  Batch 260/765, Loss: 8.2715, Time: 0.065s\n  Batch 270/765, Loss: 7.3980, Time: 0.089s\n  Batch 280/765, Loss: 9.5296, Time: 0.090s\n  Batch 290/765, Loss: 8.5712, Time: 0.088s\n  Batch 300/765, Loss: 9.6787, Time: 0.087s\n  Batch 310/765, Loss: 9.3496, Time: 0.102s\n  Batch 320/765, Loss: 8.9384, Time: 0.089s\n  Batch 330/765, Loss: 9.7840, Time: 0.091s\n  Batch 340/765, Loss: 9.6708, Time: 0.091s\n  Batch 350/765, Loss: 8.5787, Time: 0.087s\n  Batch 360/765, Loss: 6.7487, Time: 0.087s\n  Batch 370/765, Loss: 11.7287, Time: 0.089s\n  Batch 380/765, Loss: 8.6958, Time: 0.089s\n  Batch 390/765, Loss: 9.2797, Time: 0.089s\n  Batch 400/765, Loss: 7.6943, Time: 0.088s\n  Batch 410/765, Loss: 9.7960, Time: 0.087s\n  Batch 420/765, Loss: 10.4466, Time: 0.062s\n  Batch 430/765, Loss: 7.6555, Time: 0.100s\n  Batch 440/765, Loss: 8.2507, Time: 0.087s\n  Batch 450/765, Loss: 11.0985, Time: 0.088s\n  Batch 460/765, Loss: 9.1897, Time: 0.099s\n  Batch 470/765, Loss: 6.8667, Time: 0.088s\n  Batch 480/765, Loss: 9.3788, Time: 0.087s\n  Batch 490/765, Loss: 8.3125, Time: 0.089s\n  Batch 500/765, Loss: 9.8152, Time: 0.087s\n  Batch 510/765, Loss: 10.1583, Time: 0.087s\n  Batch 520/765, Loss: 7.2930, Time: 0.089s\n  Batch 530/765, Loss: 6.6464, Time: 0.097s\n  Batch 540/765, Loss: 10.3128, Time: 0.089s\n  Batch 550/765, Loss: 8.2856, Time: 0.088s\n  Batch 560/765, Loss: 9.7374, Time: 0.088s\n  Batch 570/765, Loss: 5.5186, Time: 0.089s\n  Batch 580/765, Loss: 11.6920, Time: 0.088s\n  Batch 590/765, Loss: 9.7959, Time: 0.092s\n  Batch 600/765, Loss: 9.1225, Time: 0.085s\n  Batch 610/765, Loss: 10.8933, Time: 0.087s\n  Batch 620/765, Loss: 9.5793, Time: 0.088s\n  Batch 630/765, Loss: 8.5570, Time: 0.090s\n  Batch 640/765, Loss: 8.5083, Time: 0.087s\n  Batch 650/765, Loss: 9.8685, Time: 0.145s\n  Batch 660/765, Loss: 9.7617, Time: 0.088s\n  Batch 670/765, Loss: 8.9974, Time: 0.090s\n  Batch 680/765, Loss: 9.6874, Time: 0.089s\n  Batch 690/765, Loss: 7.6882, Time: 0.088s\n  Batch 700/765, Loss: 9.0762, Time: 0.088s\n  Batch 710/765, Loss: 8.3924, Time: 0.093s\n  Batch 720/765, Loss: 6.8174, Time: 0.088s\n  Batch 730/765, Loss: 8.1253, Time: 0.089s\n  Batch 740/765, Loss: 9.3440, Time: 0.087s\n  Batch 750/765, Loss: 9.1696, Time: 0.089s\n  Batch 760/765, Loss: 8.5821, Time: 0.086s\nEpoch 16/20\n  Train Loss: 9.0889 (765 batches, avg batch time: 0.093s)\n  Val Loss: 8.6854 (validation time: 118.75s)\n  Epoch Time: 683.86s, Est. Remaining: 45.45 minutes\n  No improvement for 5/5 epochs\nEarly stopping at epoch 16\nTraining completed in 181.81 minutes (10908.8 seconds)\nParams: [2.25785446e+01 5.92025245e+01 1.84335552e-02 3.01717051e+01\n 2.03969484e+00], Validation MAPE: 13.3277%\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/193834280.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Run ADE optimization to find best hyperparameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Running ADE optimization for hyperparameters...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m best_params = optimize_hyperparameters(\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mtrain_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mval_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_31/147766104.py\u001b[0m in \u001b[0;36moptimize_hyperparameters\u001b[0;34m(train_loader, val_loader, feature_cols, num_stations)\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;31m# Run ADE optimization with smaller population for faster convergence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 788\u001b[0;31m     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mADE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj_func\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbounds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpop_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    789\u001b[0m     \u001b[0mbest_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_mape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_31/147766104.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, obj_func, bounds, pop_size, max_iter, F, CR, F_range, CR_range)\u001b[0m\n\u001b[1;32m    646\u001b[0m         \u001b[0;31m# Initialize population\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_population\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 648\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfitness\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mind\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m         \u001b[0;31m# Track best solution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_31/147766104.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    646\u001b[0m         \u001b[0;31m# Initialize population\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_population\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 648\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfitness\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mind\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m         \u001b[0;31m# Track best solution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_31/147766104.py\u001b[0m in \u001b[0;36mevaluate_tft_params\u001b[0;34m(params, train_loader, val_loader, feature_cols, num_stations)\u001b[0m\n\u001b[1;32m    728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m     \u001b[0;31m# Train model with early stopping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m     model, train_losses, val_losses = train_model(\n\u001b[0m\u001b[1;32m    731\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m         \u001b[0mtrain_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_31/147766104.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, learning_rate, epochs, patience)\u001b[0m\n\u001b[1;32m    837\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m         \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 839\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    840\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'optimizer_state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m         \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'scheduler_state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2582\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2583\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2584\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m   2585\u001b[0m                 \"Error(s) in loading state_dict for {}:\\n\\t{}\".format(\n\u001b[1;32m   2586\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n\\t\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for TemporalFusionTransformer:\n\tsize mismatch for static_var_processor.skip_layer.weight: copying a param with shape torch.Size([30, 2]) from checkpoint, the shape in current model is torch.Size([27, 2]).\n\tsize mismatch for static_var_processor.skip_layer.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for static_var_processor.fc1.weight: copying a param with shape torch.Size([30, 2]) from checkpoint, the shape in current model is torch.Size([27, 2]).\n\tsize mismatch for static_var_processor.fc1.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for static_var_processor.fc2.weight: copying a param with shape torch.Size([30, 30]) from checkpoint, the shape in current model is torch.Size([27, 27]).\n\tsize mismatch for static_var_processor.fc2.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for static_var_processor.gate.weight: copying a param with shape torch.Size([30, 32]) from checkpoint, the shape in current model is torch.Size([27, 29]).\n\tsize mismatch for static_var_processor.gate.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for static_var_processor.layer_norm.weight: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for static_var_processor.layer_norm.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.weight_grn.fc1.weight: copying a param with shape torch.Size([30, 504]) from checkpoint, the shape in current model is torch.Size([27, 504]).\n\tsize mismatch for temporal_var_selection.weight_grn.fc1.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.weight_grn.fc2.weight: copying a param with shape torch.Size([21, 30]) from checkpoint, the shape in current model is torch.Size([21, 27]).\n\tsize mismatch for temporal_var_selection.var_grns.0.skip_layer.weight: copying a param with shape torch.Size([30, 24]) from checkpoint, the shape in current model is torch.Size([27, 24]).\n\tsize mismatch for temporal_var_selection.var_grns.0.skip_layer.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.0.fc1.weight: copying a param with shape torch.Size([30, 24]) from checkpoint, the shape in current model is torch.Size([27, 24]).\n\tsize mismatch for temporal_var_selection.var_grns.0.fc1.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.0.fc2.weight: copying a param with shape torch.Size([30, 30]) from checkpoint, the shape in current model is torch.Size([27, 27]).\n\tsize mismatch for temporal_var_selection.var_grns.0.fc2.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.0.gate.weight: copying a param with shape torch.Size([30, 54]) from checkpoint, the shape in current model is torch.Size([27, 51]).\n\tsize mismatch for temporal_var_selection.var_grns.0.gate.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.0.layer_norm.weight: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.0.layer_norm.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.1.skip_layer.weight: copying a param with shape torch.Size([30, 24]) from checkpoint, the shape in current model is torch.Size([27, 24]).\n\tsize mismatch for temporal_var_selection.var_grns.1.skip_layer.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.1.fc1.weight: copying a param with shape torch.Size([30, 24]) from checkpoint, the shape in current model is torch.Size([27, 24]).\n\tsize mismatch for temporal_var_selection.var_grns.1.fc1.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.1.fc2.weight: copying a param with shape torch.Size([30, 30]) from checkpoint, the shape in current model is torch.Size([27, 27]).\n\tsize mismatch for temporal_var_selection.var_grns.1.fc2.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.1.gate.weight: copying a param with shape torch.Size([30, 54]) from checkpoint, the shape in current model is torch.Size([27, 51]).\n\tsize mismatch for temporal_var_selection.var_grns.1.gate.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.1.layer_norm.weight: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.1.layer_norm.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.2.skip_layer.weight: copying a param with shape torch.Size([30, 24]) from checkpoint, the shape in current model is torch.Size([27, 24]).\n\tsize mismatch for temporal_var_selection.var_grns.2.skip_layer.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.2.fc1.weight: copying a param with shape torch.Size([30, 24]) from checkpoint, the shape in current model is torch.Size([27, 24]).\n\tsize mismatch for temporal_var_selection.var_grns.2.fc1.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.2.fc2.weight: copying a param with shape torch.Size([30, 30]) from checkpoint, the shape in current model is torch.Size([27, 27]).\n\tsize mismatch for temporal_var_selection.var_grns.2.fc2.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.2.gate.weight: copying a param with shape torch.Size([30, 54]) from checkpoint, the shape in current model is torch.Size([27, 51]).\n\tsize mismatch for temporal_var_selection.var_grns.2.gate.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.2.layer_norm.weight: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.2.layer_norm.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.3.skip_layer.weight: copying a param with shape torch.Size([30, 24]) from checkpoint, the shape in current model is torch.Size([27, 24]).\n\tsize mismatch for temporal_var_selection.var_grns.3.skip_layer.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.3.fc1.weight: copying a param with shape torch.Size([30, 24]) from checkpoint, the shape in current model is torch.Size([27, 24]).\n\tsize mismatch for temporal_var_selection.var_grns.3.fc1.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.3.fc2.weight: copying a param with shape torch.Size([30, 30]) from checkpoint, the shape in current model is torch.Size([27, 27]).\n\tsize mismatch for temporal_var_selection.var_grns.3.fc2.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.3.gate.weight: copying a param with shape torch.Size([30, 54]) from checkpoint, the shape in current model is torch.Size([27, 51]).\n\tsize mismatch for temporal_var_selection.var_grns.3.gate.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.3.layer_norm.weight: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.3.layer_norm.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.4.skip_layer.weight: copying a param with shape torch.Size([30, 24]) from checkpoint, the shape in current model is torch.Size([27, 24]).\n\tsize mismatch for temporal_var_selection.var_grns.4.skip_layer.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.4.fc1.weight: copying a param with shape torch.Size([30, 24]) from checkpoint, the shape in current model is torch.Size([27, 24]).\n\tsize mismatch for temporal_var_selection.var_grns.4.fc1.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.4.fc2.weight: copying a param with shape torch.Size([30, 30]) from checkpoint, the shape in current model is torch.Size([27, 27]).\n\tsize mismatch for temporal_var_selection.var_grns.4.fc2.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.4.gate.weight: copying a param with shape torch.Size([30, 54]) from checkpoint, the shape in current model is torch.Size([27, 51]).\n\tsize mismatch for temporal_var_selection.var_grns.4.gate.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.4.layer_norm.weight: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.4.layer_norm.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.5.skip_layer.weight: copying a param with shape torch.Size([30, 24]) from checkpoint, the shape in current model is torch.Size([27, 24]).\n\tsize mismatch for temporal_var_selection.var_grns.5.skip_layer.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.5.fc1.weight: copying a param with shape torch.Size([30, 24]) from checkpoint, the shape in current model is torch.Size([27, 24]).\n\tsize mismatch for temporal_var_selection.var_grns.5.fc1.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.5.fc2.weight: copying a param with shape torch.Size([30, 30]) from checkpoint, the shape in current model is torch.Size([27, 27]).\n\tsize mismatch for temporal_var_selection.var_grns.5.fc2.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.5.gate.weight: copying a param with shape torch.Size([30, 54]) from checkpoint, the shape in current model is torch.Size([27, 51]).\n\tsize mismatch for temporal_var_selection.var_grns.5.gate.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.5.layer_norm.weight: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.5.layer_norm.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.6.skip_layer.weight: copying a param with shape torch.Size([30, 24]) from checkpoint, the shape in current model is torch.Size([27, 24]).\n\tsize mismatch for temporal_var_selection.var_grns.6.skip_layer.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.6.fc1.weight: copying a param with shape torch.Size([30, 24]) from checkpoint, the shape in current model is torch.Size([27, 24]).\n\tsize mismatch for temporal_var_selection.var_grns.6.fc1.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.6.fc2.weight: copying a param with shape torch.Size([30, 30]) from checkpoint, the shape in current model is torch.Size([27, 27]).\n\tsize mismatch for temporal_var_selection.var_grns.6.fc2.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.6.gate.weight: copying a param with shape torch.Size([30, 54]) from checkpoint, the shape in current model is torch.Size([27, 51]).\n\tsize mismatch for temporal_var_selection.var_grns.6.gate.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.6.layer_norm.weight: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.6.layer_norm.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.7.skip_layer.weight: copying a param with shape torch.Size([30, 24]) from checkpoint, the shape in current model is torch.Size([27, 24]).\n\tsize mismatch for temporal_var_selection.var_grns.7.skip_layer.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.7.fc1.weight: copying a param with shape torch.Size([30, 24]) from checkpoint, the shape in current model is torch.Size([27, 24]).\n\tsize mismatch for temporal_var_selection.var_grns.7.fc1.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.7.fc2.weight: copying a param with shape torch.Size([30, 30]) from checkpoint, the shape in current model is torch.Size([27, 27]).\n\tsize mismatch for temporal_var_selection.var_grns.7.fc2.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.7.gate.weight: copying a param with shape torch.Size([30, 54]) from checkpoint, the shape in current model is torch.Size([27, 51]).\n\tsize mismatch for temporal_var_selection.var_grns.7.gate.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.7.layer_norm.weight: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.7.layer_norm.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.8.skip_layer.weight: copying a param with shape torch.Size([30, 24]) from checkpoint, the shape in current model is torch.Size([27, 24]).\n\tsize mismatch for temporal_var_selection.var_grns.8.skip_layer.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.8.fc1.weight: copying a param with shape torch.Size([30, 24]) from checkpoint, the shape in current model is torch.Size([27, 24]).\n\tsize mismatch for temporal_var_selection.var_grns.8.fc1.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.8.fc2.weight: copying a param with shape torch.Size([30, 30]) from checkpoint, the shape in current model is torch.Size([27, 27]).\n\tsize mismatch for temporal_var_selection.var_grns.8.fc2.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.8.gate.weight: copying a param with shape torch.Size([30, 54]) from checkpoint, the shape in current model is torch.Size([27, 51]).\n\tsize mismatch for temporal_var_selection.var_grns.8.gate.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.8.layer_norm.weight: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.8.layer_norm.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.9.skip_layer.weight: copying a param with shape torch.Size([30, 24]) from checkpoint, the shape in current model is torch.Size([27, 24]).\n\tsize mismatch for temporal_var_selection.var_grns.9.skip_layer.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.9.fc1.weight: copying a param with shape torch.Size([30, 24]) from checkpoint, the shape in current model is torch.Size([27, 24]).\n\tsize mismatch for temporal_var_selection.var_grns.9.fc1.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.9.fc2.weight: copying a param with shape torch.Size([30, 30]) from checkpoint, the shape in current model is torch.Size([27, 27]).\n\tsize mismatch for temporal_var_selection.var_grns.9.fc2.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.9.gate.weight: copying a param with shape torch.Size([30, 54]) from checkpoint, the shape in current model is torch.Size([27, 51]).\n\tsize mismatch for temporal_var_selection.var_grns.9.gate.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.9.layer_norm.weight: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.9.layer_norm.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.10.skip_layer.weight: copying a param with shape torch.Size([30, 24]) from checkpoint, the shape in current model is torch.Size([27, 24]).\n\tsize mismatch for temporal_var_selection.var_grns.10.skip_layer.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.10.fc1.weight: copying a param with shape torch.Size([30, 24]) from checkpoint, the shape in current model is torch.Size([27, 24]).\n\tsize mismatch for temporal_var_selection.var_grns.10.fc1.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.10.fc2.weight: copying a param with shape torch.Size([30, 30]) from checkpoint, the shape in current model is torch.Size([27, 27]).\n\tsize mismatch for temporal_var_selection.var_grns.10.fc2.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.10.gate.weight: copying a param with shape torch.Size([30, 54]) from checkpoint, the shape in current model is torch.Size([27, 51]).\n\tsize mismatch for temporal_var_selection.var_grns.10.gate.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.10.layer_norm.weight: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.10.layer_norm.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.11.skip_layer.weight: copying a param with shape torch.Size([30, 24]) from checkpoint, the shape in current model is torch.Size([27, 24]).\n\tsize mismatch for temporal_var_selection.var_grns.11.skip_layer.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.11.fc1.weight: copying a param with shape torch.Size([30, 24]) from checkpoint, the shape in current model is torch.Size([27, 24]).\n\tsize mismatch for temporal_var_selection.var_grns.11.fc1.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.11.fc2.weight: copying a param with shape torch.Size([30, 30]) from checkpoint, the shape in current model is torch.Size([27, 27]).\n\tsize mismatch for temporal_var_selection.var_grns.11.fc2.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.11.gate.weight: copying a param with shape torch.Size([30, 54]) from checkpoint, the shape in current model is torch.Size([27, 51]).\n\tsize mismatch for temporal_var_selection.var_grns.11.gate.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.11.layer_norm.weight: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.11.layer_norm.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.12.skip_layer.weight: copying a param with shape torch.Size([30, 24]) from checkpoint, the shape in current model is torch.Size([27, 24]).\n\tsize mismatch for temporal_var_selection.var_grns.12.skip_layer.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.12.fc1.weight: copying a param with shape torch.Size([30, 24]) from checkpoint, the shape in current model is torch.Size([27, 24]).\n\tsize mismatch for temporal_var_selection.var_grns.12.fc1.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.12.fc2.weight: copying a param with shape torch.Size([30, 30]) from checkpoint, the shape in current model is torch.Size([27, 27]).\n\tsize mismatch for temporal_var_selection.var_grns.12.fc2.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.12.gate.weight: copying a param with shape torch.Size([30, 54]) from checkpoint, the shape in current model is torch.Size([27, 51]).\n\tsize mismatch for temporal_var_selection.var_grns.12.gate.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.12.layer_norm.weight: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.12.layer_norm.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.13.skip_layer.weight: copying a param with shape torch.Size([30, 24]) from checkpoint, the shape in current model is torch.Size([27, 24]).\n\tsize mismatch for temporal_var_selection.var_grns.13.skip_layer.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.13.fc1.weight: copying a param with shape torch.Size([30, 24]) from checkpoint, the shape in current model is torch.Size([27, 24]).\n\tsize mismatch for temporal_var_selection.var_grns.13.fc1.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.13.fc2.weight: copying a param with shape torch.Size([30, 30]) from checkpoint, the shape in current model is torch.Size([27, 27]).\n\tsize mismatch for temporal_var_selection.var_grns.13.fc2.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.13.gate.weight: copying a param with shape torch.Size([30, 54]) from checkpoint, the shape in current model is torch.Size([27, 51]).\n\tsize mismatch for temporal_var_selection.var_grns.13.gate.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.13.layer_norm.weight: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.13.layer_norm.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.14.skip_layer.weight: copying a param with shape torch.Size([30, 24]) from checkpoint, the shape in current model is torch.Size([27, 24]).\n\tsize mismatch for temporal_var_selection.var_grns.14.skip_layer.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.14.fc1.weight: copying a param with shape torch.Size([30, 24]) from checkpoint, the shape in current model is torch.Size([27, 24]).\n\tsize mismatch for temporal_var_selection.var_grns.14.fc1.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.14.fc2.weight: copying a param with shape torch.Size([30, 30]) from checkpoint, the shape in current model is torch.Size([27, 27]).\n\tsize mismatch for temporal_var_selection.var_grns.14.fc2.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.14.gate.weight: copying a param with shape torch.Size([30, 54]) from checkpoint, the shape in current model is torch.Size([27, 51]).\n\tsize mismatch for temporal_var_selection.var_grns.14.gate.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.14.layer_norm.weight: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.14.layer_norm.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.15.skip_layer.weight: copying a param with shape torch.Size([30, 24]) from checkpoint, the shape in current model is torch.Size([27, 24]).\n\tsize mismatch for temporal_var_selection.var_grns.15.skip_layer.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.15.fc1.weight: copying a param with shape torch.Size([30, 24]) from checkpoint, the shape in current model is torch.Size([27, 24]).\n\tsize mismatch for temporal_var_selection.var_grns.15.fc1.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.15.fc2.weight: copying a param with shape torch.Size([30, 30]) from checkpoint, the shape in current model is torch.Size([27, 27]).\n\tsize mismatch for temporal_var_selection.var_grns.15.fc2.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.15.gate.weight: copying a param with shape torch.Size([30, 54]) from checkpoint, the shape in current model is torch.Size([27, 51]).\n\tsize mismatch for temporal_var_selection.var_grns.15.gate.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.15.layer_norm.weight: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.15.layer_norm.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.16.skip_layer.weight: copying a param with shape torch.Size([30, 24]) from checkpoint, the shape in current model is torch.Size([27, 24]).\n\tsize mismatch for temporal_var_selection.var_grns.16.skip_layer.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.16.fc1.weight: copying a param with shape torch.Size([30, 24]) from checkpoint, the shape in current model is torch.Size([27, 24]).\n\tsize mismatch for temporal_var_selection.var_grns.16.fc1.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.16.fc2.weight: copying a param with shape torch.Size([30, 30]) from checkpoint, the shape in current model is torch.Size([27, 27]).\n\tsize mismatch for temporal_var_selection.var_grns.16.fc2.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.16.gate.weight: copying a param with shape torch.Size([30, 54]) from checkpoint, the shape in current model is torch.Size([27, 51]).\n\tsize mismatch for temporal_var_selection.var_grns.16.gate.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.16.layer_norm.weight: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.16.layer_norm.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.17.skip_layer.weight: copying a param with shape torch.Size([30, 24]) from checkpoint, the shape in current model is torch.Size([27, 24]).\n\tsize mismatch for temporal_var_selection.var_grns.17.skip_layer.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.17.fc1.weight: copying a param with shape torch.Size([30, 24]) from checkpoint, the shape in current model is torch.Size([27, 24]).\n\tsize mismatch for temporal_var_selection.var_grns.17.fc1.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.17.fc2.weight: copying a param with shape torch.Size([30, 30]) from checkpoint, the shape in current model is torch.Size([27, 27]).\n\tsize mismatch for temporal_var_selection.var_grns.17.fc2.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.17.gate.weight: copying a param with shape torch.Size([30, 54]) from checkpoint, the shape in current model is torch.Size([27, 51]).\n\tsize mismatch for temporal_var_selection.var_grns.17.gate.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.17.layer_norm.weight: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.17.layer_norm.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.18.skip_layer.weight: copying a param with shape torch.Size([30, 24]) from checkpoint, the shape in current model is torch.Size([27, 24]).\n\tsize mismatch for temporal_var_selection.var_grns.18.skip_layer.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.18.fc1.weight: copying a param with shape torch.Size([30, 24]) from checkpoint, the shape in current model is torch.Size([27, 24]).\n\tsize mismatch for temporal_var_selection.var_grns.18.fc1.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.18.fc2.weight: copying a param with shape torch.Size([30, 30]) from checkpoint, the shape in current model is torch.Size([27, 27]).\n\tsize mismatch for temporal_var_selection.var_grns.18.fc2.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.18.gate.weight: copying a param with shape torch.Size([30, 54]) from checkpoint, the shape in current model is torch.Size([27, 51]).\n\tsize mismatch for temporal_var_selection.var_grns.18.gate.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.18.layer_norm.weight: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.18.layer_norm.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.19.skip_layer.weight: copying a param with shape torch.Size([30, 24]) from checkpoint, the shape in current model is torch.Size([27, 24]).\n\tsize mismatch for temporal_var_selection.var_grns.19.skip_layer.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.19.fc1.weight: copying a param with shape torch.Size([30, 24]) from checkpoint, the shape in current model is torch.Size([27, 24]).\n\tsize mismatch for temporal_var_selection.var_grns.19.fc1.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.19.fc2.weight: copying a param with shape torch.Size([30, 30]) from checkpoint, the shape in current model is torch.Size([27, 27]).\n\tsize mismatch for temporal_var_selection.var_grns.19.fc2.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.19.gate.weight: copying a param with shape torch.Size([30, 54]) from checkpoint, the shape in current model is torch.Size([27, 51]).\n\tsize mismatch for temporal_var_selection.var_grns.19.gate.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.19.layer_norm.weight: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.19.layer_norm.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.20.skip_layer.weight: copying a param with shape torch.Size([30, 24]) from checkpoint, the shape in current model is torch.Size([27, 24]).\n\tsize mismatch for temporal_var_selection.var_grns.20.skip_layer.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.20.fc1.weight: copying a param with shape torch.Size([30, 24]) from checkpoint, the shape in current model is torch.Size([27, 24]).\n\tsize mismatch for temporal_var_selection.var_grns.20.fc1.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.20.fc2.weight: copying a param with shape torch.Size([30, 30]) from checkpoint, the shape in current model is torch.Size([27, 27]).\n\tsize mismatch for temporal_var_selection.var_grns.20.fc2.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.20.gate.weight: copying a param with shape torch.Size([30, 54]) from checkpoint, the shape in current model is torch.Size([27, 51]).\n\tsize mismatch for temporal_var_selection.var_grns.20.gate.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.20.layer_norm.weight: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.20.layer_norm.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for lstm_layers.0.weight_ih_l0: copying a param with shape torch.Size([120, 30]) from checkpoint, the shape in current model is torch.Size([108, 27]).\n\tsize mismatch for lstm_layers.0.weight_hh_l0: copying a param with shape torch.Size([120, 30]) from checkpoint, the shape in current model is torch.Size([108, 27]).\n\tsize mismatch for lstm_layers.0.bias_ih_l0: copying a param with shape torch.Size([120]) from checkpoint, the shape in current model is torch.Size([108]).\n\tsize mismatch for lstm_layers.0.bias_hh_l0: copying a param with shape torch.Size([120]) from checkpoint, the shape in current model is torch.Size([108]).\n\tsize mismatch for lstm_layers.1.weight_ih_l0: copying a param with shape torch.Size([120, 30]) from checkpoint, the shape in current model is torch.Size([108, 27]).\n\tsize mismatch for lstm_layers.1.weight_hh_l0: copying a param with shape torch.Size([120, 30]) from checkpoint, the shape in current model is torch.Size([108, 27]).\n\tsize mismatch for lstm_layers.1.bias_ih_l0: copying a param with shape torch.Size([120]) from checkpoint, the shape in current model is torch.Size([108]).\n\tsize mismatch for lstm_layers.1.bias_hh_l0: copying a param with shape torch.Size([120]) from checkpoint, the shape in current model is torch.Size([108]).\n\tsize mismatch for self_attention.query.weight: copying a param with shape torch.Size([30, 30]) from checkpoint, the shape in current model is torch.Size([27, 27]).\n\tsize mismatch for self_attention.query.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for self_attention.key.weight: copying a param with shape torch.Size([30, 30]) from checkpoint, the shape in current model is torch.Size([27, 27]).\n\tsize mismatch for self_attention.key.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for self_attention.value.weight: copying a param with shape torch.Size([30, 30]) from checkpoint, the shape in current model is torch.Size([27, 27]).\n\tsize mismatch for self_attention.value.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for self_attention.out.weight: copying a param with shape torch.Size([30, 30]) from checkpoint, the shape in current model is torch.Size([27, 27]).\n\tsize mismatch for self_attention.out.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for forecast_projection.0.weight: copying a param with shape torch.Size([30, 30]) from checkpoint, the shape in current model is torch.Size([27, 27]).\n\tsize mismatch for forecast_projection.0.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for forecast_projection.2.weight: copying a param with shape torch.Size([24, 30]) from checkpoint, the shape in current model is torch.Size([24, 27])."],"ename":"RuntimeError","evalue":"Error(s) in loading state_dict for TemporalFusionTransformer:\n\tsize mismatch for static_var_processor.skip_layer.weight: copying a param with shape torch.Size([30, 2]) from checkpoint, the shape in current model is torch.Size([27, 2]).\n\tsize mismatch for static_var_processor.skip_layer.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for static_var_processor.fc1.weight: copying a param with shape torch.Size([30, 2]) from checkpoint, the shape in current model is torch.Size([27, 2]).\n\tsize mismatch for static_var_processor.fc1.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for static_var_processor.fc2.weight: copying a param with shape torch.Size([30, 30]) from checkpoint, the shape in current model is torch.Size([27, 27]).\n\tsize mismatch for static_var_processor.fc2.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for static_var_processor.gate.weight: copying a param with shape torch.Size([30, 32]) from checkpoint, the shape in current model is torch.Size([27, 29]).\n\tsize mismatch for static_var_processor.gate.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for static_var_processor.layer_norm.weight: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for static_var_processor.layer_norm.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.weight_grn.fc1.weight: copying a param with shape torch.Size([30, 504]) from checkpoint, the shape in current model is torch.Size([27, 504]).\n\tsize mismatch for temporal_var_selection.weight_grn.fc1.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.weight_grn.fc2.weight: copying a param with shape torch.Size([21, 30]) from checkpoint, the shape in current model is torch.Size([21, 27]).\n\tsize mismatch for temporal_var_selection.var_grns.0.skip_layer.weight: copying a param with shape torch.Size([30, 24]) from checkpoint, the shape in current model is torch.Size([27, 24]).\n\tsize mismatch for temporal_var_selection.var_grns.0.skip_layer.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.0.fc1.weight: copying a param with shape torch.Size([30, 24]) from checkpoint, the shape in current model is torch.Size([27, 24]).\n\tsize mismatch for temporal_var_selection.var_grns.0.fc1.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.0.fc2.weight: copying a param with shape torch.Size([30, 30]) from checkpoint, the shape in current model is torch.Size([27, 27]).\n\tsize mismatch for temporal_var_selection.var_grns.0.fc2.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.0.gate.weight: copying a param with shape torch.Size([30, 54]) from checkpoint, the shape in current model is torch.Size([27, 51]).\n\tsize mismatch for temporal_var_selection.var_grns.0.gate.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.0.layer_norm.weight: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.0.layer_norm.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.1.skip_layer.weight: copying a param with shape torch.Size([30, 24]) from checkpoint, the shape in current model is torch.Size([27, 24]).\n\tsize mismatch for temporal_var_selection.var_grns.1.skip_layer.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.1.fc1.weight: copying a param with shape torch.Size([30, 24]) from checkpoint, the shape in current model is torch.Size([27, 24]).\n\tsize mismatch for temporal_var_selection.var_grns.1.fc1.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.1.fc2.weight: copying a param with shape torch.Size([30, 30]) from checkpoint, the shape in current model is torch.Size([27, 27]).\n\tsize mismatch for temporal_var_selection.var_grns.1.fc2.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.1.gate.weight: copying a param with shape torch.Size([30, 54]) from checkpoint, the shape in current model is torch.Size([27, 51]).\n\tsize mismatch for temporal_var_selection.var_grns.1.gate.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.1.layer_norm.weight: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.1.layer_norm.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.2.skip_layer.weight: copying a param with shape torch.Size([30, 24]) from checkpoint, the shape in current model is torch.Size([27, 24]).\n\tsize mismatch for temporal_var_selection.var_grns.2.skip_layer.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.2.fc1.weight: copying a param with shape torch.Size([30, 24]) from checkpoint, the shape in current model is torch.Size([27, 24]).\n\tsize mismatch for temporal_var_selection.var_grns.2.fc1.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.2.fc2.weight: copying a param with shape torch.Size([30, 30]) from checkpoint, the shape in current model is torch.Size([27, 27]).\n\tsize mismatch for temporal_var_selection.var_grns.2.fc2.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.2.gate.weight: copying a param with shape torch.Size([30, 54]) from checkpoint, the shape in current model is torch.Size([27, 51]).\n\tsize mismatch for temporal_var_selection.var_grns.2.gate.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.2.layer_norm.weight: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.2.layer_norm.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.3.skip_layer.weight: copying a param with shape torch.Size([30, 24]) from checkpoint, the shape in current model is torch.Size([27, 24]).\n\tsize mismatch for temporal_var_selection.var_grns.3.skip_layer.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.3.fc1.weight: copying a param with shape torch.Size([30, 24]) from checkpoint, the shape in current model is torch.Size([27, 24]).\n\tsize mismatch for temporal_var_selection.var_grns.3.fc1.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.3.fc2.weight: copying a param with shape torch.Size([30, 30]) from checkpoint, the shape in current model is torch.Size([27, 27]).\n\tsize mismatch for temporal_var_selection.var_grns.3.fc2.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.3.gate.weight: copying a param with shape torch.Size([30, 54]) from checkpoint, the shape in current model is torch.Size([27, 51]).\n\tsize mismatch for temporal_var_selection.var_grns.3.gate.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.3.layer_norm.weight: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.3.layer_norm.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.4.skip_layer.weight: copying a param with shape torch.Size([30, 24]) from checkpoint, the shape in current model is torch.Size([27, 24]).\n\tsize mismatch for temporal_var_selection.var_grns.4.skip_layer.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.4.fc1.weight: copying a param with shape torch.Size([30, 24]) from checkpoint, the shape in current model is torch.Size([27, 24]).\n\tsize mismatch for temporal_var_selection.var_grns.4.fc1.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.4.fc2.weight: copying a param with shape torch.Size([30, 30]) from checkpoint, the shape in current model is torch.Size([27, 27]).\n\tsize mismatch for temporal_var_selection.var_grns.4.fc2.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.4.gate.weight: copying a param with shape torch.Size([30, 54]) from checkpoint, the shape in current model is torch.Size([27, 51]).\n\tsize mismatch for temporal_var_selection.var_grns.4.gate.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.4.layer_norm.weight: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.4.layer_norm.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.5.skip_layer.weight: copying a param with shape torch.Size([30, 24]) from checkpoint, the shape in current model is torch.Size([27, 24]).\n\tsize mismatch for temporal_var_selection.var_grns.5.skip_layer.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.5.fc1.weight: copying a param with shape torch.Size([30, 24]) from checkpoint, the shape in current model is torch.Size([27, 24]).\n\tsize mismatch for temporal_var_selection.var_grns.5.fc1.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.5.fc2.weight: copying a param with shape torch.Size([30, 30]) from checkpoint, the shape in current model is torch.Size([27, 27]).\n\tsize mismatch for temporal_var_selection.var_grns.5.fc2.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.5.gate.weight: copying a param with shape torch.Size([30, 54]) from checkpoint, the shape in current model is torch.Size([27, 51]).\n\tsize mismatch for temporal_var_selection.var_grns.5.gate.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.5.layer_norm.weight: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.5.layer_norm.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.6.skip_layer.weight: copying a param with shape torch.Size([30, 24]) from checkpoint, the shape in current model is torch.Size([27, 24]).\n\tsize mismatch for temporal_var_selection.var_grns.6.skip_layer.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.6.fc1.weight: copying a param with shape torch.Size([30, 24]) from checkpoint, the shape in current model is torch.Size([27, 24]).\n\tsize mismatch for temporal_var_selection.var_grns.6.fc1.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.6.fc2.weight: copying a param with shape torch.Size([30, 30]) from checkpoint, the shape in current model is torch.Size([27, 27]).\n\tsize mismatch for temporal_var_selection.var_grns.6.fc2.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.6.gate.weight: copying a param with shape torch.Size([30, 54]) from checkpoint, the shape in current model is torch.Size([27, 51]).\n\tsize mismatch for temporal_var_selection.var_grns.6.gate.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.6.layer_norm.weight: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.6.layer_norm.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.7.skip_layer.weight: copying a param with shape torch.Size([30, 24]) from checkpoint, the shape in current model is torch.Size([27, 24]).\n\tsize mismatch for temporal_var_selection.var_grns.7.skip_layer.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.7.fc1.weight: copying a param with shape torch.Size([30, 24]) from checkpoint, the shape in current model is torch.Size([27, 24]).\n\tsize mismatch for temporal_var_selection.var_grns.7.fc1.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.7.fc2.weight: copying a param with shape torch.Size([30, 30]) from checkpoint, the shape in current model is torch.Size([27, 27]).\n\tsize mismatch for temporal_var_selection.var_grns.7.fc2.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.7.gate.weight: copying a param with shape torch.Size([30, 54]) from checkpoint, the shape in current model is torch.Size([27, 51]).\n\tsize mismatch for temporal_var_selection.var_grns.7.gate.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.7.layer_norm.weight: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.7.layer_norm.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.8.skip_layer.weight: copying a param with shape torch.Size([30, 24]) from checkpoint, the shape in current model is torch.Size([27, 24]).\n\tsize mismatch for temporal_var_selection.var_grns.8.skip_layer.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.8.fc1.weight: copying a param with shape torch.Size([30, 24]) from checkpoint, the shape in current model is torch.Size([27, 24]).\n\tsize mismatch for temporal_var_selection.var_grns.8.fc1.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.8.fc2.weight: copying a param with shape torch.Size([30, 30]) from checkpoint, the shape in current model is torch.Size([27, 27]).\n\tsize mismatch for temporal_var_selection.var_grns.8.fc2.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.8.gate.weight: copying a param with shape torch.Size([30, 54]) from checkpoint, the shape in current model is torch.Size([27, 51]).\n\tsize mismatch for temporal_var_selection.var_grns.8.gate.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.8.layer_norm.weight: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.8.layer_norm.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.9.skip_layer.weight: copying a param with shape torch.Size([30, 24]) from checkpoint, the shape in current model is torch.Size([27, 24]).\n\tsize mismatch for temporal_var_selection.var_grns.9.skip_layer.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.9.fc1.weight: copying a param with shape torch.Size([30, 24]) from checkpoint, the shape in current model is torch.Size([27, 24]).\n\tsize mismatch for temporal_var_selection.var_grns.9.fc1.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.9.fc2.weight: copying a param with shape torch.Size([30, 30]) from checkpoint, the shape in current model is torch.Size([27, 27]).\n\tsize mismatch for temporal_var_selection.var_grns.9.fc2.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.9.gate.weight: copying a param with shape torch.Size([30, 54]) from checkpoint, the shape in current model is torch.Size([27, 51]).\n\tsize mismatch for temporal_var_selection.var_grns.9.gate.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.9.layer_norm.weight: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.9.layer_norm.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.10.skip_layer.weight: copying a param with shape torch.Size([30, 24]) from checkpoint, the shape in current model is torch.Size([27, 24]).\n\tsize mismatch for temporal_var_selection.var_grns.10.skip_layer.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.10.fc1.weight: copying a param with shape torch.Size([30, 24]) from checkpoint, the shape in current model is torch.Size([27, 24]).\n\tsize mismatch for temporal_var_selection.var_grns.10.fc1.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.10.fc2.weight: copying a param with shape torch.Size([30, 30]) from checkpoint, the shape in current model is torch.Size([27, 27]).\n\tsize mismatch for temporal_var_selection.var_grns.10.fc2.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.10.gate.weight: copying a param with shape torch.Size([30, 54]) from checkpoint, the shape in current model is torch.Size([27, 51]).\n\tsize mismatch for temporal_var_selection.var_grns.10.gate.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.10.layer_norm.weight: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.10.layer_norm.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.11.skip_layer.weight: copying a param with shape torch.Size([30, 24]) from checkpoint, the shape in current model is torch.Size([27, 24]).\n\tsize mismatch for temporal_var_selection.var_grns.11.skip_layer.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.11.fc1.weight: copying a param with shape torch.Size([30, 24]) from checkpoint, the shape in current model is torch.Size([27, 24]).\n\tsize mismatch for temporal_var_selection.var_grns.11.fc1.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.11.fc2.weight: copying a param with shape torch.Size([30, 30]) from checkpoint, the shape in current model is torch.Size([27, 27]).\n\tsize mismatch for temporal_var_selection.var_grns.11.fc2.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.11.gate.weight: copying a param with shape torch.Size([30, 54]) from checkpoint, the shape in current model is torch.Size([27, 51]).\n\tsize mismatch for temporal_var_selection.var_grns.11.gate.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.11.layer_norm.weight: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.11.layer_norm.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.12.skip_layer.weight: copying a param with shape torch.Size([30, 24]) from checkpoint, the shape in current model is torch.Size([27, 24]).\n\tsize mismatch for temporal_var_selection.var_grns.12.skip_layer.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.12.fc1.weight: copying a param with shape torch.Size([30, 24]) from checkpoint, the shape in current model is torch.Size([27, 24]).\n\tsize mismatch for temporal_var_selection.var_grns.12.fc1.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.12.fc2.weight: copying a param with shape torch.Size([30, 30]) from checkpoint, the shape in current model is torch.Size([27, 27]).\n\tsize mismatch for temporal_var_selection.var_grns.12.fc2.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.12.gate.weight: copying a param with shape torch.Size([30, 54]) from checkpoint, the shape in current model is torch.Size([27, 51]).\n\tsize mismatch for temporal_var_selection.var_grns.12.gate.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.12.layer_norm.weight: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.12.layer_norm.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.13.skip_layer.weight: copying a param with shape torch.Size([30, 24]) from checkpoint, the shape in current model is torch.Size([27, 24]).\n\tsize mismatch for temporal_var_selection.var_grns.13.skip_layer.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.13.fc1.weight: copying a param with shape torch.Size([30, 24]) from checkpoint, the shape in current model is torch.Size([27, 24]).\n\tsize mismatch for temporal_var_selection.var_grns.13.fc1.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.13.fc2.weight: copying a param with shape torch.Size([30, 30]) from checkpoint, the shape in current model is torch.Size([27, 27]).\n\tsize mismatch for temporal_var_selection.var_grns.13.fc2.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.13.gate.weight: copying a param with shape torch.Size([30, 54]) from checkpoint, the shape in current model is torch.Size([27, 51]).\n\tsize mismatch for temporal_var_selection.var_grns.13.gate.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.13.layer_norm.weight: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.13.layer_norm.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.14.skip_layer.weight: copying a param with shape torch.Size([30, 24]) from checkpoint, the shape in current model is torch.Size([27, 24]).\n\tsize mismatch for temporal_var_selection.var_grns.14.skip_layer.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.14.fc1.weight: copying a param with shape torch.Size([30, 24]) from checkpoint, the shape in current model is torch.Size([27, 24]).\n\tsize mismatch for temporal_var_selection.var_grns.14.fc1.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.14.fc2.weight: copying a param with shape torch.Size([30, 30]) from checkpoint, the shape in current model is torch.Size([27, 27]).\n\tsize mismatch for temporal_var_selection.var_grns.14.fc2.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.14.gate.weight: copying a param with shape torch.Size([30, 54]) from checkpoint, the shape in current model is torch.Size([27, 51]).\n\tsize mismatch for temporal_var_selection.var_grns.14.gate.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.14.layer_norm.weight: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.14.layer_norm.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.15.skip_layer.weight: copying a param with shape torch.Size([30, 24]) from checkpoint, the shape in current model is torch.Size([27, 24]).\n\tsize mismatch for temporal_var_selection.var_grns.15.skip_layer.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.15.fc1.weight: copying a param with shape torch.Size([30, 24]) from checkpoint, the shape in current model is torch.Size([27, 24]).\n\tsize mismatch for temporal_var_selection.var_grns.15.fc1.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.15.fc2.weight: copying a param with shape torch.Size([30, 30]) from checkpoint, the shape in current model is torch.Size([27, 27]).\n\tsize mismatch for temporal_var_selection.var_grns.15.fc2.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.15.gate.weight: copying a param with shape torch.Size([30, 54]) from checkpoint, the shape in current model is torch.Size([27, 51]).\n\tsize mismatch for temporal_var_selection.var_grns.15.gate.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.15.layer_norm.weight: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.15.layer_norm.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.16.skip_layer.weight: copying a param with shape torch.Size([30, 24]) from checkpoint, the shape in current model is torch.Size([27, 24]).\n\tsize mismatch for temporal_var_selection.var_grns.16.skip_layer.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.16.fc1.weight: copying a param with shape torch.Size([30, 24]) from checkpoint, the shape in current model is torch.Size([27, 24]).\n\tsize mismatch for temporal_var_selection.var_grns.16.fc1.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.16.fc2.weight: copying a param with shape torch.Size([30, 30]) from checkpoint, the shape in current model is torch.Size([27, 27]).\n\tsize mismatch for temporal_var_selection.var_grns.16.fc2.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.16.gate.weight: copying a param with shape torch.Size([30, 54]) from checkpoint, the shape in current model is torch.Size([27, 51]).\n\tsize mismatch for temporal_var_selection.var_grns.16.gate.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.16.layer_norm.weight: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.16.layer_norm.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.17.skip_layer.weight: copying a param with shape torch.Size([30, 24]) from checkpoint, the shape in current model is torch.Size([27, 24]).\n\tsize mismatch for temporal_var_selection.var_grns.17.skip_layer.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.17.fc1.weight: copying a param with shape torch.Size([30, 24]) from checkpoint, the shape in current model is torch.Size([27, 24]).\n\tsize mismatch for temporal_var_selection.var_grns.17.fc1.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.17.fc2.weight: copying a param with shape torch.Size([30, 30]) from checkpoint, the shape in current model is torch.Size([27, 27]).\n\tsize mismatch for temporal_var_selection.var_grns.17.fc2.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.17.gate.weight: copying a param with shape torch.Size([30, 54]) from checkpoint, the shape in current model is torch.Size([27, 51]).\n\tsize mismatch for temporal_var_selection.var_grns.17.gate.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.17.layer_norm.weight: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.17.layer_norm.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.18.skip_layer.weight: copying a param with shape torch.Size([30, 24]) from checkpoint, the shape in current model is torch.Size([27, 24]).\n\tsize mismatch for temporal_var_selection.var_grns.18.skip_layer.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.18.fc1.weight: copying a param with shape torch.Size([30, 24]) from checkpoint, the shape in current model is torch.Size([27, 24]).\n\tsize mismatch for temporal_var_selection.var_grns.18.fc1.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.18.fc2.weight: copying a param with shape torch.Size([30, 30]) from checkpoint, the shape in current model is torch.Size([27, 27]).\n\tsize mismatch for temporal_var_selection.var_grns.18.fc2.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.18.gate.weight: copying a param with shape torch.Size([30, 54]) from checkpoint, the shape in current model is torch.Size([27, 51]).\n\tsize mismatch for temporal_var_selection.var_grns.18.gate.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.18.layer_norm.weight: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.18.layer_norm.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.19.skip_layer.weight: copying a param with shape torch.Size([30, 24]) from checkpoint, the shape in current model is torch.Size([27, 24]).\n\tsize mismatch for temporal_var_selection.var_grns.19.skip_layer.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.19.fc1.weight: copying a param with shape torch.Size([30, 24]) from checkpoint, the shape in current model is torch.Size([27, 24]).\n\tsize mismatch for temporal_var_selection.var_grns.19.fc1.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.19.fc2.weight: copying a param with shape torch.Size([30, 30]) from checkpoint, the shape in current model is torch.Size([27, 27]).\n\tsize mismatch for temporal_var_selection.var_grns.19.fc2.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.19.gate.weight: copying a param with shape torch.Size([30, 54]) from checkpoint, the shape in current model is torch.Size([27, 51]).\n\tsize mismatch for temporal_var_selection.var_grns.19.gate.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.19.layer_norm.weight: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.19.layer_norm.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.20.skip_layer.weight: copying a param with shape torch.Size([30, 24]) from checkpoint, the shape in current model is torch.Size([27, 24]).\n\tsize mismatch for temporal_var_selection.var_grns.20.skip_layer.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.20.fc1.weight: copying a param with shape torch.Size([30, 24]) from checkpoint, the shape in current model is torch.Size([27, 24]).\n\tsize mismatch for temporal_var_selection.var_grns.20.fc1.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.20.fc2.weight: copying a param with shape torch.Size([30, 30]) from checkpoint, the shape in current model is torch.Size([27, 27]).\n\tsize mismatch for temporal_var_selection.var_grns.20.fc2.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.20.gate.weight: copying a param with shape torch.Size([30, 54]) from checkpoint, the shape in current model is torch.Size([27, 51]).\n\tsize mismatch for temporal_var_selection.var_grns.20.gate.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.20.layer_norm.weight: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for temporal_var_selection.var_grns.20.layer_norm.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for lstm_layers.0.weight_ih_l0: copying a param with shape torch.Size([120, 30]) from checkpoint, the shape in current model is torch.Size([108, 27]).\n\tsize mismatch for lstm_layers.0.weight_hh_l0: copying a param with shape torch.Size([120, 30]) from checkpoint, the shape in current model is torch.Size([108, 27]).\n\tsize mismatch for lstm_layers.0.bias_ih_l0: copying a param with shape torch.Size([120]) from checkpoint, the shape in current model is torch.Size([108]).\n\tsize mismatch for lstm_layers.0.bias_hh_l0: copying a param with shape torch.Size([120]) from checkpoint, the shape in current model is torch.Size([108]).\n\tsize mismatch for lstm_layers.1.weight_ih_l0: copying a param with shape torch.Size([120, 30]) from checkpoint, the shape in current model is torch.Size([108, 27]).\n\tsize mismatch for lstm_layers.1.weight_hh_l0: copying a param with shape torch.Size([120, 30]) from checkpoint, the shape in current model is torch.Size([108, 27]).\n\tsize mismatch for lstm_layers.1.bias_ih_l0: copying a param with shape torch.Size([120]) from checkpoint, the shape in current model is torch.Size([108]).\n\tsize mismatch for lstm_layers.1.bias_hh_l0: copying a param with shape torch.Size([120]) from checkpoint, the shape in current model is torch.Size([108]).\n\tsize mismatch for self_attention.query.weight: copying a param with shape torch.Size([30, 30]) from checkpoint, the shape in current model is torch.Size([27, 27]).\n\tsize mismatch for self_attention.query.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for self_attention.key.weight: copying a param with shape torch.Size([30, 30]) from checkpoint, the shape in current model is torch.Size([27, 27]).\n\tsize mismatch for self_attention.key.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for self_attention.value.weight: copying a param with shape torch.Size([30, 30]) from checkpoint, the shape in current model is torch.Size([27, 27]).\n\tsize mismatch for self_attention.value.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for self_attention.out.weight: copying a param with shape torch.Size([30, 30]) from checkpoint, the shape in current model is torch.Size([27, 27]).\n\tsize mismatch for self_attention.out.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for forecast_projection.0.weight: copying a param with shape torch.Size([30, 30]) from checkpoint, the shape in current model is torch.Size([27, 27]).\n\tsize mismatch for forecast_projection.0.bias: copying a param with shape torch.Size([30]) from checkpoint, the shape in current model is torch.Size([27]).\n\tsize mismatch for forecast_projection.2.weight: copying a param with shape torch.Size([24, 30]) from checkpoint, the shape in current model is torch.Size([24, 27]).","output_type":"error"}],"execution_count":15},{"cell_type":"code","source":"# BLOCK 5: MODEL TRAINING WITH OPTIMIZED PARAMETERS\n# ============================================================\n\n# Now recreate data loaders with optimized batch size\ntrain_loader = DataLoader(\n    train_dataset, \n    batch_size=best_params['batch_size'], \n    shuffle=True,\n    pin_memory=True,\n    num_workers=2,\n    persistent_workers=True\n)\n\nval_loader = DataLoader(\n    val_dataset, \n    batch_size=best_params['batch_size'], \n    shuffle=False,\n    pin_memory=True,\n    num_workers=2,\n    persistent_workers=True\n)\n\ntest_loader = DataLoader(\n    test_dataset, \n    batch_size=1,  # Usually keep test batch size at 1 for detailed evaluation\n    shuffle=False,\n    pin_memory=True\n)\n\n# Create model with optimized parameters\nmodel = TemporalFusionTransformer(\n    num_features=len(expanded_feature_cols),\n    num_stations=len(all_stations),\n    hidden_size=best_params['hidden_size'],\n    num_heads=1,  # Fixed as per requirements\n    dropout=0.1,\n    forecast_horizon=24,\n    hidden_layers=best_params['hidden_layers'] \n)\n\n# Train model with optimized parameters\nprint(f\"Training optimized TFT model...\")\nmodel, train_losses, val_losses = train_model(\n    model=model,\n    train_loader=train_loader,\n    val_loader=val_loader,\n    learning_rate=best_params['learning_rate'],\n    epochs=20,\n    patience=5\n)\n\nprint(\"BLOCK 5 COMPLETED: Model training successful.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================\n# BLOCK 6: LOSS VISUALIZATION\n# ============================================================\n\n# Plot training curves\nplt.figure(figsize=(10, 6))\nplt.plot(train_losses, label='Train Loss')\nplt.plot(val_losses, label='Validation Loss')\nplt.title('Training and Validation Loss - LA Downtown')\nplt.xlabel('Epoch')\nplt.ylabel('Loss (MSE)')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.savefig('training_validation_loss.png')\nplt.show()\n\nprint(\"BLOCK 6 COMPLETED: Loss visualization successful.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================\n# BLOCK 7: MODEL EVALUATION\n# ============================================================\n\n# Evaluate model\nprint(f\"Evaluating model on LA Downtown...\")\nrmse, mae, r2, station_metrics = evaluate_model(\n    model=model,\n    data_loader=test_loader,\n    station_ids=all_stations,\n    regions=regions\n)\n\nprint(f\"Overall metrics - RMSE: {rmse:.4f}, MAE: {mae:.4f}, R²: {r2:.4f}\")\n\n# Visualize predictions\nvisualize_predictions(\n    model=model,\n    data_loader=test_loader,\n    station_ids=all_stations,\n    regions=regions,\n    season=\"All Seasons - LA Downtown\"\n)\n\nprint(\"BLOCK 7 COMPLETED: Model evaluation successful.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================\n# BLOCK 8: SEASONAL ANALYSIS - DATASET CREATION\n# ============================================================\n\nprint(\"\\nCreating seasonal datasets...\")\nseasonal_datasets = create_seasonal_datasets(df, target_days, all_stations, feature_cols)\n\n# Visualize predictions for each season\nfor season, season_loader in seasonal_datasets.items():\n    print(f\"Visualizing predictions for {season}...\")\n    visualize_predictions(\n        model=model,\n        data_loader=season_loader,\n        station_ids=all_stations,\n        regions=regions,\n        season=season\n    )\n\nprint(\"BLOCK 8 COMPLETED: Seasonal datasets creation successful.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================\n# BLOCK 9: SEASONAL PERFORMANCE EVALUATION\n# ============================================================\n\n# Evaluate for each season\nseasonal_results = {}\nfor season, season_loader in seasonal_datasets.items():\n    if len(season_loader) > 0:\n        print(f\"Evaluating model on {season} data...\")\n        rmse, mae, r2, _ = evaluate_model(\n            model=model,\n            data_loader=season_loader,\n            station_ids=all_stations,\n            regions=regions\n        )\n        seasonal_results[season] = {\n            'rmse': rmse,\n            'mae': mae,\n            'r2': r2\n        }\n        print(f\"{season} - RMSE: {rmse:.4f}, MAE: {mae:.4f}, R²: {r2:.4f}\")\n\nprint(\"BLOCK 9 COMPLETED: Seasonal performance evaluation successful.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================\n# BLOCK 10: SEASONAL MIRROR PLOTS\n# ============================================================\n\n# Function to unnormalize temperature data\ndef unnormalize_temperature(temp_normalized, temp_min, temp_max):\n    \"\"\"\n    Convert normalized temperature back to original scale.\n    \"\"\"\n    return temp_normalized * (temp_max - temp_min) + temp_min\n\n# Get the temperature normalization parameters\ntemp_min = df['Temperature_C'].min()\ntemp_max = df['Temperature_C'].max()\nprint(f\"Temperature normalization range: min={temp_min:.2f}, max={temp_max:.2f}\")\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nfor season, seasonal_data in seasonal_datasets.items():\n    print(f\"Processing {season} forecast...\")\n    \n    try:\n        # This is a list with a single data point\n        (X_temporal, X_static), y = seasonal_data[0]\n        \n        # Move to device\n        X_temporal = X_temporal.to(device)\n        X_static = X_static.to(device)\n        \n        # Forward pass\n        model.eval()\n        with torch.no_grad():\n            outputs = model((X_temporal, X_static))\n        \n        # Move to CPU for plotting\n        outputs = outputs.cpu().numpy()\n        y_np = y.numpy()\n        \n        # Unnormalize the temperature data\n        outputs_unnorm = unnormalize_temperature(outputs, temp_min, temp_max)\n        y_np_unnorm = unnormalize_temperature(y_np, temp_min, temp_max)\n        \n        # Create the mirror plot with unnormalized data\n        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6), sharey=True)\n        \n        hours = np.arange(24)\n        \n        # Left plot - Actual temperatures\n        ax1.plot(hours, y_np_unnorm[0, 0, :], 'b-o', linewidth=2)\n        ax1.set_title(f\"Actual Temperature - {season}\", fontsize=14)\n        ax1.set_xlabel('Hour of Day', fontsize=12)\n        ax1.set_ylabel('Temperature (°C)', fontsize=12)\n        ax1.set_xlim(0, 23)\n        ax1.grid(True, alpha=0.3)\n        ax1.invert_xaxis()  # Invert x-axis for mirror effect\n        \n        # Right plot - Predicted temperatures\n        ax2.plot(hours, outputs_unnorm[0, 0, :], 'r-o', linewidth=2)\n        ax2.set_title(f\"Predicted Temperature - {season}\", fontsize=14)\n        ax2.set_xlabel('Hour of Day', fontsize=12)\n        ax2.set_xlim(0, 23)\n        ax2.grid(True, alpha=0.3)\n        \n        # Add overall title\n        plt.suptitle(f\"{all_stations[0]} ({regions.get(all_stations[0], 'Unknown')}) - {season} Day Comparison\", \n                    fontsize=16, y=1.05)\n        \n        # Tight layout with minimal space between plots\n        fig.tight_layout()\n        plt.subplots_adjust(wspace=0.05)  # Reduce space between subplots\n        \n        # Add a legend to explain colors\n        ax1.plot([], [], 'b-o', label='Actual Temperature')\n        ax2.plot([], [], 'r-o', label='Predicted Temperature')\n        ax1.legend(loc='upper left')\n        ax2.legend(loc='upper right')\n        \n        # Save and display\n        plt.savefig(f\"{season}_mirror_comparison.png\", bbox_inches='tight')\n        plt.show()\n        plt.close()\n        \n    except Exception as e:\n        print(f\"Error processing {season} forecast: {e}\")\n        import traceback\n        traceback.print_exc()\n\nprint(\"BLOCK 10 COMPLETED: Seasonal mirror plots created successfully.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================\n# BLOCK 11: FINAL ANALYSIS AND RESULTS SAVING\n# ============================================================\n\n# Analyze seasonal performance\nif seasonal_results:\n    analyze_seasonal_performance(seasonal_results)\n\nprint(\"Skipping topographic analysis since we only have data for LA Downtown\")\n\n# Save results to CSV\nresults_df = pd.DataFrame([{\n    'Station': 'LA Downtown',\n    'Region': 'urban',\n    'RMSE': rmse,\n    'MAE': mae,\n    'R2': r2\n}])\n\nresults_df.to_csv('la_downtown_results.csv', index=False)\nprint(\"Results saved to la_downtown_results.csv\")\n\nprint(\"BLOCK 11 COMPLETED: Final analysis and results saving successful.\")\nprint(\"=\" * 70)\nprint(\"ALL BLOCKS EXECUTED SUCCESSFULLY! Analysis complete.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}